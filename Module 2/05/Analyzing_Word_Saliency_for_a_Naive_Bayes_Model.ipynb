{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analyzing Word Saliency for a Naive Bayes Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%202/05/Analyzing_Word_Saliency_for_a_Naive_Bayes_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7_Yl4gzYH6K",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification Using Na&iuml;ve Bayes\n",
        "\n",
        "***Abstract***\n",
        "\n",
        "In this notebook you'll be interpreting the results of fitting a Na&iuml;ve Bayes model for classifying the sentiment of a movie review.\n",
        "\n",
        "## Sentiment Analysis\n",
        "\n",
        "The [Wikipedia Article on Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) provides the following definition for sentiment analysis.\n",
        "\n",
        "> Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n",
        "\n",
        "In this notebook we'll be focusing on predicting the sentiment of a movie review from IMDB based on the text of the movie review.  This dataset is one that was originally used in a Kaggle competition called [Bag of Words meets Bag of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial) (you'll understand that joke by the end of this notebook!)\n",
        "\n",
        "The [data](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) consists of the following.\n",
        "\n",
        "> The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
        "\n",
        "Our goal will be to see if we can learn a model, using Na&iuml;ve Bayes on a training set to accurately estimate sentiment of new reviews.\n",
        "\n",
        "Without further ado, let's download and parse the data into a data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74jnAfgu1pk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "06859391-d962-44d2-8f53-4fff80a48ee5"
      },
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download',\n",
        "               'labeledTrainData.tsv',\n",
        "               quiet=False)\n",
        "df = pd.read_csv('labeledTrainData.tsv', header=0, delimiter='\\t')\n",
        "df"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download\n",
            "To: /content/labeledTrainData.tsv\n",
            "33.6MB [00:00, 161MB/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7759_3</td>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3630_4</td>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9495_8</td>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>3453_3</td>\n",
              "      <td>0</td>\n",
              "      <td>It seems like more consideration has gone into...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>5064_1</td>\n",
              "      <td>0</td>\n",
              "      <td>I don't believe they made this film. Completel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>10905_3</td>\n",
              "      <td>0</td>\n",
              "      <td>Guy is a loser. Can't get girls, needs to buil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>10194_3</td>\n",
              "      <td>0</td>\n",
              "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>8478_8</td>\n",
              "      <td>1</td>\n",
              "      <td>I saw this movie as a child and it broke my he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  sentiment                                             review\n",
              "0       5814_8          1  With all this stuff going down at the moment w...\n",
              "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
              "3       3630_4          0  It must be assumed that those who praised this...\n",
              "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
              "...        ...        ...                                                ...\n",
              "24995   3453_3          0  It seems like more consideration has gone into...\n",
              "24996   5064_1          0  I don't believe they made this film. Completel...\n",
              "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
              "24998  10194_3          0  This 30 minute documentary Buñuel made in the ...\n",
              "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
              "\n",
              "[25000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1vAHoeEwDMn",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the average sentiment to see what we are dealing with (1 is positive sentiment and 0 is negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiyxr0P5vduW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0fad3f1e-7fc2-43fd-89d4-acbf4cb99584"
      },
      "source": [
        "df['sentiment'].mean()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq6YwjQzwIFP",
        "colab_type": "text"
      },
      "source": [
        "Looks like we're dealing with a balanced set of positives and negatives.\n",
        "\n",
        "Next, let's look at a particular review.  To make the output look nicer, we'll create a [new Pandas series with line wrapping](https://www.geeksforgeeks.org/python-pandas-series-str-wrap/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4xQ5xN5wQqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this takes a little while to run\n",
        "reviews_wrapped = df['review'].str.wrap(80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN17b9RTxFaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "9afd1e55-dcac-416f-9f36-cf3efb850cb5"
      },
      "source": [
        "print(reviews_wrapped.iloc[20])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\Soylent Green\\\" is one of the best and most disturbing science fiction movies\n",
            "of the 70's and still very persuasive even by today's standards. Although flawed\n",
            "and a little dated, the apocalyptic touch and the environmental premise (typical\n",
            "for that time) still feel very unsettling and thought-provoking. This film's\n",
            "quality-level surpasses the majority of contemporary SF flicks because of its\n",
            "strong cast and some intense sequences that I personally consider classic. The\n",
            "New York of 2022 is a depressing place to be alive, with over-population,\n",
            "unemployment, an unhealthy climate and the total scarcity of every vital food\n",
            "product. The only form of food available is synthetic and distributed by the\n",
            "Soylent company. Charlton Heston (in a great shape) plays a cop investigating\n",
            "the murder of one of Soylent's most eminent executives and he stumbles upon\n",
            "scandals and dark secrets... The script is a little over-sentimental at times\n",
            "and the climax doesn't really come as a big surprise, still the atmosphere is\n",
            "very tense and uncanny. The riot-sequence is truly grueling and easily one of\n",
            "the most macabre moments in 70's cinema. Edward G. Robinson is ultimately\n",
            "impressive in his last role and there's a great (but too modest) supportive role\n",
            "for Joseph Cotton (\\\"Baron Blood\\\", \\\"The Abominable Dr. Phibes\\\"). THIS is\n",
            "Science-Fiction in my book: a nightmarish and inevitable fade for humanity! No\n",
            "fancy space-ships with hairy monsters attacking our planet.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caV4neeywOJ1",
        "colab_type": "text"
      },
      "source": [
        "## The Bag of Words Model\n",
        "\n",
        "We know that in order to apply Na&iuml;ve Bayes we need to convert each of our reviews into a vector of features.  There are lots of different methods to convert text into vectors.  In this notebook we'll be using a pretty basic (but suprisingly powerful) form of vectorization where we construct a feature vector with $k$ entries (where $k$ is the total number of unique words in the dataset) and for any particular review we set the corresponding entry to $1$ if that word appears in the review and $0$ otherwise.  This representation is called  ***bag of words*** since the encoding of the review into a vector is independent of where the words occur in the review (you could shuffle the words in the review and still have the same feature vector).  The [Wikipedia article on Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) has more information.\n",
        "\n",
        "Instead of writing our own code to convert from text to a bag of words representation we're going to use scikit learn's built-in [count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).  Before we apply it to the data, let's apply it to toy dataset to help you better understand the bag of words model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Dz_L-pcuk1",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizing the Whole Dataset\n",
        "\n",
        "Now that you have a general idea what bag of words is all about, let's apply it to our movie reviews.  To make our lives easier we're only going to include words in our feature vector if they occur in at least 100 reviews.  Doing this will help with overfitting (although next assignment we will be learning another technique to deal with this).  While we're at it we'll also convert the sentiment labels to a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GJLVF0fv6Lu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "297c1266-943a-47c5-e5ab-36f71109a5da"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True, min_df=100)\n",
        "vectorizer.fit(df['review'])\n",
        "X = vectorizer.transform(df['review']).todense()\n",
        "y = np.array(df['sentiment'])\n",
        "print(\"X.shape\", X.shape)\n",
        "print(\"y.shape\", y.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape (25000, 3833)\n",
            "y.shape (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dk2x8CCmTsy",
        "colab_type": "text"
      },
      "source": [
        "As a quick intuition builder, let's look at a word we think would probably differ across sentiment values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKUe7CbBmYwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "05eda3d8-d9a6-412e-85c0-6ef808b6adcc"
      },
      "source": [
        "terrible_index = vectorizer.get_feature_names().index('terrible')\n",
        "print(\"terrible occurs in\", X[y==1, terrible_index].mean(), \"for Y=1\")\n",
        "print(\"terrible occurs in\", X[y==0, terrible_index].mean(), \"for Y=0\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "terrible occurs in 0.01736 for Y=1\n",
            "terrible occurs in 0.08944 for Y=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHpeMudlmuzi",
        "colab_type": "text"
      },
      "source": [
        "## Fitting a Model with sklearn\n",
        "\n",
        "Instead of coding it ourselves, let's using sklearn's built-in algorithm for Na&iuml;ve Bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iOy-js8YjeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32843f2c-9a3b-4f32-e934-00d93f1b5dce"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "np.mean(y_pred == y_test)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85168"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vioLZaJn2BZd",
        "colab_type": "text"
      },
      "source": [
        "## Understanding the Model\n",
        "\n",
        "Now that we've fit the model, let's look at a few different ways to understand what the model is doing.\n",
        "\n",
        "### Which Words Are Most Important\n",
        "\n",
        "One way to investigate the model is to examine which words contribute the most to reviews being judged as positive versus negative.  We define contribution in this case as a combination of a word being strongly indicative of a particular sentiment as well as being relatively common.\n",
        "\n",
        "Here is some code for computing this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAYVpVYW1WNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.feature_log_prob_ gives us the log probability of each of the features\n",
        "# conditioned on a particular class.\n",
        "log_probs = model.feature_log_prob_\n",
        "# model.classes_ tells us which class corresponds to a particular row in\n",
        "# model.feature_log_prob_.\n",
        "class_mapping = model.classes_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RphttkmR2i-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2b1f869-342a-473a-ad52-4c1ec6bbc541"
      },
      "source": [
        "pos_index = np.where(class_mapping == 1)[0][0]\n",
        "neg_index = np.where(class_mapping == 0)[0][0]\n",
        "\n",
        "difference_in_log_probs_by_word = log_probs[pos_index,:] - log_probs[neg_index,:]\n",
        "print(\"the word that bumps up the log probability of a positive review as much as possible is\",\n",
        "      vectorizer.get_feature_names()[np.argmax(difference_in_log_probs_by_word)])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the word that bumps up the log probability of a positive review as much as possible is flawless\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpp0aR-g5BLL",
        "colab_type": "text"
      },
      "source": [
        "We can do the same thing for negative sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0UVbAjA5Efo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4edb6f55-7795-40a6-f549-4ea452fe7b5b"
      },
      "source": [
        "print(\"the word that bumps up the log probability of a positive review as much as possible is\",\n",
        "      vectorizer.get_feature_names()[np.argmin(difference_in_log_probs_by_word)])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the word that bumps up the log probability of a positive review as much as possible is incoherent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP1wVe9O4H8d",
        "colab_type": "text"
      },
      "source": [
        "While flawless and incoherent might be the words that bumps up or down the prediction as much as possible, they may not be all that likely to occur.  Next, we'll reweight these differences by how commonly they occur in the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRyJYgs_2w-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d32c3ea9-f0e3-4b88-a7ab-8f0909b2a005"
      },
      "source": [
        "weighted_differences = np.multiply(X_test.mean(axis=0), difference_in_log_probs_by_word)\n",
        "print(\"the word that bumps up the log probability of a positive review as much as possible weighted by prevalence is\",\n",
        "      vectorizer.get_feature_names()[np.argmax(weighted_differences)])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the word that bumps up the log probability of a positive review as much as possible weighted by prevalence is great\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfIe2hWM4mnW",
        "colab_type": "text"
      },
      "source": [
        "We can do the same thing for negative sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uMP00AO4QV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9020ba4b-2cc6-416c-9f67-77c3821298b8"
      },
      "source": [
        "print(\"the word that bumps up the log probability of a negative review as much as possible weighted by prevalence is\",\n",
        "      vectorizer.get_feature_names()[np.argmin(weighted_differences)])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the word that bumps up the log probability of a negative review as much as possible weighted by prevalence is bad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PchZgWyv4rji",
        "colab_type": "text"
      },
      "source": [
        "We'll leave it to you to modify the code to print out words other than the topmost (e.g., using sort)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EavcGpL5UYe",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing the Model on a particular review\n",
        "\n",
        "Another interesting strategy is to look at a particular review and see how each word contributes to the overall judgment of the model.\n",
        "\n",
        "We'll do this by showing a running total of theve log likelihood ratio of positive versus negative sentiment as the review unfolds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuVFB5VP5mKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5898da7a-729b-4c24-e1a5-ef5af56948f9"
      },
      "source": [
        "review_to_analyze_index = 20\n",
        "\n",
        "# we'll work with the first review in the dataframe\n",
        "review_to_analyze = vectorizer.transform([df['review'].iloc[review_to_analyze_index]]).todense()\n",
        "\n",
        "# first compute the log likelihood ratio of probability of positive review versus\n",
        "# negative review (should be about 0 if these are balanced)\n",
        "running_llr = model.class_log_prior_[pos_index] - model.class_log_prior_[neg_index]\n",
        "running_llr"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0034133366473572124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa5OnlRXKX-1",
        "colab_type": "text"
      },
      "source": [
        "In the notebook where we implemented Na&iuml;ve Bayes, we took into account how the absence of words contributes to the judgment of positive versus negative sentiment.  In most cases, this barely changes the probability of positive sentiment and comes at a substantial computational cost. As a result, some implementations of Na&iuml;ve Bayes (such as sklearn's) just ignore this component.  If you're interested in how this code would work, you can set `use_word_absences` to `True` in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIQl_PVC50M0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_word_absences = False\n",
        "if use_word_absences:\n",
        "    # next, we calculate the contribution to the log likelihood ratio from the words\n",
        "    # that *do not* occur in the review.  These contributions in the case of bag of\n",
        "    # words will matter much less than the words that do occur in the review\n",
        "\n",
        "    probs_positive = np.exp(log_probs[pos_index,:])\n",
        "    probs_negative = np.exp(log_probs[neg_index,:])\n",
        "    llr_for_word_not_occurring = np.log((1-probs_positive)/(1-probs_negative))\n",
        "\n",
        "    llr_contribution_for_words_not_occurring = llr_for_word_not_occurring[np.where(review_to_analyze[0,:] == 0)[1]].sum()\n",
        "    print(\"llr_contribution_for_words_not_occurring\", llr_contribution_for_words_not_occurring)\n",
        "    # running_llr += llr_contribution_for_words_not_occurring\n",
        "    print(\"running_llr\", running_llr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfdxmPwg7j89",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll examine the actual review text and see how it affects the model's output.  We'll print out the review word by word along with the running_llr and and the contribution to the llr from the last word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-T5WsDB6pLP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68f53799-094e-408e-faf4-b90386b9b751"
      },
      "source": [
        "import re\n",
        "\n",
        "# lookup index by word (there may be a better way to do this)\n",
        "reverse_word_lookup = dict(zip(vectorizer.get_feature_names(), range(len(vectorizer.get_feature_names()))))\n",
        "pattern = re.compile(r'(?u)\\b\\w\\w+\\b')\n",
        "processed_words = set()\n",
        "df_llr = pd.DataFrame(columns=['word', 'contribution to llr', 'cumulative llr'])\n",
        "\n",
        "for word in re.findall(pattern, df['review'].iloc[review_to_analyze_index]):\n",
        "    if word.lower() in reverse_word_lookup and word.lower() not in processed_words:\n",
        "        # if already counted, don't count it again\n",
        "        contribution_to_llr = difference_in_log_probs_by_word[reverse_word_lookup[word.lower()]]\n",
        "        processed_words.add(word.lower())\n",
        "    else:\n",
        "        contribution_to_llr = 0\n",
        "\n",
        "    running_llr += contribution_to_llr\n",
        "    df_llr = df_llr.append({'word': word, 'contribution to llr': contribution_to_llr, 'cumulative llr': running_llr}, ignore_index=True)\n",
        "\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "    display(df_llr)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>contribution to llr</th>\n",
              "      <th>cumulative llr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Soylent</td>\n",
              "      <td>0</td>\n",
              "      <td>0.003413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Green</td>\n",
              "      <td>-0.0958766</td>\n",
              "      <td>-0.092463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>is</td>\n",
              "      <td>0.0368863</td>\n",
              "      <td>-0.055577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>one</td>\n",
              "      <td>0.0241627</td>\n",
              "      <td>-0.031414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>of</td>\n",
              "      <td>0.0072271</td>\n",
              "      <td>-0.024187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>the</td>\n",
              "      <td>0.00677751</td>\n",
              "      <td>-0.017410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>best</td>\n",
              "      <td>0.617731</td>\n",
              "      <td>0.600321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>and</td>\n",
              "      <td>0.0201221</td>\n",
              "      <td>0.620443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>most</td>\n",
              "      <td>0.15582</td>\n",
              "      <td>0.776263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>disturbing</td>\n",
              "      <td>0.0842356</td>\n",
              "      <td>0.860498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>science</td>\n",
              "      <td>-0.325185</td>\n",
              "      <td>0.535313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>fiction</td>\n",
              "      <td>0.318557</td>\n",
              "      <td>0.853870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>movies</td>\n",
              "      <td>-0.100125</td>\n",
              "      <td>0.753745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>0.753745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>0.753745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>70</td>\n",
              "      <td>-0.00692133</td>\n",
              "      <td>0.746824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>0.746824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>still</td>\n",
              "      <td>0.322512</td>\n",
              "      <td>1.069336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>very</td>\n",
              "      <td>0.257975</td>\n",
              "      <td>1.327310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>persuasive</td>\n",
              "      <td>0</td>\n",
              "      <td>1.327310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>even</td>\n",
              "      <td>-0.340877</td>\n",
              "      <td>0.986433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>by</td>\n",
              "      <td>0.0554235</td>\n",
              "      <td>1.041857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>today</td>\n",
              "      <td>1.02535</td>\n",
              "      <td>2.067204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>standards</td>\n",
              "      <td>-0.0933059</td>\n",
              "      <td>1.973898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Although</td>\n",
              "      <td>0.324969</td>\n",
              "      <td>2.298867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>flawed</td>\n",
              "      <td>0.390736</td>\n",
              "      <td>2.689604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>2.689604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>little</td>\n",
              "      <td>0.0654876</td>\n",
              "      <td>2.755091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>dated</td>\n",
              "      <td>0.625817</td>\n",
              "      <td>3.380908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>3.380908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>apocalyptic</td>\n",
              "      <td>0</td>\n",
              "      <td>3.380908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>touch</td>\n",
              "      <td>0.517603</td>\n",
              "      <td>3.898511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>3.898511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>3.898511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>environmental</td>\n",
              "      <td>0</td>\n",
              "      <td>3.898511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>premise</td>\n",
              "      <td>-0.779952</td>\n",
              "      <td>3.118559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>typical</td>\n",
              "      <td>0.272618</td>\n",
              "      <td>3.391177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>for</td>\n",
              "      <td>-0.00204234</td>\n",
              "      <td>3.389134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>that</td>\n",
              "      <td>-0.0329541</td>\n",
              "      <td>3.356180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>time</td>\n",
              "      <td>0.0175122</td>\n",
              "      <td>3.373692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>still</td>\n",
              "      <td>0</td>\n",
              "      <td>3.373692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>feel</td>\n",
              "      <td>0.161934</td>\n",
              "      <td>3.535626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>very</td>\n",
              "      <td>0</td>\n",
              "      <td>3.535626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>unsettling</td>\n",
              "      <td>0</td>\n",
              "      <td>3.535626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>3.535626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>thought</td>\n",
              "      <td>-0.0587743</td>\n",
              "      <td>3.476852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>provoking</td>\n",
              "      <td>0.736739</td>\n",
              "      <td>4.213590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>This</td>\n",
              "      <td>-0.0299498</td>\n",
              "      <td>4.183641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>film</td>\n",
              "      <td>0.0486534</td>\n",
              "      <td>4.232294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>quality</td>\n",
              "      <td>-0.0883115</td>\n",
              "      <td>4.143983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>level</td>\n",
              "      <td>-0.00519868</td>\n",
              "      <td>4.138784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>surpasses</td>\n",
              "      <td>0</td>\n",
              "      <td>4.138784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>4.138784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>majority</td>\n",
              "      <td>0.109725</td>\n",
              "      <td>4.248509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>4.248509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>contemporary</td>\n",
              "      <td>0.485515</td>\n",
              "      <td>4.734024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>SF</td>\n",
              "      <td>0</td>\n",
              "      <td>4.734024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>flicks</td>\n",
              "      <td>-0.443292</td>\n",
              "      <td>4.290732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>because</td>\n",
              "      <td>-0.174292</td>\n",
              "      <td>4.116440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>4.116440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>its</td>\n",
              "      <td>0.195074</td>\n",
              "      <td>4.311515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>strong</td>\n",
              "      <td>0.801277</td>\n",
              "      <td>5.112792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>cast</td>\n",
              "      <td>0.217471</td>\n",
              "      <td>5.330263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>5.330263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>some</td>\n",
              "      <td>-0.0881129</td>\n",
              "      <td>5.242150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>intense</td>\n",
              "      <td>1.03979</td>\n",
              "      <td>6.281942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>sequences</td>\n",
              "      <td>0.107703</td>\n",
              "      <td>6.389646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>that</td>\n",
              "      <td>0</td>\n",
              "      <td>6.389646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>personally</td>\n",
              "      <td>0.283764</td>\n",
              "      <td>6.673410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>consider</td>\n",
              "      <td>0.0833176</td>\n",
              "      <td>6.756728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>classic</td>\n",
              "      <td>0.574762</td>\n",
              "      <td>7.331489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>The</td>\n",
              "      <td>0</td>\n",
              "      <td>7.331489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>New</td>\n",
              "      <td>0.243907</td>\n",
              "      <td>7.575397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>York</td>\n",
              "      <td>0.618375</td>\n",
              "      <td>8.193771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>8.193771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>2022</td>\n",
              "      <td>0</td>\n",
              "      <td>8.193771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>8.193771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>depressing</td>\n",
              "      <td>-0.0860042</td>\n",
              "      <td>8.107767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>place</td>\n",
              "      <td>0.0895746</td>\n",
              "      <td>8.197342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>to</td>\n",
              "      <td>-0.00743777</td>\n",
              "      <td>8.189904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>be</td>\n",
              "      <td>-0.100377</td>\n",
              "      <td>8.089527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>alive</td>\n",
              "      <td>0.0258257</td>\n",
              "      <td>8.115353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>with</td>\n",
              "      <td>0.0378539</td>\n",
              "      <td>8.153207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>over</td>\n",
              "      <td>-0.0636426</td>\n",
              "      <td>8.089564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>population</td>\n",
              "      <td>0.197833</td>\n",
              "      <td>8.287397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>unemployment</td>\n",
              "      <td>0</td>\n",
              "      <td>8.287397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>an</td>\n",
              "      <td>0.0561757</td>\n",
              "      <td>8.343573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>unhealthy</td>\n",
              "      <td>0</td>\n",
              "      <td>8.343573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>climate</td>\n",
              "      <td>0</td>\n",
              "      <td>8.343573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>8.343573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>8.343573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>total</td>\n",
              "      <td>-0.841663</td>\n",
              "      <td>7.501910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>scarcity</td>\n",
              "      <td>0</td>\n",
              "      <td>7.501910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>7.501910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>every</td>\n",
              "      <td>-0.0315476</td>\n",
              "      <td>7.470363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>vital</td>\n",
              "      <td>0</td>\n",
              "      <td>7.470363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>food</td>\n",
              "      <td>-0.0919573</td>\n",
              "      <td>7.378405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>product</td>\n",
              "      <td>-0.241402</td>\n",
              "      <td>7.137003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>The</td>\n",
              "      <td>0</td>\n",
              "      <td>7.137003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>only</td>\n",
              "      <td>-0.229573</td>\n",
              "      <td>6.907430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>form</td>\n",
              "      <td>0.43296</td>\n",
              "      <td>7.340390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>7.340390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>food</td>\n",
              "      <td>0</td>\n",
              "      <td>7.340390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>available</td>\n",
              "      <td>0.628269</td>\n",
              "      <td>7.968659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>7.968659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>synthetic</td>\n",
              "      <td>0</td>\n",
              "      <td>7.968659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>7.968659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>distributed</td>\n",
              "      <td>0</td>\n",
              "      <td>7.968659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>by</td>\n",
              "      <td>0</td>\n",
              "      <td>7.968659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>7.968659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>Soylent</td>\n",
              "      <td>0</td>\n",
              "      <td>7.968659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>company</td>\n",
              "      <td>-0.010514</td>\n",
              "      <td>7.958145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Charlton</td>\n",
              "      <td>0</td>\n",
              "      <td>7.958145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Heston</td>\n",
              "      <td>0</td>\n",
              "      <td>7.958145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>in</td>\n",
              "      <td>0.0230871</td>\n",
              "      <td>7.981232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>great</td>\n",
              "      <td>0.722768</td>\n",
              "      <td>8.704001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>shape</td>\n",
              "      <td>0.321271</td>\n",
              "      <td>9.025271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>plays</td>\n",
              "      <td>0.474287</td>\n",
              "      <td>9.499559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>cop</td>\n",
              "      <td>-0.344885</td>\n",
              "      <td>9.154674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>investigating</td>\n",
              "      <td>0</td>\n",
              "      <td>9.154674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>9.154674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>murder</td>\n",
              "      <td>0.293505</td>\n",
              "      <td>9.448179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>9.448179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>one</td>\n",
              "      <td>0</td>\n",
              "      <td>9.448179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>9.448179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>Soylent</td>\n",
              "      <td>0</td>\n",
              "      <td>9.448179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>most</td>\n",
              "      <td>0</td>\n",
              "      <td>9.448179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>eminent</td>\n",
              "      <td>0</td>\n",
              "      <td>9.448179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>executives</td>\n",
              "      <td>0</td>\n",
              "      <td>9.448179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>9.448179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>he</td>\n",
              "      <td>0.0729216</td>\n",
              "      <td>9.521100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>stumbles</td>\n",
              "      <td>0</td>\n",
              "      <td>9.521100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>upon</td>\n",
              "      <td>0.129652</td>\n",
              "      <td>9.650752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>scandals</td>\n",
              "      <td>0</td>\n",
              "      <td>9.650752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>9.650752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>dark</td>\n",
              "      <td>0.35231</td>\n",
              "      <td>10.003062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>secrets</td>\n",
              "      <td>0.993273</td>\n",
              "      <td>10.996335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>The</td>\n",
              "      <td>0</td>\n",
              "      <td>10.996335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>script</td>\n",
              "      <td>-0.674526</td>\n",
              "      <td>10.321809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>10.321809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>little</td>\n",
              "      <td>0</td>\n",
              "      <td>10.321809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>over</td>\n",
              "      <td>0</td>\n",
              "      <td>10.321809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>sentimental</td>\n",
              "      <td>0.77295</td>\n",
              "      <td>11.094758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>at</td>\n",
              "      <td>-0.0677591</td>\n",
              "      <td>11.026999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>times</td>\n",
              "      <td>0.249489</td>\n",
              "      <td>11.276489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>11.276489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>11.276489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>climax</td>\n",
              "      <td>0.0393537</td>\n",
              "      <td>11.315843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>doesn</td>\n",
              "      <td>-0.239584</td>\n",
              "      <td>11.076258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>really</td>\n",
              "      <td>-0.117688</td>\n",
              "      <td>10.958570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>come</td>\n",
              "      <td>-0.0191481</td>\n",
              "      <td>10.939422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>as</td>\n",
              "      <td>0.0902697</td>\n",
              "      <td>11.029691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>big</td>\n",
              "      <td>0.0250209</td>\n",
              "      <td>11.054712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>surprise</td>\n",
              "      <td>0.335282</td>\n",
              "      <td>11.389994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>still</td>\n",
              "      <td>0</td>\n",
              "      <td>11.389994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>11.389994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>atmosphere</td>\n",
              "      <td>0.693955</td>\n",
              "      <td>12.083948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>12.083948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>very</td>\n",
              "      <td>0</td>\n",
              "      <td>12.083948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>tense</td>\n",
              "      <td>0.757083</td>\n",
              "      <td>12.841032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>12.841032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>uncanny</td>\n",
              "      <td>0</td>\n",
              "      <td>12.841032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>The</td>\n",
              "      <td>0</td>\n",
              "      <td>12.841032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>riot</td>\n",
              "      <td>0</td>\n",
              "      <td>12.841032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>sequence</td>\n",
              "      <td>0.125435</td>\n",
              "      <td>12.966467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>12.966467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>truly</td>\n",
              "      <td>0.354339</td>\n",
              "      <td>13.320806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>grueling</td>\n",
              "      <td>0</td>\n",
              "      <td>13.320806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>13.320806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>easily</td>\n",
              "      <td>0.187916</td>\n",
              "      <td>13.508722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>one</td>\n",
              "      <td>0</td>\n",
              "      <td>13.508722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>13.508722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>13.508722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>most</td>\n",
              "      <td>0</td>\n",
              "      <td>13.508722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>macabre</td>\n",
              "      <td>0</td>\n",
              "      <td>13.508722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>moments</td>\n",
              "      <td>0.241369</td>\n",
              "      <td>13.750091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>in</td>\n",
              "      <td>0</td>\n",
              "      <td>13.750091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>13.750091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>cinema</td>\n",
              "      <td>0.359961</td>\n",
              "      <td>14.110052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>Edward</td>\n",
              "      <td>0.319152</td>\n",
              "      <td>14.429205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>Robinson</td>\n",
              "      <td>0</td>\n",
              "      <td>14.429205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>14.429205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>ultimately</td>\n",
              "      <td>0.173832</td>\n",
              "      <td>14.603036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>impressive</td>\n",
              "      <td>0.668742</td>\n",
              "      <td>15.271778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>in</td>\n",
              "      <td>0</td>\n",
              "      <td>15.271778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>his</td>\n",
              "      <td>0.171887</td>\n",
              "      <td>15.443665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>last</td>\n",
              "      <td>0.0906977</td>\n",
              "      <td>15.534363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>role</td>\n",
              "      <td>0.410414</td>\n",
              "      <td>15.944777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>15.944777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>there</td>\n",
              "      <td>-0.188949</td>\n",
              "      <td>15.755828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>great</td>\n",
              "      <td>0</td>\n",
              "      <td>15.755828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>but</td>\n",
              "      <td>-0.0339598</td>\n",
              "      <td>15.721868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>too</td>\n",
              "      <td>-0.092489</td>\n",
              "      <td>15.629379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>modest</td>\n",
              "      <td>0</td>\n",
              "      <td>15.629379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>supportive</td>\n",
              "      <td>0</td>\n",
              "      <td>15.629379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>role</td>\n",
              "      <td>0</td>\n",
              "      <td>15.629379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>for</td>\n",
              "      <td>0</td>\n",
              "      <td>15.629379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>Joseph</td>\n",
              "      <td>0.596977</td>\n",
              "      <td>16.226356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>Cotton</td>\n",
              "      <td>0</td>\n",
              "      <td>16.226356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>Baron</td>\n",
              "      <td>0</td>\n",
              "      <td>16.226356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>Blood</td>\n",
              "      <td>-0.426931</td>\n",
              "      <td>15.799425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>The</td>\n",
              "      <td>0</td>\n",
              "      <td>15.799425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>Abominable</td>\n",
              "      <td>0</td>\n",
              "      <td>15.799425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>Dr</td>\n",
              "      <td>0.0594212</td>\n",
              "      <td>15.858846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>Phibes</td>\n",
              "      <td>0</td>\n",
              "      <td>15.858846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>THIS</td>\n",
              "      <td>0</td>\n",
              "      <td>15.858846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>15.858846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>Science</td>\n",
              "      <td>0</td>\n",
              "      <td>15.858846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>Fiction</td>\n",
              "      <td>0</td>\n",
              "      <td>15.858846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>in</td>\n",
              "      <td>0</td>\n",
              "      <td>15.858846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>my</td>\n",
              "      <td>0.0387646</td>\n",
              "      <td>15.897611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>book</td>\n",
              "      <td>0.0786979</td>\n",
              "      <td>15.976309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>nightmarish</td>\n",
              "      <td>0</td>\n",
              "      <td>15.976309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>and</td>\n",
              "      <td>0</td>\n",
              "      <td>15.976309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>inevitable</td>\n",
              "      <td>0.456303</td>\n",
              "      <td>16.432612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>fade</td>\n",
              "      <td>0</td>\n",
              "      <td>16.432612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>for</td>\n",
              "      <td>0</td>\n",
              "      <td>16.432612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>humanity</td>\n",
              "      <td>0.373403</td>\n",
              "      <td>16.806014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>No</td>\n",
              "      <td>-0.42969</td>\n",
              "      <td>16.376324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>fancy</td>\n",
              "      <td>-0.322702</td>\n",
              "      <td>16.053622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>space</td>\n",
              "      <td>-0.142335</td>\n",
              "      <td>15.911287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>ships</td>\n",
              "      <td>0</td>\n",
              "      <td>15.911287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>with</td>\n",
              "      <td>0</td>\n",
              "      <td>15.911287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>hairy</td>\n",
              "      <td>0</td>\n",
              "      <td>15.911287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>monsters</td>\n",
              "      <td>-0.806329</td>\n",
              "      <td>15.104958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>attacking</td>\n",
              "      <td>0</td>\n",
              "      <td>15.104958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>our</td>\n",
              "      <td>0.281753</td>\n",
              "      <td>15.386711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>planet</td>\n",
              "      <td>-0.0354228</td>\n",
              "      <td>15.351288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              word contribution to llr  cumulative llr\n",
              "0          Soylent                   0        0.003413\n",
              "1            Green          -0.0958766       -0.092463\n",
              "2               is           0.0368863       -0.055577\n",
              "3              one           0.0241627       -0.031414\n",
              "4               of           0.0072271       -0.024187\n",
              "5              the          0.00677751       -0.017410\n",
              "6             best            0.617731        0.600321\n",
              "7              and           0.0201221        0.620443\n",
              "8             most             0.15582        0.776263\n",
              "9       disturbing           0.0842356        0.860498\n",
              "10         science           -0.325185        0.535313\n",
              "11         fiction            0.318557        0.853870\n",
              "12          movies           -0.100125        0.753745\n",
              "13              of                   0        0.753745\n",
              "14             the                   0        0.753745\n",
              "15              70         -0.00692133        0.746824\n",
              "16             and                   0        0.746824\n",
              "17           still            0.322512        1.069336\n",
              "18            very            0.257975        1.327310\n",
              "19      persuasive                   0        1.327310\n",
              "20            even           -0.340877        0.986433\n",
              "21              by           0.0554235        1.041857\n",
              "22           today             1.02535        2.067204\n",
              "23       standards          -0.0933059        1.973898\n",
              "24        Although            0.324969        2.298867\n",
              "25          flawed            0.390736        2.689604\n",
              "26             and                   0        2.689604\n",
              "27          little           0.0654876        2.755091\n",
              "28           dated            0.625817        3.380908\n",
              "29             the                   0        3.380908\n",
              "30     apocalyptic                   0        3.380908\n",
              "31           touch            0.517603        3.898511\n",
              "32             and                   0        3.898511\n",
              "33             the                   0        3.898511\n",
              "34   environmental                   0        3.898511\n",
              "35         premise           -0.779952        3.118559\n",
              "36         typical            0.272618        3.391177\n",
              "37             for         -0.00204234        3.389134\n",
              "38            that          -0.0329541        3.356180\n",
              "39            time           0.0175122        3.373692\n",
              "40           still                   0        3.373692\n",
              "41            feel            0.161934        3.535626\n",
              "42            very                   0        3.535626\n",
              "43      unsettling                   0        3.535626\n",
              "44             and                   0        3.535626\n",
              "45         thought          -0.0587743        3.476852\n",
              "46       provoking            0.736739        4.213590\n",
              "47            This          -0.0299498        4.183641\n",
              "48            film           0.0486534        4.232294\n",
              "49         quality          -0.0883115        4.143983\n",
              "50           level         -0.00519868        4.138784\n",
              "51       surpasses                   0        4.138784\n",
              "52             the                   0        4.138784\n",
              "53        majority            0.109725        4.248509\n",
              "54              of                   0        4.248509\n",
              "55    contemporary            0.485515        4.734024\n",
              "56              SF                   0        4.734024\n",
              "57          flicks           -0.443292        4.290732\n",
              "58         because           -0.174292        4.116440\n",
              "59              of                   0        4.116440\n",
              "60             its            0.195074        4.311515\n",
              "61          strong            0.801277        5.112792\n",
              "62            cast            0.217471        5.330263\n",
              "63             and                   0        5.330263\n",
              "64            some          -0.0881129        5.242150\n",
              "65         intense             1.03979        6.281942\n",
              "66       sequences            0.107703        6.389646\n",
              "67            that                   0        6.389646\n",
              "68      personally            0.283764        6.673410\n",
              "69        consider           0.0833176        6.756728\n",
              "70         classic            0.574762        7.331489\n",
              "71             The                   0        7.331489\n",
              "72             New            0.243907        7.575397\n",
              "73            York            0.618375        8.193771\n",
              "74              of                   0        8.193771\n",
              "75            2022                   0        8.193771\n",
              "76              is                   0        8.193771\n",
              "77      depressing          -0.0860042        8.107767\n",
              "78           place           0.0895746        8.197342\n",
              "79              to         -0.00743777        8.189904\n",
              "80              be           -0.100377        8.089527\n",
              "81           alive           0.0258257        8.115353\n",
              "82            with           0.0378539        8.153207\n",
              "83            over          -0.0636426        8.089564\n",
              "84      population            0.197833        8.287397\n",
              "85    unemployment                   0        8.287397\n",
              "86              an           0.0561757        8.343573\n",
              "87       unhealthy                   0        8.343573\n",
              "88         climate                   0        8.343573\n",
              "89             and                   0        8.343573\n",
              "90             the                   0        8.343573\n",
              "91           total           -0.841663        7.501910\n",
              "92        scarcity                   0        7.501910\n",
              "93              of                   0        7.501910\n",
              "94           every          -0.0315476        7.470363\n",
              "95           vital                   0        7.470363\n",
              "96            food          -0.0919573        7.378405\n",
              "97         product           -0.241402        7.137003\n",
              "98             The                   0        7.137003\n",
              "99            only           -0.229573        6.907430\n",
              "100           form             0.43296        7.340390\n",
              "101             of                   0        7.340390\n",
              "102           food                   0        7.340390\n",
              "103      available            0.628269        7.968659\n",
              "104             is                   0        7.968659\n",
              "105      synthetic                   0        7.968659\n",
              "106            and                   0        7.968659\n",
              "107    distributed                   0        7.968659\n",
              "108             by                   0        7.968659\n",
              "109            the                   0        7.968659\n",
              "110        Soylent                   0        7.968659\n",
              "111        company           -0.010514        7.958145\n",
              "112       Charlton                   0        7.958145\n",
              "113         Heston                   0        7.958145\n",
              "114             in           0.0230871        7.981232\n",
              "115          great            0.722768        8.704001\n",
              "116          shape            0.321271        9.025271\n",
              "117          plays            0.474287        9.499559\n",
              "118            cop           -0.344885        9.154674\n",
              "119  investigating                   0        9.154674\n",
              "120            the                   0        9.154674\n",
              "121         murder            0.293505        9.448179\n",
              "122             of                   0        9.448179\n",
              "123            one                   0        9.448179\n",
              "124             of                   0        9.448179\n",
              "125        Soylent                   0        9.448179\n",
              "126           most                   0        9.448179\n",
              "127        eminent                   0        9.448179\n",
              "128     executives                   0        9.448179\n",
              "129            and                   0        9.448179\n",
              "130             he           0.0729216        9.521100\n",
              "131       stumbles                   0        9.521100\n",
              "132           upon            0.129652        9.650752\n",
              "133       scandals                   0        9.650752\n",
              "134            and                   0        9.650752\n",
              "135           dark             0.35231       10.003062\n",
              "136        secrets            0.993273       10.996335\n",
              "137            The                   0       10.996335\n",
              "138         script           -0.674526       10.321809\n",
              "139             is                   0       10.321809\n",
              "140         little                   0       10.321809\n",
              "141           over                   0       10.321809\n",
              "142    sentimental             0.77295       11.094758\n",
              "143             at          -0.0677591       11.026999\n",
              "144          times            0.249489       11.276489\n",
              "145            and                   0       11.276489\n",
              "146            the                   0       11.276489\n",
              "147         climax           0.0393537       11.315843\n",
              "148          doesn           -0.239584       11.076258\n",
              "149         really           -0.117688       10.958570\n",
              "150           come          -0.0191481       10.939422\n",
              "151             as           0.0902697       11.029691\n",
              "152            big           0.0250209       11.054712\n",
              "153       surprise            0.335282       11.389994\n",
              "154          still                   0       11.389994\n",
              "155            the                   0       11.389994\n",
              "156     atmosphere            0.693955       12.083948\n",
              "157             is                   0       12.083948\n",
              "158           very                   0       12.083948\n",
              "159          tense            0.757083       12.841032\n",
              "160            and                   0       12.841032\n",
              "161        uncanny                   0       12.841032\n",
              "162            The                   0       12.841032\n",
              "163           riot                   0       12.841032\n",
              "164       sequence            0.125435       12.966467\n",
              "165             is                   0       12.966467\n",
              "166          truly            0.354339       13.320806\n",
              "167       grueling                   0       13.320806\n",
              "168            and                   0       13.320806\n",
              "169         easily            0.187916       13.508722\n",
              "170            one                   0       13.508722\n",
              "171             of                   0       13.508722\n",
              "172            the                   0       13.508722\n",
              "173           most                   0       13.508722\n",
              "174        macabre                   0       13.508722\n",
              "175        moments            0.241369       13.750091\n",
              "176             in                   0       13.750091\n",
              "177             70                   0       13.750091\n",
              "178         cinema            0.359961       14.110052\n",
              "179         Edward            0.319152       14.429205\n",
              "180       Robinson                   0       14.429205\n",
              "181             is                   0       14.429205\n",
              "182     ultimately            0.173832       14.603036\n",
              "183     impressive            0.668742       15.271778\n",
              "184             in                   0       15.271778\n",
              "185            his            0.171887       15.443665\n",
              "186           last           0.0906977       15.534363\n",
              "187           role            0.410414       15.944777\n",
              "188            and                   0       15.944777\n",
              "189          there           -0.188949       15.755828\n",
              "190          great                   0       15.755828\n",
              "191            but          -0.0339598       15.721868\n",
              "192            too           -0.092489       15.629379\n",
              "193         modest                   0       15.629379\n",
              "194     supportive                   0       15.629379\n",
              "195           role                   0       15.629379\n",
              "196            for                   0       15.629379\n",
              "197         Joseph            0.596977       16.226356\n",
              "198         Cotton                   0       16.226356\n",
              "199          Baron                   0       16.226356\n",
              "200          Blood           -0.426931       15.799425\n",
              "201            The                   0       15.799425\n",
              "202     Abominable                   0       15.799425\n",
              "203             Dr           0.0594212       15.858846\n",
              "204         Phibes                   0       15.858846\n",
              "205           THIS                   0       15.858846\n",
              "206             is                   0       15.858846\n",
              "207        Science                   0       15.858846\n",
              "208        Fiction                   0       15.858846\n",
              "209             in                   0       15.858846\n",
              "210             my           0.0387646       15.897611\n",
              "211           book           0.0786979       15.976309\n",
              "212    nightmarish                   0       15.976309\n",
              "213            and                   0       15.976309\n",
              "214     inevitable            0.456303       16.432612\n",
              "215           fade                   0       16.432612\n",
              "216            for                   0       16.432612\n",
              "217       humanity            0.373403       16.806014\n",
              "218             No            -0.42969       16.376324\n",
              "219          fancy           -0.322702       16.053622\n",
              "220          space           -0.142335       15.911287\n",
              "221          ships                   0       15.911287\n",
              "222           with                   0       15.911287\n",
              "223          hairy                   0       15.911287\n",
              "224       monsters           -0.806329       15.104958\n",
              "225      attacking                   0       15.104958\n",
              "226            our            0.281753       15.386711\n",
              "227         planet          -0.0354228       15.351288"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0JqM6xB6sy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b917fe22-06f5-40b9-aa4f-ab66967cea17"
      },
      "source": [
        "# as a sanity check, let's see what sklearn gives us\n",
        "probs = model.predict_log_proba(vectorizer.transform([df['review'].iloc[review_to_analyze_index]]).todense())\n",
        "print(probs[0][pos_index] - probs[0][neg_index])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15.351287983285033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSPdxIp8Linu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}