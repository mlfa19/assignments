{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "All the News Sample Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%202/05/All_the_News_Sample_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLdvDGsm80P7",
        "colab_type": "text"
      },
      "source": [
        "# All The News Dataset\n",
        "\n",
        "If you want to spend more time on the modeling, experimentation, and interpretation portion of the mini-project, you might consider using this dataset as we have cleaned up the data and tokenized it so you can use it with Na&iuml;ve Bayes.\n",
        "\n",
        "Here is the description of the dataset from the person who put it together.\n",
        "\n",
        "> I wanted to see how articles clustered together if the articles were rendered into document-term matrices---would there be greater affinity among political affiliations, or medium, subject matter, etc. The data was scraped using BeautifulSoup and stored in Sqlite, but I've chopped it up into three separate CSVs here, because the entire Sqlite database came out to about 1.2 gb, beyond Kaggle's max.\n",
        ">\n",
        "> The publications include the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. Sampling wasn't quite scientific; I chose publications based on my familiarity of the domain and tried to get a range of political alignments, as well as a mix of print and digital publications. By count, the publications break down accordingly:\n",
        ">\n",
        "> The data primarily falls between the years of 2016 and July 2017, although there is a not-insignificant number of articles from 2015, and a possibly insignificant number from before then."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjxFnptfEQjh",
        "colab_type": "text"
      },
      "source": [
        "## Downloading and Parsing the Data\n",
        "\n",
        "We have put one of the three csv files on Google Drive to make it easy / fast to download.  If you want the rest of the data, we leave it up to you to modify the notebook to handle that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPnYVR0R8uE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2b3613c-ac8e-4743-fbd1-9b84104f6c64"
      },
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=1T8V87Hdz2IvhKjzwzKyLWA4vI6sA2wTX&export=download',\n",
        "               'articles1.csv',\n",
        "               quiet=False)\n",
        "df = pd.read_csv('articles1.csv')\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?authuser=0&id=1T8V87Hdz2IvhKjzwzKyLWA4vI6sA2wTX&export=download\n",
            "To: /content/articles1.csv\n",
            "204MB [00:01, 199MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>publication</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>17283</td>\n",
              "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Carl Hulse</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>17284</td>\n",
              "      <td>Rift Between Officers and Residents as Killing...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Benjamin Mueller and Al Baker</td>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>After the bullet shells get counted, the blood...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17285</td>\n",
              "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Margalit Fox</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>17286</td>\n",
              "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>William McDonald</td>\n",
              "      <td>2017-04-10</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>17287</td>\n",
              "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Choe Sang-Hun</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>53287</td>\n",
              "      <td>73465</td>\n",
              "      <td>Rex Tillerson Says Climate Change Is Real, but …</td>\n",
              "      <td>Atlantic</td>\n",
              "      <td>Robinson Meyer</td>\n",
              "      <td>2017-01-11</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As chairman and CEO of ExxonMobil, Rex Tillers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>53288</td>\n",
              "      <td>73466</td>\n",
              "      <td>The Biggest Intelligence Questions Raised by t...</td>\n",
              "      <td>Atlantic</td>\n",
              "      <td>Amy Zegart</td>\n",
              "      <td>2017-01-11</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I’ve spent nearly 20 years looking at intellig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>53289</td>\n",
              "      <td>73467</td>\n",
              "      <td>Trump Announces Plan That Does Little to Resol...</td>\n",
              "      <td>Atlantic</td>\n",
              "      <td>Jeremy Venook</td>\n",
              "      <td>2017-01-11</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Donald Trump will not be taking necessary st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>53290</td>\n",
              "      <td>73468</td>\n",
              "      <td>Dozens of For-Profit Colleges Could Soon Close</td>\n",
              "      <td>Atlantic</td>\n",
              "      <td>Emily DeRuy</td>\n",
              "      <td>2017-01-11</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Dozens of   colleges could be forced to close ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>53291</td>\n",
              "      <td>73469</td>\n",
              "      <td>The Milky Way’s Stolen Stars</td>\n",
              "      <td>Atlantic</td>\n",
              "      <td>Marina Koren</td>\n",
              "      <td>2017-01-11</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The force of gravity can be described using a ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ...                                            content\n",
              "0               0  ...  WASHINGTON  —   Congressional Republicans have...\n",
              "1               1  ...  After the bullet shells get counted, the blood...\n",
              "2               2  ...  When Walt Disney’s “Bambi” opened in 1942, cri...\n",
              "3               3  ...  Death may be the great equalizer, but it isn’t...\n",
              "4               4  ...  SEOUL, South Korea  —   North Korea’s leader, ...\n",
              "...           ...  ...                                                ...\n",
              "49995       53287  ...  As chairman and CEO of ExxonMobil, Rex Tillers...\n",
              "49996       53288  ...  I’ve spent nearly 20 years looking at intellig...\n",
              "49997       53289  ...    Donald Trump will not be taking necessary st...\n",
              "49998       53290  ...  Dozens of   colleges could be forced to close ...\n",
              "49999       53291  ...  The force of gravity can be described using a ...\n",
              "\n",
              "[50000 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T0Kd5yTEfGQ",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing Articles to Words\n",
        "\n",
        "In order to tokenize the article into words in this example we will use [NLTK](https://www.nltk.org/)'s built-in word tokenizer (you can use your own method, such as the one we used in assignment 4 if you like).  NLTK stands for the Natural Language Toolkit and there are some really cool things built into it for dealing with text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBt70Oj49ZXT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "88d44dfb-ddeb-4636-9d6d-0d44f5bc7918"
      },
      "source": [
        "# NLTK has a built-in module for extracting words from text.\n",
        "# This takes a few minutes to run, so be patient.\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def remove_punctuation(article):\n",
        "    # substitute in a regular apostrophe for '’' to word with word_tokenize\n",
        "    article = article.replace('’', \"'\")\n",
        "    tokens = nltk.tokenize.word_tokenize(article)\n",
        "    words = list(filter(lambda w: any(x.isalpha() for x in w), tokens))\n",
        "    return \" \".join(words)\n",
        "\n",
        "df['content_no_punctuation'] = df['content'].map(remove_punctuation)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY2eNjn6-Gai",
        "colab_type": "text"
      },
      "source": [
        "Here's what the code article text looks like after the words have all been separated and punctuation has been removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hz945qE9hVk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "0eca1534-d866-4079-8404-c2ffbb8cc601"
      },
      "source": [
        "df['content_no_punctuation']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        WASHINGTON Congressional Republicans have a ne...\n",
              "1        After the bullet shells get counted the blood ...\n",
              "2        When Walt Disney 's Bambi opened in critics pr...\n",
              "3        Death may be the great equalizer but it is n't...\n",
              "4        SEOUL South Korea North Korea 's leader Kim sa...\n",
              "                               ...                        \n",
              "49995    As chairman and CEO of ExxonMobil Rex Tillerso...\n",
              "49996    I 've spent nearly years looking at intelligen...\n",
              "49997    Donald Trump will not be taking necessary step...\n",
              "49998    Dozens of colleges could be forced to close in...\n",
              "49999    The force of gravity can be described using a ...\n",
              "Name: content_no_punctuation, Length: 50000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWsjeZnf-PXX",
        "colab_type": "text"
      },
      "source": [
        "At this point you can take the dataset in many different directions.  If you want to use ngrams to analyze the data, you can use sklearn's `CountVectorizer` as we did in the previous notebooks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjyXQFOm-qnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True, ngram_range=(1,2), min_df=20, tokenizer=lambda x: x.split(' '))\n",
        "X = vectorizer.fit_transform(df['content_no_punctuation'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcVaGa4X-uMm",
        "colab_type": "text"
      },
      "source": [
        "This would allow you to then fit a model with Na&iuml;ve Bayes.  Sample code is available for this upon request, but we think there is enough guidance from previous assignments and that it would be a good exercise to write the code on your own."
      ]
    }
  ]
}