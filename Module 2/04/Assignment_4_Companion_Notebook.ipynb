{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4 Companion Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jl-WOh0s173d",
        "s3SSv1_zFW7Y"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%202/04/Assignment_4_Companion_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw7g0QF3UbU9",
        "colab_type": "text"
      },
      "source": [
        "# More Na&iuml;ve Bayes with Smoothing and N-Grams\n",
        "\n",
        "***Abstract***\n",
        "\n",
        "In this notebook you'll be explanding on our previous implementation of the Na&iuml;ve Bayes algorithm and exploring the fun new world of bigrams which are \"pretty useful\". We'll practice some techniques for manipulating text and take advantage of some of sklearn's built-in implementations. This notebook will help you practice some tools to use for a sequence learning mini-project.\n",
        "\n",
        "\n",
        "***Takeaways***\n",
        "\n",
        "*   When we use Na&iuml;ve Bayes, it's beneficial to use \"smoothing\" to avoid the problems that arise because our model can run into problems (probabilities of 0, or values of negative infinity) when a word shows up in our test set that was not in our training set.\n",
        "*   In English (and most other languages), word order matters (\"people eat\" is different than \"eat people\"). Therefore, it's often useful to included some information about word order in our model. If we include complete information about word order (e.g., this word showed up 100 words before that word), our model would require extreme amounts of memory. We can typically get a way with a little information about word order, such as with bigrams, which are just pairs of words.\n",
        "*   We can use bigrams (or other ngrams) in various applications in natural language processing including text classification, sequence prediction, word generation, spell checking. These applications also extend beyond actual words or letters, and these techniques can really be applied to any sequences of items (e.g. series of coin flips).  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KbrKo8jWMNn",
        "colab_type": "text"
      },
      "source": [
        "##Sentiment Analysis with Na&iuml;ve Bayes (now with smoothing)\n",
        "Building on the last assignment, we'll be focusing on predicting the sentiment of a movie review from IMDB based on the text of the movie review.  This dataset is one that was originally used in a Kaggle competition called [Bag of Words meets Bag of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial). \n",
        "\n",
        "Again, the [data](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) consist of the following.\n",
        "\n",
        "> The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
        "\n",
        "Our goal will be to see if we can learn a model, using Na&iuml;ve Bayes on a training set to accurately estimate sentiment of new reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTUjkvrNV1_W",
        "colab_type": "code",
        "outputId": "d566d3a4-d053-40b6-ec29-f53d5ae7bd8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "# Import our movie review data (from last assignment)\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download',\n",
        "               'labeledTrainData.tsv',\n",
        "               quiet=False)\n",
        "df = pd.read_csv('labeledTrainData.tsv', header=0, delimiter='\\t')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download\n",
            "To: /content/labeledTrainData.tsv\n",
            "33.6MB [00:00, 56.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7759_3</td>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3630_4</td>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9495_8</td>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>3453_3</td>\n",
              "      <td>0</td>\n",
              "      <td>It seems like more consideration has gone into...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>5064_1</td>\n",
              "      <td>0</td>\n",
              "      <td>I don't believe they made this film. Completel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>10905_3</td>\n",
              "      <td>0</td>\n",
              "      <td>Guy is a loser. Can't get girls, needs to buil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>10194_3</td>\n",
              "      <td>0</td>\n",
              "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>8478_8</td>\n",
              "      <td>1</td>\n",
              "      <td>I saw this movie as a child and it broke my he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  sentiment                                             review\n",
              "0       5814_8          1  With all this stuff going down at the moment w...\n",
              "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
              "3       3630_4          0  It must be assumed that those who praised this...\n",
              "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
              "...        ...        ...                                                ...\n",
              "24995   3453_3          0  It seems like more consideration has gone into...\n",
              "24996   5064_1          0  I don't believe they made this film. Completel...\n",
              "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
              "24998  10194_3          0  This 30 minute documentary Buñuel made in the ...\n",
              "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
              "\n",
              "[25000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QYpM8qiXFDf",
        "colab_type": "text"
      },
      "source": [
        "Just like last time, we will convert from text to a **bag of words** representation using scikit learn's built-in [count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Last time, we only included words in our feature vector if they occur in at least 100 reviews. Now we reduce this limitation, so that included words only need to appear in at least 20 reviews. Note the the previous shape of X was:\n",
        "X.shape (25000, 3833), and now it should have many more features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ0UE8IoW8KN",
        "colab_type": "code",
        "outputId": "16ceca98-d30e-467f-ffe7-051893c76b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = np.array(df['sentiment'])\n",
        "\n",
        "dfX_train, dfX_test, y_train, y_test = train_test_split(df['review'], y)\n",
        "print(\"df_train.shape\",dfX_train.shape)\n",
        "print(\"y_train.shape\",y_train.shape)\n",
        "print(\"dfX_test.shape\",dfX_test.shape)\n",
        "print(\"y_test.shape\",y_test.shape)\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True, min_df = 20) #convert a collection of text documents into a matrix of token counts\n",
        "vectorizer.fit(dfX_train) #learn a vocabulary dictionary of all tokens in the raw documents\n",
        "\n",
        "X_train = vectorizer.transform(dfX_train).todense() #transform to a document-term matrix\n",
        "X_test = vectorizer.transform(dfX_test).todense()\n",
        "print(\"X_test.shape\",X_test.shape)\n",
        "print(\"X_train.shape\", X_train.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_train.shape (18750,)\n",
            "y_train.shape (18750,)\n",
            "dfX_test.shape (6250,)\n",
            "y_test.shape (6250,)\n",
            "X_test.shape (6250, 10094)\n",
            "X_train.shape (18750, 10094)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ed5j_vlr-Cu",
        "colab_type": "text"
      },
      "source": [
        "We also split the data into training and test right from the start. We'll check to make sure our data is organized properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYaufLj5g65W",
        "colab_type": "code",
        "outputId": "07fc8c2c-27bc-480b-ba90-1e4f8340795b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# Looking at a review to make sure things work\n",
        "reviews_wrapped = dfX_train.str.wrap(80)\n",
        "terrible_index = vectorizer.get_feature_names().index('terrible')\n",
        "print(\"terrible occurs in\", X_train[y_train==1, terrible_index].mean(), \"for Y=1\")\n",
        "print(\"terrible occurs in\", X_train[y_train==0, terrible_index].mean(), \"for Y=0\")\n",
        "print(reviews_wrapped.iloc[1]) # Just in case you want to read a random review"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "terrible occurs in 0.017667092379736057 for Y=1\n",
            "terrible occurs in 0.09193927731451786 for Y=0\n",
            "I think this piece of garbage is the best proof that good ideas can be\n",
            "destroyed, why all the American animators thinks that the kids this days wants\n",
            "stupid GI JOE versions of good stories??? the Looney Tunes are some of the most\n",
            "beloved characters in history, but they weren't created to be Xtreme, i mean\n",
            "come on!!! Tiny Toons was a great example of how an old idea can be updated\n",
            "without loosing it's original charm, but this piece of garbage is just an\n",
            "example of stupid corporate decisions that only wants to create a cheap idiotic\n",
            "show that kids will love because hey!!! kids loves superheroes right??? the\n",
            "whole show is only a waste of time in which we see the new versions of the\n",
            "Looney Tunes but this time in superhero form, this doesn't sound too bad but the\n",
            "problem is that this show tries too hard to copy series like batman the animated\n",
            "series, or the new justice league, the result??? bad copies of flash (the road\n",
            "runner) or superman (who else??? bugs bunny) the problem is that Looney Tunes\n",
            "weren't meant to be dramatic, the were supposed to be funny!!!! as i said before\n",
            "this series sucks, and many people wonders why anime is taking all over the\n",
            "world??? this show tries to be dramatic and action packed, but that's something\n",
            "that few series and anime are able to do, if you want to see a good upgrade of\n",
            "an old show watch Tiny Toons, that's an example that it's possible to bring back\n",
            "to life old characters, but with a good story and respecting the original roots.\n",
            "too bad that show is already dead, another corporate wise decision i suppose.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7esYTmjpfqgr",
        "colab_type": "text"
      },
      "source": [
        "### Fitting the Parameters of the Model & Making Predictions\n",
        "\n",
        "As you may recall from the last assignment:\n",
        "\n",
        "What we see from looking at the log odds ratio equation is that in order to apply the model we must have an estimate of the following probabilities.\n",
        "* $p(Y=0)$\n",
        "* $p(Y=1)$\n",
        "* $p(X_i = x_i | Y=0)$ (for $i$ from $1$ to $d$)\n",
        "* $p(X_i = x_i | Y=1)$ (for $i$ from $1$ to $d$)\n",
        "\n",
        "The MLE problem in the earlier part of the assignment gave a more formal justification of this process. \n",
        "\n",
        "We'll start with the solution from the last assignment. Here, we count the number of times the event occurs across the dataset in order to estimate a probability.\n",
        "\n",
        "For instance, if we want to estimate $p(Y=0)$ we would count the number of instances in the dataset where $Y=0$ and divide that by the total number of instances in the dataset.  If we wanted to estimated $p(X_i = 1 | Y = 0)$ (suppose X_i represents the word \"terrible\") we would count the number of reviews that included the word terrible and had sentiment 0 and divide that by the number of reviews that were sentiment 0. Remember that $p(A|B) = \\frac{p(A, B)}{p(B)}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvXgkDmK60vI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Essentially same functions as solutions from previous assignment\n",
        "def fit_nb_model(X, y):\n",
        "    X_1 = np.asarray(X[y == 1, :]) # all reviews with sentiment 1\n",
        "    X_0 = np.asarray(X[y == 0, :])\n",
        "    return y.mean(), 1 - y.mean(), X_1.mean(axis=0), X_0.mean(axis=0)\n",
        "\n",
        "def get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X):\n",
        "    \"\"\" Predict the labels for the data X given the Naive Bayes model \"\"\"\n",
        "    log_odds_ratios = np.zeros(X.shape[0])\n",
        "    for i in range(X.shape[0]): # loop over data points\n",
        "        if i%(X.shape[0]/10) == 0: print(\"progress\", i/X.shape[0])\n",
        "        log_odds_ratios[i] += np.log(p_y_1) - np.log(p_y_0)\n",
        "        for j in range(X.shape[1]): #loop over words\n",
        "            if X[i, j] == 1: #if this example includes word j\n",
        "                log_odds_ratios[i] += np.log(p_x_y_1[j]) - np.log(p_x_y_0[j])\n",
        "            else: \n",
        "                log_odds_ratios[i] += np.log(1 - p_x_y_1[j]) - np.log(1 - p_x_y_0[j])\n",
        "    return (log_odds_ratios >= 0).astype(np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUouXFCjnxmG",
        "colab_type": "text"
      },
      "source": [
        "## Testing the Accuracy of the Model from Assignment 3\n",
        "The following lines of code run the solution function (above) and calculate the accuracy. We're just using the first 100 entries for faster computation. \n",
        "\n",
        "**What do you observe happens when you run this?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNU-v-DzjcY-",
        "colab_type": "code",
        "outputId": "4a45b268-4b34-4396-b632-4967acd02e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model(X_train, y_train)\n",
        "y_pred_train = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_train[:100,:])\n",
        "print(\"Train accuracy is\", (y_pred_train == y_train[:100]).astype(np.float).mean()) #also only need to compare first 100 y\n",
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
        "print(\"Test accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 0.0\n",
            "progress 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "progress 0.2\n",
            "progress 0.3\n",
            "progress 0.4\n",
            "progress 0.5\n",
            "progress 0.6\n",
            "progress 0.7\n",
            "progress 0.8\n",
            "progress 0.9\n",
            "Train accuracy is 0.91\n",
            "progress 0.0\n",
            "progress 0.1\n",
            "progress 0.2\n",
            "progress 0.3\n",
            "progress 0.4\n",
            "progress 0.5\n",
            "progress 0.6\n",
            "progress 0.7\n",
            "progress 0.8\n",
            "progress 0.9\n",
            "Test accuracy is 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TXmSxXgtqxQ",
        "colab_type": "text"
      },
      "source": [
        "You may have noticed a \"RuntimeWarning: divide by zero encountered in log\". It is common practice to use something call [Laplace smoothing or additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to avoid this and other issues. \n",
        "\n",
        "We can see just how common this practice is by running our classification again with the sklearn toolbox."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1nt-J91rXCy",
        "colab_type": "text"
      },
      "source": [
        "Note what happens when you set alpha=0 (which means no smoothing), would be similar to what we have implemented above. \n",
        "\n",
        "You will likely see a warning that says: \"alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
        "  'setting alpha = %.1e' % _ALPHA_MIN).\"\n",
        "\n",
        "You can instead change this value to alpha = 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJn8MvlckybG",
        "colab_type": "code",
        "outputId": "b89a2103-b14a-46b5-e607-a26c7976a8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB(alpha=0) \n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test[:100,:])\n",
        "np.mean(y_pred == y_test[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.82"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ButisCHuopUG",
        "colab_type": "text"
      },
      "source": [
        "### A bit about Laplace Smoothing\n",
        "\n",
        "Imagine that you are trying to classify a review that contains the word 'awesomesauce' and that your classifier hasn't seen this word before. Naturally, the probability P(x_i|y) will be 0, making log of this go to negative infinity. Eeek!\n",
        "\n",
        "Even if the training corpus is very large, it does not contain all possible words (or combinations of words... foreshaddowing!). \n",
        "\n",
        "This is a common problem in NLP, but thankfully it has an easy fix: smoothing. This technique consists in adding a constant to each count in the P(x_i|y) formula, with the most basic type of smoothing being called add-one (Laplace) smoothing, where the constant is just 1. \n",
        "\n",
        "*Sources: [Medium article](https://medium.com/datadriveninvestor/implementing-naive-bayes-for-sentiment-analysis-in-python-951fa8dcd928) and [section 3.4 of this paper](http://www.diva-portal.org/smash/get/diva2:839705/FULLTEXT01.pdf).*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS0fO4gcugQ4",
        "colab_type": "text"
      },
      "source": [
        "### Notebook Exercise 1\n",
        "\n",
        "Revise the fit_nb_model_smooth() function below to include Laplace smoothing (right now, it's the same as our original function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdRWrkLw2Cn9",
        "colab_type": "code",
        "outputId": "eb4165b1-8ead-4bea-c227-5bbb1e44918d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "def fit_nb_model_smooth(X, y,alpha):\n",
        "  X_1 = np.asarray(X[y == 1, :]) # all reviews with sentiment 1\n",
        "  X_0 = np.asarray(X[y == 0, :])\n",
        "  return y.mean(), 1 - y.mean(), X_1.mean(axis=0), X_0.mean(axis=0)\n",
        "\n",
        "\n",
        "# Code to call and run your new fitting with alpha =1\n",
        "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model_smooth(X_train, y_train,1) #Model with smoothing\n",
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
        "print(\"accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 0.0\n",
            "progress 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "progress 0.2\n",
            "progress 0.3\n",
            "progress 0.4\n",
            "progress 0.5\n",
            "progress 0.6\n",
            "progress 0.7\n",
            "progress 0.8\n",
            "progress 0.9\n",
            "accuracy is 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl-WOh0s173d",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YgnBMrdyMso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ***Solution***\n",
        "def fit_nb_model_smooth(X, y,alpha):\n",
        "    X_1 = np.asarray(X[y == 1, :])\n",
        "    X_0 = np.asarray(X[y == 0, :])\n",
        "    N_1,V_1 = X_1.shape\n",
        "    N_0,V_0 = X_0.shape #should actually be the same size in our case\n",
        "    return y.mean(), 1 - y.mean(), np.divide(X_1.sum(axis=0)+1,N_1), np.divide(X_0.sum(axis=0)+1,N_0) \n",
        "\n",
        "\n",
        "# Code to call and run your new fitting with alpha =1\n",
        "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model_smooth(X_train, y_train,1) #Model with smoothing\n",
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
        "print(\"accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x08auTXzqD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ***Solution***\n",
        "X_0 = np.asarray(X_train[y_train == 0, :])\n",
        "print(np.shape(X_0))\n",
        "print(np.shape(p_x_y_0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-woHjkn2zz6",
        "colab_type": "text"
      },
      "source": [
        "#***Sequence Prediction: Whats next?***\n",
        "\n",
        "Time, the final, final frontier. Or, that thread that ties together events. These events could be major things in your life, or just a series of word or musical notes. Often the sequence of events contains very important information that we want to capture. For example, word order is often essential to the meaning of a phrase. Or, a set of descending musical notes gives a different feeling than an ascending one.\n",
        "\n",
        "We can apply sequence information for various tasks, for example: \n",
        "-Predicting if a coin is fair or not \n",
        "- Choosing the next song to play in a playlist based on previous songs\n",
        "- Deciding what to suggest when watching videos\n",
        "- Determining if text came from one corpus or another\n",
        "- Predicting the next word (or letter) in a phrase (or word)\n",
        "\n",
        "In this notebook, we'll explore these last two applications, building on our movie reivew data set (out of convenience).\n",
        "\n",
        "***What other applications can you think of for sequence prediction?***\n",
        "\n",
        "###One problem with the Bag of Words\n",
        "Until now, we've been using the Bag of Words strategy. Here, words are treated individually. Consider two sentences \"big red machine and carpet\" and \"big red carpet and machine\". If you use a bag of words approach, you will get the same vectors for these two sentences. However, we can clearly see that in the first sentence we are talking about a \"big red machine\", while the second sentence contains information about the \"big red carpet\". Hence, context information is very important. \n",
        "\n",
        "[*Source*](https://stackabuse.com/python-for-nlp-developing-an-automatic-text-filler-using-n-grams/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd0WtdrM1K-7",
        "colab_type": "text"
      },
      "source": [
        "##\"Hello Bigrams\" and Other Ngrams\n",
        "\n",
        "Enter, the bigram (two-words). The bigram model can help us capture context information.\n",
        "\n",
        "**What is an N-gram?**\n",
        "A contiguous sequence of N items from a given sample of text or speech or any other sequence.  Here an item can be a character, a word or a sentence and N can be any integer. When N is 2, we call the sequence a bigram. Similarly, a sequence of 3 items is called a trigram, and so on.\n",
        "\n",
        "Bigrams (or ngrams) are one strategy for sequence prediction (though they can also be used for other tasks including classification). We'll explore these below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WpKd3-s3sDF",
        "colab_type": "text"
      },
      "source": [
        "### A mini-primer on text formatting and tokens\n",
        "\n",
        "If you're really comfortable in python, you can skip this.\n",
        "\n",
        "Otherwise, take a look at some examples of manipulating text (this might be useful in your next assignment if you use your own text with its own formatting quirks).\n",
        "\n",
        "**Start by writing some text and assigning it to s.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCoFD2wok82Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write some text and store it in the variable s\n",
        "s = \"This is where your sentence goes, or however much I want to write >\"\\\n",
        "\"your own profound thoughts! Oh-my! You should probably include some repeat phrases like repeat phrases or like repeat phrases\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkpNUjU_3nyB",
        "colab_type": "code",
        "outputId": "5cd6f60f-3f7f-4524-c298-7d7b1c2e3334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import re\n",
        "# Now, we need to tidy up our text to get rid of the riff-raff (to make it more consistent to find common pairs of words)\n",
        "# Convert to lowercases\n",
        "s = s.lower()  \n",
        "# Replace all none alphanumeric characters with spaces\n",
        "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
        "# Replace series of spaces with single space\n",
        "s = re.sub(' +',' ',s) \n",
        "print(s)\n",
        "\n",
        "# Tokens refer to the words in our case (whatever the chunks are that we are breaking things up into)\n",
        "tokens = [token for token in s.split(\" \") if token != \"\"] # Break sentence in the token, remove empty tokens\n",
        "print(tokens)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is where your sentence goes or however much i want to write your own profound thoughts oh my you should probably include some repeat phrases like repeat phrases or like repeat phrases\n",
            "['this', 'is', 'where', 'your', 'sentence', 'goes', 'or', 'however', 'much', 'i', 'want', 'to', 'write', 'your', 'own', 'profound', 'thoughts', 'oh', 'my', 'you', 'should', 'probably', 'include', 'some', 'repeat', 'phrases', 'like', 'repeat', 'phrases', 'or', 'like', 'repeat', 'phrases']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIXth_XDrTI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "# Essentially the same as above, but putting it into a function for later\n",
        "def clean_text(s):\n",
        "  s = s.lower() # Convert to lowercases\n",
        "  s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s) # Replace all non alphanumeric characters with spaces\n",
        "  s = re.sub(' +',' ',s) # Replace series of spaces with single space\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxPlyTo01hou",
        "colab_type": "text"
      },
      "source": [
        "Now that we have a sense of how the word parsing works, let's move this into a function called generate_ngrams that takes in our text and the number of words to group by (n). For a bigram, we should choose n=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJopKT2x34or",
        "colab_type": "code",
        "outputId": "a498c625-62f1-45cb-aef3-021833a44c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "def generate_ngrams(s, n):   \n",
        "    s = clean_text(s) # Clean text\n",
        "    # Break sentence in the token, remove empty tokens\n",
        "    tokens = [token for token in s.split(\" \") if token != \"\"]    \n",
        "    # Generate sequences of tokens starting from different\n",
        "    # elements of the list of tokens.\n",
        "    # The parameter in the range() function controls how many sequences\n",
        "    # to generate.\n",
        "    sequences = [tokens[i:] for i in range(n)]\n",
        "    # Use the zip function to help us generate n-grams\n",
        "    # Concatentate the tokens into ngrams and return\n",
        "    ngrams = zip(*sequences)\n",
        "    #ngrams = zip(*[tokens[i:] for i in range(n)]) # Or you could combine into one line like this.\n",
        "    return [\" \".join(ngram) for ngram in ngrams]\n",
        "\n",
        "generate_ngrams(s,2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this is',\n",
              " 'is where',\n",
              " 'where your',\n",
              " 'your sentence',\n",
              " 'sentence goes',\n",
              " 'goes or',\n",
              " 'or however',\n",
              " 'however much',\n",
              " 'much i',\n",
              " 'i want',\n",
              " 'want to',\n",
              " 'to write',\n",
              " 'write your',\n",
              " 'your own',\n",
              " 'own profound',\n",
              " 'profound thoughts',\n",
              " 'thoughts oh',\n",
              " 'oh my',\n",
              " 'my you',\n",
              " 'you should',\n",
              " 'should probably',\n",
              " 'probably include',\n",
              " 'include some',\n",
              " 'some repeat',\n",
              " 'repeat phrases',\n",
              " 'phrases like',\n",
              " 'like repeat',\n",
              " 'repeat phrases',\n",
              " 'phrases or',\n",
              " 'or like',\n",
              " 'like repeat',\n",
              " 'repeat phrases']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZ0jAmQe4pV",
        "colab_type": "text"
      },
      "source": [
        "Yay, ngrams! (Bigrams in our case.)\n",
        "Often, it will be more interesting to collapse things and look at a the frequencies of our ngrams.\n",
        "\n",
        "This example uses the nltk library to create your ngrams (a second option for future work)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzgQaVhm_nZ_",
        "colab_type": "code",
        "outputId": "7ea35c33-f1b0-47df-f2fe-837006b7f9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import collections\n",
        "from nltk.util import ngrams\n",
        "\n",
        "s = clean_text(s) # Clean text\n",
        "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
        "bigramWords = list(ngrams(tokens, 2))\n",
        "bigramFreq = collections.Counter(bigramWords)\n",
        "\n",
        "bigramFreq.most_common(10)\n",
        "#print(bigramFreq.most_common(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('repeat', 'phrases'), 3),\n",
              " (('like', 'repeat'), 2),\n",
              " (('this', 'is'), 1),\n",
              " (('is', 'where'), 1),\n",
              " (('where', 'your'), 1),\n",
              " (('your', 'sentence'), 1),\n",
              " (('sentence', 'goes'), 1),\n",
              " (('goes', 'or'), 1),\n",
              " (('or', 'however'), 1),\n",
              " (('however', 'much'), 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41V1Y5TPyYSt",
        "colab_type": "text"
      },
      "source": [
        "##Applying bigrams to the movie reviews\n",
        "Now, we'll explore bigrams with a larger data set. Oh hey, we already loaded in a bunch of movie reviews, how convenient!\n",
        "\n",
        "First, we'll want to clean up the text in our reviews a bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEvVQAyJ2vIG",
        "colab_type": "text"
      },
      "source": [
        "###Notebook Exercise 2\n",
        "\n",
        "Check 5-10 of the movie reviews to make sure our text cleaning is working. dfX_train contains all the movie reviews in the training set.\n",
        "\n",
        "(a) Consider the word parsing done in the previous example. Think about how your text cleaning (e.g., removing hyphens) affected the bigrams. What are some potential limitations of this type of text processing?\n",
        "\n",
        "(b) Determine how you want to inspect the reviews without checking them all. Do you want to just look at the raw text for each? Or perhaps you want to look at the bigrams for each example? Or perhaps you want to look at \n",
        "\n",
        "(c) Update the clean_text() function to remove the most common formatting issue.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9XCTRDS4dkn",
        "colab_type": "code",
        "outputId": "b865e27d-8fd0-4e0c-bce2-9f90d3db2e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Here's a little code to get you started\n",
        "print(\"An original text from the training set:\")\n",
        "print(list(dfX_train)[2])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An original text from the training set:\n",
            "this is the only movie i have ever walked out on. bad acting-- bad plot-- bad casting-- bad directing-- bad cinematography-- if they had set out to make a bad picture they couldn't have done a better job. i hope they are proud of his turkey. i'm surprised anyone associated with this film was ever hired again in hollywood. don't waste your time!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp0aOzzm2vzT",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da28q-1P0J6h",
        "colab_type": "text"
      },
      "source": [
        "***Solution***\n",
        "\n",
        "(a) Depending on the context of what you want to do, you may choose to keep in things like hyphens (perhaps ninety-nine is more meaningful together than apart). Or perhaps appostrophes are meaningful in your corpus. Whenever we are dealing with text, we want to think about our syntax choices, like including flags for start and end words in a sentence (instead of assuming one infinite string of words). \n",
        "\n",
        "(b) \"Dealer's choice\" here, some examples below (you don't need to use this approach). \n",
        "\n",
        "(c) Adding the line s = re.sub('br />',' ',s)  will remove those pesky break indicators, just make sure to do this before you run the line that removes all of the non-alphaneumeric characters. Otherwise, you'll end up with what appears to be the word \"br\" everywhere, and we doubt every movie reviewer is freezing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZMbLwn7jY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ***Solution***\n",
        "# Checking the first few movie reviews\n",
        "for i in range(10):\n",
        "  origtext = list(dfX_train)[i]\n",
        "  print(\"Original text: \",origtext)\n",
        "  cleaned = clean_text(origtext)\n",
        "  print(\"Cleaned  text: \",cleaned)\n",
        "  tokens = [token for token in cleaned.split(\" \") if token != \"\"]\n",
        "  bigramWords = list(ngrams(tokens, 2))\n",
        "  bigramFreq = collections.Counter(bigramWords)\n",
        "  print(\"Bigrams: \", bigramFreq.most_common(10))\n",
        "  print('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDRJFEs-2yFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ***Solution***\n",
        "def clean_text(s):\n",
        "  s = s.lower() # Convert to lowercases\n",
        "  s = re.sub('<br />',' ',s) #Added this new line to get rid of the breaks\n",
        "  s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s) # Replace all non alphanumeric characters with spaces\n",
        "  s = re.sub(' +',' ',s) # Replace series of spaces with single space\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsh1wWW07g4i",
        "colab_type": "text"
      },
      "source": [
        "#### Now, let's clean all the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JMfLVyry0z4",
        "colab_type": "code",
        "outputId": "8addb5f3-b2dd-44f7-bb48-84e83850ea92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "def clean_series_data(sdata):\n",
        "  sdata = list(sdata)\n",
        "  for i in range(len(sdata)):\n",
        "    sdata[i] = clean_text(sdata[i])\n",
        "    if i%(len(sdata)/5)==0:\n",
        "      print(sdata[i]) #Printing occasional text can be helpful for making sure that your cleaning is working how you want it to. Or you can comment this out.\n",
        "  return sdata\n",
        "\n",
        "dfX_train = clean_series_data(dfX_train)\n",
        "dfX_test = clean_series_data(dfX_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "of all the kung fu films made through the 70 s and 80 s this is one that has developed a real cult following with the exception of all the films bruce lee starred in this is a film that has stood the test of time and its due to the unique story an aging kung fu master tells his last pupil yang tieh sheng chiang about five pupils he has trained in the past all five wore masks and nobody has seen the face of each other and they have all been trained differently their specialty in kung fu is the name they have adopted like lizard snake centipede toad and scorpion the master called them the poison clan and he does not know what has happened to them so he wants tieh to find them and help the ones that are doing good to stop the others that are evil an old man who was once a member of the poison clan has a map to where he has hidden a lot of money and he seems to be a target tieh does not know what they look like so he has to mingle in society and try and figure out who they are tieh has discovered that the snake is hung wen tung pai wei and along with tang sen kue feng lu who is the centipede they kill a family to find the map a map is found by a mystery man who turns out to be the scorpion but know one knows who he is a local policeman named ho yung sin philip kwok investigates the murders along with his partner ma chow chien sun sin has a friend called li ho meng lo who is the toad and they do know of each others identity the snake bribes the local officials to pin the murders on li ho and while he is in prison he is tortured and killed when sin finds out he teams up with tieh and together they go to combat tung and kue this film was directed by cheh chang and he was a very special director when it came to these films chang was not your run of the mill kung fu director and his films always had a special quality to them while most martial arts films deal with revenge chang did not use that as a central theme even though there is some revenge going on later in this story this film is more complex than that five men trained by the same master in different ways and wearing masks then they are all in the same area and not knowing who the other is very unique story makes this film different from all the others and most of changs stories were in a class all by themselves i wouldn t exactly put it in the same league as enter the dragon because bruce lee was a worldwide icon and the martial arts he exhibited were more authentic looking this film still has some impossible feats like clinging to sides of walls and all the flipping through the air but this film isn t necessarily about fight scenes its more about the intrigue of the story and the characters that are involved that alone makes this different from all the other kung fu films very well made with a unique story \n",
            "it was simple and yet so nice i think the whole sense of sex segregation in society which can be bitter was shown very delicately it had a bitter kind of hummer in it the fact that most of the actors were not professionals made the movie more tangible and more realistic there was a documentary side to the movie too the best scenes were those that all the girls banned from watching were listening passionately to the soldier who is supposed to keep an eye on them broadcasting the game if you are an iranian the familiar cheering and dancing in the streets after a game won fills you up with national pride if you are not iranian you ll still love it all the same \n",
            "a friend of mine bought this film for 1 and even then it was grossly overpriced despite featuring big names such as adam sandler billy bob thornton and the incredibly talented burt young this film was about as funny as taking a chisel and hammering it straight through your earhole it uses tired bottom of the barrel comedic techniques consistently breaking the fourth wall as sandler talks to the audience and seemingly pointless montages of hot girls adam sandler plays a waiter on a cruise ship who wants to make it as a successful comedian in order to become successful with women when the ship s resident comedian the shamelessly named dickie due to his unfathomable success with the opposite gender is presumed lost at sea sandler s character shecker gets his big break dickie is not dead he s rather locked in the bathroom presumably sea sick perhaps from his mouth he just vomited the worst film of all time \n",
            "this was a movie that i hoped i could suggest to my american friends but after 4 attempts to watch the movie to finish i knew i couldn t even watch the damn thing to close you are almost convinced the actual war didn t even last that long other s will try to question my patriotism for criticizing a movie like this but flat out you can t go from watching saving private ryan to loc forget about the movie budget difference or the audience those don t preclude a director from making an intelligent movie the length of the movie is not so bad and the fact that it is repetitive they keep attacking the same hill but give it different names i thought the loc was a terrible terrain this hill looked like my backyard the character development sequences the soilders flashbacks looking back to their last moments before being deployed should have been throughout the movie and not just clumped into one long memory to this day i have yet to watch the ending but there was a much better movie not saying much called border \n",
            "no scenario bad actors poor melissa gilbert beurk beurk beurk give a such budget to make this in belgium we make ten films which win all prices in cannes with this last time that i ve seen a such null film was hypercube but scenario was better is anyone knows if the director was a graduate in school film or a cop the better things in this film was the word end why authorize to sell this 1 is to expensive i ve pay ten dollars to buy this for me pay for this was my big mistake of millennium too bad next time i ll break my arm but buy this type of sh t \n",
            "i d passed this title 15 or 20 times while in blockbuster looking for something halfway decent to watch all i can say is that they were all out halfway decent films the night i chose to rent this i will give it credit for being leaps and bounds better than dracula 3000 but in actuality that s pretty easy to say since this one didn t have caper van dien in it the other things it lacked in spades were an interesting cast interesting story good dialog and originality but i suppose you can t have everything spoilers ahead the misfit crew of vampire hunters one of which was a vampire go figure flew around from mining colony to mining colony trying to wipe out all the vampires it could find the crew consists of the cocky ne er do well captain who dies early on his by the book yet inexperienced first mate the aforementioned vampire vampire hunter not a typo a wannabe cowboy and a really really butch asian commando tough guy female it almost sounds like the cast for mtv s the real world after the captain dies in a violent confrontation with a mob of bloodsuckers and as it turns out lovers of the finer parts of human anatomy his first mate takes over blah blah blah everyone hates him blah blah blah i could go on but as i said it s the same storyline used in about a hundred other films of the genre the effects were cheese and why michael ironside was in this movie is beyond me the vampire vampire hunter still not a typo was pretty hot but of course if you re waiting for her to expose more than just her cleavage look elsewhere i can t say that it was the worst film i ve seen as i actually rented starship troopers 2 but wait for this one to come on the sci fi channel \n",
            "paris is the place to be to enjoy beautiful art and music and to fall madly in love as is the case in this film boy meets girl they fall in love but something stands in their way of eternal happiness the classic story the wonderful music of george gerschwin complements the great dancing by gene kelly and leslie caron an american in paris is a humorous light hearted loving film well worth watching 8 10 \n",
            "it is an almost ideal romantic anime must see for all ages but the english dubbed version is not too good perhaps the 1999 version will be better \n",
            "a shirley temple short subject it can get mighty rough at buttermilk pete s cafe when the local contingency of diaper clad war babies come in for their midday milk break this primitive little film a spoof of military movies provides a few chuckles but little else tiny tots talking tough can begin to pall in a short time shirley temple playing a duplicitous hip swinging french miss hasn t much to do in this pre celebrity performance highlight the real signs of toddler temper when a few of the infants unexpectedly get well truly soaked with milk often overlooked or neglected today the one and two reel short subjects were useful to the studios as important training grounds for new or burgeoning talents both in front behind the camera the dynamics for creating a successful short subject was completely different from that of a feature length film something akin to writing a topnotch short story rather than a novel economical to produce in terms of both budget schedule and capable of portraying a wide range of material short subjects were the perfect complement to the studios feature films \n",
            "this is the page for house of exorcism but most people have confused this film with the mario bava masterpiece lisa the devil which explains the ridiculously high rating for this house of exorcism when lisa the devil was shown at film festivals in the early 70 s it was a critical success audiences responded well to that gorgeous gothic horror film unfortunately it was a bit ahead of it s time and was considered too unusual and not commercial enough for mass consumption no distributor would buy it so producer alfredo leone decided to edit lisa seemingly with a chainsaw by removing just about half of the original film and adding new scenes which he filmed two years after the original product it is important to note that bava had little to do with these new hideous additions so technically house of exorcism is not a bava film the original product is a slow dreamy classy production a few minutes into the film the viewer is jarred out of this dream world as suddenly we see lisa two years older and with a very different haircut begin to writhe on the ground making guttural sounds and croaking epitaphs like suck my co k etc subtle huh and the film continues like this jumping back and forth between a beautiful visual film and a grade z exorcist rip off leone was trying to incorporate these shock scenes while keeping some semblance of a story intact he failed miserably when the choice was made to basically destroy lisa and the devil bava himself refused saying that his film was too beautiful to cut he was right and it must have been quite sad for this artist to see all his work destroyed and flushed down the toilet it was many years before the original lisa and the devil was seen again re surfacing on late night television i had seen lisa long before i saw this new version and it was downright disturbing to witness one of my favorite films vandalised in this way worth seeing only for curiosity sake otherwise avoid this insidious disaster like the plague \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGDdKfQP1lv0",
        "colab_type": "text"
      },
      "source": [
        "###Notebook Exercise 3 \n",
        "\n",
        "Find the top 10 bigrams for the positive and negative movie reviews in the training data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9gdqb6g4vsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes up here\n",
        "\n",
        "#bigramFreqPositive =\n",
        "#bigramFreqNegative = [] # this too\n",
        "#print(\"Positive bigrams:\")\n",
        "#bgfp = bigramFreqPositive.most_common(10)\n",
        "#for bg in bgfp:\n",
        "#  print(bg)\n",
        "\n",
        "#print(\"Negative bigrams \\n\")\n",
        "#bgfp = bigramFreqNegative.most_common(10)\n",
        "#for bg in bgfp:\n",
        "#  print(bg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8aljKSsB7A2",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for solution\n",
        "\n",
        "Note: If you are still getting 'br','br' as your top bigram. Go back and run the second block in the notebook (where dfX_train gets assigned). Then run the block above that includes the function clean_series_data(). If that doesn't work, repeat the process, after first running the solution where clean_text(s) is defined. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFkxMjSy0SwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ***Solution**\n",
        "# A solution (though there are likely faster, cleaner ways to do this)\n",
        "wordsPositive = list()\n",
        "wordsNegative = list()\n",
        "n = 2\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "  tokens = [token for token in dfX_train[i].split(\" \") if token != \"\"]\n",
        "  if y_train[i]==1:\n",
        "    wordsPositive.extend(tokens)\n",
        "  else:\n",
        "    wordsNegative.extend(tokens)\n",
        "\n",
        "bigramWordsPositive = list(ngrams(wordsPositive, n))\n",
        "bigramFreqPositive = collections.Counter(bigramWordsPositive)\n",
        "bigramWordsNegative = list(ngrams(wordsNegative, n))\n",
        "bigramFreqNegative = collections.Counter(bigramWordsNegative)\n",
        "\n",
        "print(\"Positive bigrams:\")\n",
        "bgfp = bigramFreqPositive.most_common(10)\n",
        "for bg in bgfp:\n",
        "  print(bg)\n",
        "print('\\n')\n",
        "print(\"Negative bigrams\")\n",
        "bgfp = bigramFreqNegative.most_common(10)\n",
        "for bg in bgfp:\n",
        "  print(bg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKZmbjRT7-hS",
        "colab_type": "text"
      },
      "source": [
        "#### Try a different sized n-gram (instead of a bigram / 2-gram)\n",
        "Not shockingly, this list is not very exciting. It turns out, people use some pretty standard words bigrams when talking about movies (e.g. \"this film\" and \"of the\")... really riveting stuff. If we look at a greater number of bigrams (e.g., the top 100), we can eventually start to find something relevant among mostly trite pairings.\n",
        "\n",
        "However, it might be interesting to look at a different sized ngram than the bigram. **Try something in the n = 4 to 7 range.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjeDdSF6FZsL",
        "colab_type": "text"
      },
      "source": [
        "## Classifying movie review sentiment with bigrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSEKwuOKncmZ",
        "colab_type": "text"
      },
      "source": [
        "Let's revisit our Na&iuml;ve Bayes model, but now using bigrams as our features instead of single words. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6BU3Z-g-Gcs",
        "colab_type": "text"
      },
      "source": [
        "#### Use CountVectorizer to get top bigrams and then classify sentiment\n",
        "The code below gives a black box approach to classifying with ngrams. \n",
        "\n",
        "The ngram_range(2,2) makes our code use bigrams. \n",
        "\n",
        "Note that we now have a different shape to our data because it is stored in sparse form (no longer using todense()). If we try to store this in dense form, we will run into RAM errors, which we could combat by limiting the number of ngrams that we include in our CountVectorizer by setting max_features=10000 limits the total number of feautures.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBxljA3JZsKn",
        "colab_type": "code",
        "outputId": "49d868d3-0391-4889-8266-9483d3dc51e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ngramvectorizer = CountVectorizer(ngram_range=(2,2))\n",
        "ngramvectorizer.fit(dfX_train) #learn a vocabulary dictionary of all tokens in the raw documents\n",
        "\n",
        "X_train_ngram = ngramvectorizer.transform(dfX_train)\n",
        "X_test_ngram = ngramvectorizer.transform(dfX_test)\n",
        "print(\"X_train_ngram.shape\", X_train_ngram.shape)\n",
        "print(\"X_test_ngram.shape\", X_test_ngram.shape) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_ngram.shape (18750, 1170351)\n",
            "X_test_ngram.shape (6250, 1170351)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KNyelzfhMcK",
        "colab_type": "code",
        "outputId": "feb0d066-57f9-4d93-86ea-8d58ed485f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Actually run the model and print results\n",
        "# If this is taking too long, you can run it on a subset of your data.\n",
        "model = MultinomialNB(alpha=1)\n",
        "model.fit(X_train_ngram, y_train)\n",
        "\n",
        "y_pred_train = model.predict(X_train_ngram)\n",
        "print(\"Training accuracy: \", np.mean(y_pred_train == y_train))\n",
        "y_pred = model.predict(X_test_ngram)\n",
        "print(\"Testing accuracy: \",np.mean(y_pred == y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy:  0.9978666666666667\n",
            "Testing accuracy:  0.88432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNsiQutMEWSC",
        "colab_type": "text"
      },
      "source": [
        "###Notebook Exercise 4\n",
        "\n",
        "(a) Try manipulating the size of the ngram and the number of features to explore their effects. \n",
        "\n",
        "(b) What method is essentially the same as using the parameter: ngram_range=(1,1)?  What are the relative strenghts/weaknesses of these two approaches?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3SSv1_zFW7Y",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcuSh4O3Lvg4",
        "colab_type": "text"
      },
      "source": [
        "***Solution***\n",
        "\n",
        "(a) This depends on what you looked at. We observed that a full set of bigrams performed better than a bag of words. However, this was not the case if we limit the number of features.\n",
        "\n",
        "(b) Bag of Words. An 1-gram is just a list of words that occurred.  The Bag of Words takes less memory to run... think about how many unique single words exist in text and compare this to how many unique word pairs exist in the same text. \n",
        "\n",
        "However, bigrams can provide us with more information about the context that the words are used. For example, a positive review might include 'not terrible' while a negative review might include 'so terrible'. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSUIpaW5EJi8",
        "colab_type": "text"
      },
      "source": [
        "## Predicting the next words with bigrams\n",
        "\n",
        "Now, we'll explore the idea of choosing the next word in a sequence based on the previous word. We'll do a simple implementation of this, which you will then improve upon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkDsQpVoGIPN",
        "colab_type": "text"
      },
      "source": [
        "Since we already have a list of the bigrams, we can use these to build our predictor.  We'll just use the positive bigram list ***bigramWordsPositive*** that was generated in the solution to Exercise 3. You may need to go back and run that block of code if you haven't already.\n",
        "\n",
        "This code creates a dictionary to store 2nd word in each bigram under the key of the first word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fsbbASsUOAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigramLookup = {}\n",
        "\n",
        "for i in range(len(bigramWordsPositive)-1):\n",
        "    w1 = bigramWordsPositive[i][0]\n",
        "    w2 = bigramWordsPositive[i][1]\n",
        "    #print(w1,w2)\n",
        "    if  w1 not in bigramLookup.keys():\n",
        "      bigramLookup[w1] = {w2:1}\n",
        "    elif w2 not in bigramLookup[w1].keys():\n",
        "      bigramLookup[w1][w2] = 1\n",
        "    else:\n",
        "      bigramLookup[w1][w2] = bigramLookup[w1][w2] + 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suhe9vUwrF24",
        "colab_type": "text"
      },
      "source": [
        "Now, we can write some code to generate an ouptut based on a starting word (curr_sequence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sJmGIbYausK",
        "colab_type": "code",
        "outputId": "6a89d564-2ae0-4bc5-9edb-8ea06c10285c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import random\n",
        "\n",
        "curr_sequence = \"my\" # Starting word\n",
        "output = curr_sequence\n",
        "for i in range(50):\n",
        "    if curr_sequence not in bigramLookup.keys():\n",
        "      print(\"not in my keys, choosing seed word \")\n",
        "      output += '. '\n",
        "      curr_sequence = 'the'\n",
        "      output += curr_sequence\n",
        "    else: \n",
        "      possible_words = list(bigramLookup[curr_sequence].keys())\n",
        "      next_word = possible_words[random.randrange(len(possible_words))] #Randomly choose a word\n",
        "      output += ' ' + next_word\n",
        "      curr_sequence = next_word\n",
        "      \n",
        "\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my era additional details at while its slightly above a meteoric success was required it virtually doomed to divulge what race does blackmail scheme on shakespeare gaining insight in comic character considering i paid 1930 and expressionally handcuffed russell had devoted female informants until two bullets in alexander hunting companion this\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juG7MTxOfk0b",
        "colab_type": "text"
      },
      "source": [
        "###Notebook Exercise 5\n",
        "\n",
        "Well, put together some words, sometimes coherently, sometimes not so much. What a great opportunity for... next_word!\n",
        "\n",
        "Modify the above review generation code to improve it in some way. For example, you might select the next word based on it's probability, instead of just randomly choosing any word that was previously paired.\n",
        "\n",
        "If you're feeling saucy, you could build this review generator to take use some higher value of ngrams. And if you're even saucier, you could build a function that writes your whole review based on the input of a starting phrase and whether the review should be positive or negative.\n",
        "\n",
        "***No solution for this one, but come to class prepared to talk about what you tried.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqWZ8BpHmfgj",
        "colab_type": "text"
      },
      "source": [
        "###Optional Exercise: Build a word completion tool using ngrams on letters instead of words. \n",
        "\n",
        "If you flew through this notebook (because you're familiar with this content or a python wiz, a fun way to challenge yourself is to build a tool that uses ngrams on letters instead of words. This can be applied to word auto-complete or to spell checking.  Be sure to think about the way you want to format your text. How many features will your data have if you do a 1-gram, 2-gram, or 3-gram?"
      ]
    }
  ]
}