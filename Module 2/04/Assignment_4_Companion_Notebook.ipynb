{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%202/04/Assignment_4_Companion_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nw7g0QF3UbU9"
   },
   "source": [
    "# More Na&iuml;ve Bayes with Smoothing and N-Grams\n",
    "\n",
    "***Abstract***\n",
    "\n",
    "In this notebook you'll be explanding on our previous implementation of the Na&iuml;ve Bayes algorithm and exploring the fun new world of bigrams which are \"pretty useful\". We'll practice some techniques for manipulating text and take advantage of some of sklearn's built-in implementations. This notebook will help you practice some tools to use for a sequence learning mini-project.\n",
    "\n",
    "\n",
    "***Takeaways***\n",
    "\n",
    "*   When we use Na&iuml;ve Bayes, it's beneficial to use \"smoothing\" to avoid the problems that arise because our model can run into problems (probabilities of 0, or values of negative infinity) when a word shows up in our test set that was not in our training set.\n",
    "*   In English (and most other languages), word order matters (\"people eat\" is different than \"eat people\"). Therefore, it's often useful to included some information about word order in our model. If we include complete information about word order (e.g., this word showed up 100 words before that word), our model would require extreme amounts of memory. We can typically get a way with a little information about word order, such as with bigrams, which are just pairs of words.\n",
    "*   We can use bigrams (or other ngrams) in various applications in natural language processing including text classification, sequence prediction, word generation, spell checking. These applications also extend beyond actual words or letters, and these techniques can really be applied to any sequences of items (e.g. series of coin flips).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KbrKo8jWMNn"
   },
   "source": [
    "## Sentiment Analysis with Na&iuml;ve Bayes (now with smoothing)\n",
    "\n",
    "Building on the last assignment, we'll be focusing on predicting the sentiment of a movie review from IMDB based on the text of the movie review.  This dataset is one that was originally used in a Kaggle competition called [Bag of Words meets Bag of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial). \n",
    "\n",
    "Again, the [data](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) consist of the following.\n",
    "\n",
    "> The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
    "\n",
    "Our goal will be to see if we can learn a model, using Na&iuml;ve Bayes on a training set to accurately estimate sentiment of new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "bTUjkvrNV1_W",
    "outputId": "7a46fa4e-0f99-4a1c-f26b-2ef19b598bd6"
   },
   "outputs": [],
   "source": [
    "# Import our movie review data (from last assignment)\n",
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "gdown.download('https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download',\n",
    "               'labeledTrainData.tsv',\n",
    "               quiet=False)\n",
    "df = pd.read_csv('labeledTrainData.tsv', header=0, delimiter='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QYpM8qiXFDf"
   },
   "source": [
    "Just like last time, we will convert from text to a **bag of words** representation using scikit learn's built-in [count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Last time, we only included words in our feature vector if they occur in at least 100 reviews. Now we reduce this limitation, so that included words only need to appear in at least 20 reviews. Note the the previous shape of X was:\n",
    "X.shape (25000, 3833), and now it should have many more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "iQ0UE8IoW8KN",
    "outputId": "195cc182-93b1-4866-82f8-d7c10eef9d22"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.array(df['sentiment'])\n",
    "\n",
    "dfX_train, dfX_test, y_train, y_test = train_test_split(df['review'], y)\n",
    "print(\"df_train.shape\",dfX_train.shape)\n",
    "print(\"y_train.shape\",y_train.shape)\n",
    "print(\"dfX_test.shape\",dfX_test.shape)\n",
    "print(\"y_test.shape\",y_test.shape)\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, min_df = 20) #convert a collection of text documents into a matrix of token counts\n",
    "vectorizer.fit(dfX_train) #learn a vocabulary dictionary of all tokens in the raw documents\n",
    "\n",
    "X_train = vectorizer.transform(dfX_train).todense() #transform to a document-term matrix\n",
    "X_test = vectorizer.transform(dfX_test).todense()\n",
    "print(\"X_test.shape\",X_test.shape)\n",
    "print(\"X_train.shape\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ed5j_vlr-Cu"
   },
   "source": [
    "We also split the data into training and test right from the start. We'll check to make sure our data is organized properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "aYaufLj5g65W",
    "outputId": "a931db52-5a92-4fa9-bb4b-3a00dbbbef91"
   },
   "outputs": [],
   "source": [
    "# Looking at a review to make sure things work\n",
    "reviews_wrapped = dfX_train.str.wrap(80)\n",
    "terrible_index = vectorizer.get_feature_names().index('terrible')\n",
    "print(\"terrible occurs in\", X_train[y_train==1, terrible_index].mean(), \"for Y=1\")\n",
    "print(\"terrible occurs in\", X_train[y_train==0, terrible_index].mean(), \"for Y=0\")\n",
    "print(reviews_wrapped.iloc[1]) # Just in case you want to read a random review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7esYTmjpfqgr"
   },
   "source": [
    "### Fitting the Parameters of the Model & Making Predictions\n",
    "\n",
    "As you may recall from the last assignment:\n",
    "\n",
    "What we see from looking at the log odds ratio equation is that in order to apply the model we must have an estimate of the following probabilities.\n",
    "* $p(Y=0)$\n",
    "* $p(Y=1)$\n",
    "* $p(X_i = x_i | Y=0)$ (for $i$ from $1$ to $d$)\n",
    "* $p(X_i = x_i | Y=1)$ (for $i$ from $1$ to $d$)\n",
    "\n",
    "The MLE problem in the earlier part of the assignment gave a more formal justification of this process. \n",
    "\n",
    "We'll start with the solution from the last assignment. Here, we count the number of times the event occurs across the dataset in order to estimate a probability.\n",
    "\n",
    "For instance, if we want to estimate $p(Y=0)$ we would count the number of instances in the dataset where $Y=0$ and divide that by the total number of instances in the dataset.  If we wanted to estimated $p(X_i = 1 | Y = 0)$ (suppose X_i represents the word \"terrible\") we would count the number of reviews that included the word terrible and had sentiment 0 and divide that by the number of reviews that were sentiment 0. Remember that $p(A|B) = \\frac{p(A, B)}{p(B)}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvXgkDmK60vI"
   },
   "outputs": [],
   "source": [
    "# Essentially same functions as solutions from previous assignment\n",
    "def fit_nb_model(X, y):\n",
    "    X_1 = np.asarray(X[y == 1, :]) # all reviews with sentiment 1\n",
    "    X_0 = np.asarray(X[y == 0, :])\n",
    "    return y.mean(), 1 - y.mean(), X_1.mean(axis=0), X_0.mean(axis=0)\n",
    "\n",
    "def get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X):\n",
    "    \"\"\" Predict the labels for the data X given the Naive Bayes model \"\"\"\n",
    "    log_odds_ratios = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]): # loop over data points\n",
    "        if i%(X.shape[0]/10) == 0: print(\"progress\", i/X.shape[0])\n",
    "        log_odds_ratios[i] += np.log(p_y_1) - np.log(p_y_0)\n",
    "        for j in range(X.shape[1]): #loop over words\n",
    "            if X[i, j] == 1: #if this example includes word j\n",
    "                log_odds_ratios[i] += np.log(p_x_y_1[j]) - np.log(p_x_y_0[j])\n",
    "            else: \n",
    "                log_odds_ratios[i] += np.log(1 - p_x_y_1[j]) - np.log(1 - p_x_y_0[j])\n",
    "    return (log_odds_ratios >= 0).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUouXFCjnxmG"
   },
   "source": [
    "## Testing the Accuracy of the Model from Assignment 3\n",
    "The following lines of code run the solution function (above) and calculate the accuracy. We're just using the first 100 entries for faster computation. \n",
    "\n",
    "**What do you observe happens when you run this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "FNU-v-DzjcY-",
    "outputId": "e15165c2-d3fa-48a7-b17e-1f231d560516"
   },
   "outputs": [],
   "source": [
    "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model(X_train, y_train)\n",
    "y_pred_train = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_train[:100,:])\n",
    "print(\"Train accuracy is\", (y_pred_train == y_train[:100]).astype(np.float).mean()) #also only need to compare first 100 y\n",
    "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
    "print(\"Test accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TXmSxXgtqxQ"
   },
   "source": [
    "You may have noticed a \"RuntimeWarning: divide by zero encountered in log\". It is common practice to use something call [Laplace smoothing or additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to avoid this and other issues. \n",
    "\n",
    "We can see just how common this practice is by running our classification again with the sklearn toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1nt-J91rXCy"
   },
   "source": [
    "Note what happens when you set alpha=0 (which means no smoothing), would be similar to what we have implemented above. \n",
    "\n",
    "You will likely see a warning that says: \"alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
    "  'setting alpha = %.1e' % _ALPHA_MIN).\"\n",
    "\n",
    "You can instead change this value to alpha = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "qJn8MvlckybG",
    "outputId": "47a179ae-ec64-4612-f0bf-5e0e9a9ecf0c"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB(alpha=0) \n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test[:100,:])\n",
    "np.mean(y_pred == y_test[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ButisCHuopUG"
   },
   "source": [
    "### A bit about Laplace Smoothing\n",
    "\n",
    "Imagine that you are trying to classify a review that contains the word 'awesomesauce' and that your classifier hasn't seen this word before. Naturally, the probability P(x_i|y) will be 0, making log of this go to negative infinity. Eeek!\n",
    "\n",
    "Even if the training corpus is very large, it does not contain all possible words (or combinations of words... foreshaddowing!). \n",
    "\n",
    "This is a common problem in NLP, but thankfully it has an easy fix: smoothing. This technique consists in adding a constant to each count in the P(x_i|y) formula, with the most basic type of smoothing being called add-one (Laplace) smoothing, where the constant is just 1. \n",
    "\n",
    "*Sources: [Medium article](https://medium.com/datadriveninvestor/implementing-naive-bayes-for-sentiment-analysis-in-python-951fa8dcd928) and [section 3.4 of this paper](http://www.diva-portal.org/smash/get/diva2:839705/FULLTEXT01.pdf).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SS0fO4gcugQ4"
   },
   "source": [
    "### Notebook Exercise 1\n",
    "\n",
    "Revise the fit_nb_model_smooth() function below to include Laplace smoothing (right now, it's the same as our original function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "sdRWrkLw2Cn9",
    "outputId": "beee2272-06d7-4a9a-e342-2161f3e6caf9"
   },
   "outputs": [],
   "source": [
    "def fit_nb_model_smooth(X, y,alpha):\n",
    "  X_1 = np.asarray(X[y == 1, :]) # all reviews with sentiment 1\n",
    "  X_0 = np.asarray(X[y == 0, :])\n",
    "  return y.mean(), 1 - y.mean(), X_1.mean(axis=0), X_0.mean(axis=0)\n",
    "\n",
    "\n",
    "# Code to call and run your new fitting with alpha =1\n",
    "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model_smooth(X_train, y_train,1) #Model with smoothing\n",
    "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
    "print(\"accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jl-WOh0s173d"
   },
   "source": [
    "#### Expand for Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "0YgnBMrdyMso",
    "outputId": "3288bca4-1f09-449e-ebbf-f63c30d52f8c"
   },
   "outputs": [],
   "source": [
    "# ***Solution***\n",
    "def fit_nb_model_smooth(X, y,alpha):\n",
    "    X_1 = np.asarray(X[y == 1, :])\n",
    "    X_0 = np.asarray(X[y == 0, :])\n",
    "    N_1,V_1 = X_1.shape\n",
    "    N_0,V_0 = X_0.shape #should actually be the same size in our case\n",
    "    return y.mean(), 1 - y.mean(), np.divide(X_1.sum(axis=0)+1,N_1), np.divide(X_0.sum(axis=0)+1,N_0) \n",
    "\n",
    "\n",
    "# Code to call and run your new fitting with alpha =1\n",
    "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model_smooth(X_train, y_train,1) #Model with smoothing\n",
    "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
    "print(\"accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "0x08auTXzqD2",
    "outputId": "978cdd42-cf4b-49ae-d76c-d8bf29de1dcd"
   },
   "outputs": [],
   "source": [
    "# ***Solution***\n",
    "X_0 = np.asarray(X_train[y_train == 0, :])\n",
    "print(np.shape(X_0))\n",
    "print(np.shape(p_x_y_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-woHjkn2zz6"
   },
   "source": [
    "# ***Sequence Prediction: Whats next?***\n",
    "\n",
    "Time, the final, final frontier. Or, that thread that ties together events. These events could be major things in your life, or just a series of word or musical notes. Often the sequence of events contains very important information that we want to capture. For example, word order is often essential to the meaning of a phrase. Or, a set of descending musical notes gives a different feeling than an ascending one.\n",
    "\n",
    "We can apply sequence information for various tasks, for example: \n",
    "-Predicting if a coin is fair or not \n",
    "- Choosing the next song to play in a playlist based on previous songs\n",
    "- Deciding what to suggest when watching videos\n",
    "- Determining if text came from one corpus or another\n",
    "- Predicting the next word (or letter) in a phrase (or word)\n",
    "\n",
    "In this notebook, we'll explore these last two applications, building on our movie reivew data set (out of convenience).\n",
    "\n",
    "***What other applications can you think of for sequence prediction?***\n",
    "\n",
    "### One problem with the Bag of Words\n",
    "\n",
    "Until now, we've been using the Bag of Words strategy. Here, words are treated individually. Consider two sentences \"big red machine and carpet\" and \"big red carpet and machine\". If you use a bag of words approach, you will get the same vectors for these two sentences. However, we can clearly see that in the first sentence we are talking about a \"big red machine\", while the second sentence contains information about the \"big red carpet\". Hence, context information is very important. \n",
    "\n",
    "[*Source*](https://stackabuse.com/python-for-nlp-developing-an-automatic-text-filler-using-n-grams/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nd0WtdrM1K-7"
   },
   "source": [
    "## \"Hello Bigrams\" and Other Ngrams\n",
    "\n",
    "Enter, the bigram (two-words). The bigram model can help us capture context information.\n",
    "\n",
    "**What is an N-gram?**\n",
    "A contiguous sequence of N items from a given sample of text or speech or any other sequence.  Here an item can be a character, a word or a sentence and N can be any integer. When N is 2, we call the sequence a bigram. Similarly, a sequence of 3 items is called a trigram, and so on.\n",
    "\n",
    "Bigrams (or ngrams) are one strategy for sequence prediction (though they can also be used for other tasks including classification). We'll explore these below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WpKd3-s3sDF"
   },
   "source": [
    "### A mini-primer on text formatting and tokens\n",
    "\n",
    "If you're really comfortable in python, you can skip this.\n",
    "\n",
    "Otherwise, take a look at some examples of manipulating text (this might be useful in your next assignment if you use your own text with its own formatting quirks).\n",
    "\n",
    "**Start by writing some text and assigning it to s.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCoFD2wok82Z"
   },
   "outputs": [],
   "source": [
    "# Write some text and store it in the variable s\n",
    "s = \"This is where your sentence goes, or however much I want to write >\"\\\n",
    "\"your own profound thoughts! Oh-my! You should probably include some repeat phrases like repeat phrases or like repeat phrases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "tkpNUjU_3nyB",
    "outputId": "27e961d3-a77f-48b3-8730-f20015ab242f"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Now, we need to tidy up our text to get rid of the riff-raff (to make it more consistent to find common pairs of words)\n",
    "# Convert to lowercases\n",
    "s = s.lower()  \n",
    "# Replace all none alphanumeric characters with spaces\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "# Replace series of spaces with single space\n",
    "s = re.sub(' +',' ',s) \n",
    "print(s)\n",
    "\n",
    "# Tokens refer to the words in our case (whatever the chunks are that we are breaking things up into)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"] # Break sentence in the token, remove empty tokens\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIXth_XDrTI0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Essentially the same as above, but putting it into a function for later\n",
    "def clean_text(s):\n",
    "  s = s.lower() # Convert to lowercases\n",
    "  s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s) # Replace all non alphanumeric characters with spaces\n",
    "  s = re.sub(' +',' ',s) # Replace series of spaces with single space\n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxPlyTo01hou"
   },
   "source": [
    "Now that we have a sense of how the word parsing works, let's move this into a function called generate_ngrams that takes in our text and the number of words to group by (n). For a bigram, we should choose n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "OJopKT2x34or",
    "outputId": "ad03189d-2908-4bb1-ea87-274a63b6e783"
   },
   "outputs": [],
   "source": [
    "def generate_ngrams(s, n):   \n",
    "    s = clean_text(s) # Clean text\n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]    \n",
    "    # Generate sequences of tokens starting from different\n",
    "    # elements of the list of tokens.\n",
    "    # The parameter in the range() function controls how many sequences\n",
    "    # to generate.\n",
    "    sequences = [tokens[i:] for i in range(n)]\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*sequences)\n",
    "    #ngrams = zip(*[tokens[i:] for i in range(n)]) # Or you could combine into one line like this.\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "generate_ngrams(s,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCZ0jAmQe4pV"
   },
   "source": [
    "Yay, ngrams! (Bigrams in our case.)\n",
    "Often, it will be more interesting to collapse things and look at a the frequencies of our ngrams.\n",
    "\n",
    "This example uses the nltk library to create your ngrams (a second option for future work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "IzgQaVhm_nZ_",
    "outputId": "87ebb866-e862-409a-8eb7-3f89f51cf39e"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = clean_text(s) # Clean text\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "bigramWords = list(ngrams(tokens, 2))\n",
    "bigramFreq = collections.Counter(bigramWords)\n",
    "\n",
    "bigramFreq.most_common(10)\n",
    "#print(bigramFreq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41V1Y5TPyYSt"
   },
   "source": [
    "## Applying bigrams to the movie reviews\n",
    "\n",
    "Now, we'll explore bigrams with a larger data set. Oh hey, we already loaded in a bunch of movie reviews, how convenient!\n",
    "\n",
    "First, we'll want to clean up the text in our reviews a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEvVQAyJ2vIG"
   },
   "source": [
    "### Notebook Exercise 2\n",
    "\n",
    "Check 5-10 of the movie reviews to make sure our text cleaning is working. dfX_train contains all the movie reviews in the training set.\n",
    "\n",
    "(a) Consider the word parsing done in the previous example. Think about how your text cleaning (e.g., removing hyphens) affected the bigrams. What are some potential limitations of this type of text processing?\n",
    "\n",
    "(b) Determine how you want to inspect the reviews without checking them all. Do you want to just look at the raw text for each? Or perhaps you want to look at the bigrams for each example? Or perhaps you want to look at \n",
    "\n",
    "(c) Update the clean_text() function to remove the most common formatting issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "m9XCTRDS4dkn",
    "outputId": "e9b1c3f7-a55c-4709-e96b-0150166ad0a7"
   },
   "outputs": [],
   "source": [
    "#Here's a little code to get you started\n",
    "print(\"An original text from the training set:\")\n",
    "print(dfX_train[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gp0aOzzm2vzT"
   },
   "source": [
    "#### Expand for solution\n",
    "\n",
    "(a) Depending on the context of what you want to do, you may choose to keep in things like hyphens (perhaps ninety-nine is more meaningful together than apart). Or perhaps appostrophes are meaningful in your corpus. Whenever we are dealing with text, we want to think about our syntax choices, like including flags for start and end words in a sentence (instead of assuming one infinite string of words). \n",
    "\n",
    "(b) \"Dealer's choice\" here, some examples below (you don't need to use this approach). \n",
    "\n",
    "(c) Adding the line s = re.sub('br />',' ',s)  will remove those pesky break indicators, just make sure to do this before you run the line that removes all of the non-alphaneumeric characters. Otherwise, you'll end up with what appears to be the word \"br\" everywhere, and we doubt every movie reviewer is freezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "id": "GMZMbLwn7jY-",
    "outputId": "3b2dd29b-eb7c-43be-9a8b-a2f3caedd421"
   },
   "outputs": [],
   "source": [
    "# Checking the first few movie reviews\n",
    "for i in range(10):\n",
    "  origtext = list(dfX_train)[i]\n",
    "  print(\"Original text: \",origtext)\n",
    "  cleaned = clean_text(origtext)\n",
    "  print(\"Cleaned  text: \",cleaned)\n",
    "  tokens = [token for token in cleaned.split(\" \") if token != \"\"]\n",
    "  bigramWords = list(ngrams(tokens, 2))\n",
    "  bigramFreq = collections.Counter(bigramWords)\n",
    "  print(\"Bigrams: \", bigramFreq.most_common(10))\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDRJFEs-2yFY"
   },
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "  s = s.lower() # Convert to lowercases\n",
    "  s = re.sub('<br />',' ',s) #Added this new line to get rid of the breaks\n",
    "  s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s) # Replace all non alphanumeric characters with spaces\n",
    "  s = re.sub(' +',' ',s) # Replace series of spaces with single space\n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsh1wWW07g4i"
   },
   "source": [
    "#### Now, let's clean all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "0JMfLVyry0z4",
    "outputId": "4d18101e-ae26-4cfc-c5e8-4415687cacfd"
   },
   "outputs": [],
   "source": [
    "def clean_series_data(sdata):\n",
    "  sdata = list(sdata)\n",
    "  for i in range(len(sdata)):\n",
    "    sdata[i] = clean_text(sdata[i])\n",
    "    if i%(len(sdata)/5)==0:\n",
    "      print(sdata[i]) #Printing occasional text can be helpful for making sure that your cleaning is working how you want it to. Or you can comment this out.\n",
    "  return sdata\n",
    "\n",
    "dfX_train = clean_series_data(dfX_train)\n",
    "dfX_test = clean_series_data(dfX_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGDdKfQP1lv0"
   },
   "source": [
    "### Notebook Exercise 3 \n",
    "\n",
    "Find the top 10 bigrams for the positive and negative movie reviews in the training data. (You'll need to deal with the fact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9gdqb6g4vsk"
   },
   "outputs": [],
   "source": [
    "# Your code goes up here\n",
    "\n",
    "#bigramFreqPositive =\n",
    "#bigramFreqNegative = [] # this too\n",
    "#print(\"Positive bigrams:\")\n",
    "#bgfp = bigramFreqPositive.most_common(10)\n",
    "#for bg in bgfp:\n",
    "#  print(bg)\n",
    "\n",
    "#print(\"Negative bigrams \\n\")\n",
    "#bgfp = bigramFreqNegative.most_common(10)\n",
    "#for bg in bgfp:\n",
    "#  print(bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8aljKSsB7A2"
   },
   "source": [
    "#### Expand for solution\n",
    "\n",
    "Note: If you are still getting 'br','br' as your top bigram. Go back and run the second block in the notebook (where dfX_train gets assigned). Then run the block above that includes the function clean_series_data(). If that doesn't work, repeat the process, after first running the solution where clean_text(s) is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFkxMjSy0SwT"
   },
   "outputs": [],
   "source": [
    "# ***Solution***\n",
    "# A solution (though there are likely faster, cleaner ways to do this)\n",
    "wordsPositive = list()\n",
    "wordsNegative = list()\n",
    "n = 2\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "  tokens = [token for token in dfX_train[i].split(\" \") if token != \"\"]\n",
    "  if y_train[i]==1:\n",
    "    wordsPositive.extend(tokens)\n",
    "  else:\n",
    "    wordsNegative.extend(tokens)\n",
    "\n",
    "bigramWordsPositive = list(ngrams(wordsPositive, n))\n",
    "bigramFreqPositive = collections.Counter(bigramWordsPositive)\n",
    "bigramWordsNegative = list(ngrams(wordsNegative, n))\n",
    "bigramFreqNegative = collections.Counter(bigramWordsNegative)\n",
    "\n",
    "print(\"Positive bigrams:\")\n",
    "bgfp = bigramFreqPositive.most_common(10)\n",
    "for bg in bgfp:\n",
    "  print(bg)\n",
    "print('\\n')\n",
    "print(\"Negative bigrams\")\n",
    "bgfp = bigramFreqNegative.most_common(10)\n",
    "for bg in bgfp:\n",
    "  print(bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKZmbjRT7-hS"
   },
   "source": [
    "#### Try a different sized n-gram (instead of a bigram / 2-gram)\n",
    "Not shockingly, this list is not very exciting. It turns out, people use some pretty standard words bigrams when talking about movies (e.g. \"this film\" and \"of the\")... really riveting stuff. If we look at a greater number of bigrams (e.g., the top 100), we can eventually start to find something relevant among mostly trite pairings.\n",
    "\n",
    "However, it might be interesting to look at a different sized ngram than the bigram. **Try something in the n = 4 to 7 range.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjeDdSF6FZsL"
   },
   "source": [
    "## Classifying movie review sentiment with bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSEKwuOKncmZ"
   },
   "source": [
    "Let's revisit our Na&iuml;ve Bayes model, but now using bigrams as our features instead of single words. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6BU3Z-g-Gcs"
   },
   "source": [
    "#### Use CountVectorizer to get top bigrams and then classify sentiment\n",
    "The code below gives a black box approach to classifying with ngrams. \n",
    "\n",
    "The ngram_range(2,2) makes our code use bigrams. \n",
    "\n",
    "Note that we now have a different shape to our data because it is stored in sparse form (no longer using todense()). If we try to store this in dense form, we will run into RAM errors, which we could combat by limiting the number of ngrams that we include in our CountVectorizer by setting max_features=10000 limits the total number of feautures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "QBxljA3JZsKn",
    "outputId": "1735e94f-9b70-4fa8-e907-62274cf36c11"
   },
   "outputs": [],
   "source": [
    "ngramvectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "ngramvectorizer.fit(dfX_train) #learn a vocabulary dictionary of all tokens in the raw documents\n",
    "\n",
    "X_train_ngram = ngramvectorizer.transform(dfX_train)\n",
    "X_test_ngram = ngramvectorizer.transform(dfX_test)\n",
    "print(\"X_train_ngram.shape\", X_train_ngram.shape)\n",
    "print(\"X_test_ngram.shape\", X_test_ngram.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3KNyelzfhMcK",
    "outputId": "ace71470-a3ba-43ee-ec68-37896c750f60"
   },
   "outputs": [],
   "source": [
    "# Actually run the model and print results\n",
    "# If this is taking too long, you can run it on a subset of your data.\n",
    "model = MultinomialNB(alpha=1)\n",
    "model.fit(X_train_ngram, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train_ngram)\n",
    "print(\"Training accuracy: \", np.mean(y_pred_train == y_train))\n",
    "y_pred = model.predict(X_test_ngram)\n",
    "print(\"Testing accuracy: \",np.mean(y_pred == y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNsiQutMEWSC"
   },
   "source": [
    "### Notebook Exercise 4\n",
    "\n",
    "(a) Try manipulating the size of the ngram and the number of features to explore their effects. \n",
    "\n",
    "(b) What method is essentially the same as using the parameter: ngram_range=(1,1)?  What are the relative strenghts/weaknesses of these two approaches?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3SSv1_zFW7Y"
   },
   "source": [
    "#### Expand for solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcuSh4O3Lvg4"
   },
   "source": [
    "***Solution***\n",
    "\n",
    "(a) This depends on what you looked at. We observed that a full set of bigrams performed better than a bag of words. However, this was not the case if we limit the number of features.\n",
    "\n",
    "(b) Bag of Words. An 1-gram is just a list of words that occurred.  The Bag of Words takes less memory to run... think about how many unique single words exist in text and compare this to how many unique word pairs exist in the same text. \n",
    "\n",
    "However, bigrams can provide us with more information about the context that the words are used. For example, a positive review might include 'not terrible' while a negative review might include 'so terrible'. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSUIpaW5EJi8"
   },
   "source": [
    "## Predicting the next words with bigrams\n",
    "\n",
    "Now, we'll explore the idea of choosing the next word in a sequence based on the previous word. We'll do a simple implementation of this, which you will then improve upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkDsQpVoGIPN"
   },
   "source": [
    "Since we already have a list of the bigrams, we can use these to build our predictor.  We'll just use the positive bigram list ***bigramWordsPositive*** that was generated in the solution to Exercise 3. You may need to go back and run that block of code if you haven't already.\n",
    "\n",
    "This code creates a dictionary to store 2nd word in each bigram under the key of the first word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fsbbASsUOAV"
   },
   "outputs": [],
   "source": [
    "bigramLookup = {}\n",
    "\n",
    "for i in range(len(bigramWordsPositive)-1):\n",
    "    w1 = bigramWordsPositive[i][0]\n",
    "    w2 = bigramWordsPositive[i][1]\n",
    "    #print(w1,w2)\n",
    "    if  w1 not in bigramLookup.keys():\n",
    "      bigramLookup[w1] = {w2:1}\n",
    "    elif w2 not in bigramLookup[w1].keys():\n",
    "      bigramLookup[w1][w2] = 1\n",
    "    else:\n",
    "      bigramLookup[w1][w2] = bigramLookup[w1][w2] + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Suhe9vUwrF24"
   },
   "source": [
    "Now, we can write some code to generate an ouptut based on a starting word (curr_sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8sJmGIbYausK",
    "outputId": "275f842f-8f08-445e-aacb-f6817f815958"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "curr_sequence = \"my\" # Starting word\n",
    "output = curr_sequence\n",
    "for i in range(50):\n",
    "    if curr_sequence not in bigramLookup.keys():\n",
    "      print(\"not in my keys, choosing seed word \")\n",
    "      output += '. '\n",
    "      curr_sequence = 'the'\n",
    "      output += curr_sequence\n",
    "    else: \n",
    "      possible_words = list(bigramLookup[curr_sequence].keys())\n",
    "      next_word = possible_words[random.randrange(len(possible_words))] #Randomly choose a word\n",
    "      output += ' ' + next_word\n",
    "      curr_sequence = next_word\n",
    "      \n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juG7MTxOfk0b"
   },
   "source": [
    "### Notebook Exercise 5\n",
    "\n",
    "Well, put together some words, sometimes coherently, sometimes not so much. What a great opportunity for... next_word!\n",
    "\n",
    "Modify the above review generation code to improve it in some way. For example, you might select the next word based on it's probability, instead of just randomly choosing any word that was previously paired.\n",
    "\n",
    "If you're feeling saucy, you could build this review generator to take use some higher value of ngrams. And if you're even saucier, you could build a function that writes your whole review based on the input of a starting phrase and whether the review should be positive or negative.\n",
    "\n",
    "***No solution for this one, but come to class prepared to talk about what you tried.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqWZ8BpHmfgj"
   },
   "source": [
    "### Optional Exercise: Build a word completion tool using ngrams on letters instead of words. \n",
    "\n",
    "If you flew through this notebook (because you're familiar with this content or a python wiz, a fun way to challenge yourself is to build a tool that uses ngrams on letters instead of words. This can be applied to word auto-complete or to spell checking.  Be sure to think about the way you want to format your text. How many features will your data have if you do a 1-gram, 2-gram, or 3-gram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "jl-WOh0s173d",
    "s3SSv1_zFW7Y"
   ],
   "include_colab_link": true,
   "name": "Assignment 4 Companion Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
