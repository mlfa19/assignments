{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploring COMPAS Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%202/03/Exploring_COMPAS_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1OlR8h1o4a_",
        "colab_type": "text"
      },
      "source": [
        "# Exploring the ProPublica COMPAS Analysis\n",
        "\n",
        "In this notebook you'll get a chance to examine the data used in the ProPublica story yourself.\n",
        "\n",
        "*Disclaimer:*\n",
        "Please don't over interpret what you find in the data.  We know from our discussions that methodology is key to being able to properly interpret findings.  Our goal here will be to reproduce results from the readings we did for last class.\n",
        "\n",
        "First, we'll download and parse the data into a data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjz1pFW63Nkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "!wget https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
        "df = pd.read_csv('compas-scores-two-years.csv')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foMob9RZpkLy",
        "colab_type": "text"
      },
      "source": [
        "In the ProPublica dataset they only used data where the \"days_b_screening_arrest\" feature was in the range [-30, 30]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPW--bUn3fWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter these based on propublica analysis (not sure why this doesn't match https://fairmlbook.org/classification.html)\n",
        "df = df[(df['days_b_screening_arrest'] <= 30) | (df['days_b_screening_arrest'] >= -30)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev9anR-sp5UM",
        "colab_type": "text"
      },
      "source": [
        "## Reproducing Calculations of False Positive Rate and Positive Predictive Value\n",
        "\n",
        "From a statistical point of view (but not necessarily a social justice point of view) the debate between Propublica and NorthPointe boiled down to what is the rate way to measure bias in an algorithm.  As you saw in the first part of the assignmnet, ProPublica used the evidence that the false positive rate differed between blacks and whites as evidence of bias.  Northpointe argued used the fact that the positive predictive values across the two groups were the same as evidence *against* bias.\n",
        "\n",
        "In Northpointe's report, they have a table which lists various statistics for blacks versus whites using the COMPAS risk scores as the predictor.  Here is the relevant information from their report.\n",
        "\n",
        "|Race | $\\hat{y} = 1$ | True positive rate | False Positive Rate | Positive Predictive Value | Negative Predictive Value\n",
        "------|-----------------|---------------------|--------------------|------|------|\n",
        "white | $decile \\geq 1$ | 1.00 | 1.00 | 0.39 | 1.00 |\n",
        " | |$decile \\geq 2$ | 0.85 | 0.64 | 0.46 | 0.79\n",
        " | |$decile \\geq 3$ | 0.74 | 0.47 | 0.5 | 0.76\n",
        " | |$decile \\geq 4$ | 0.64 | 0.35 | 0.54 | 0.74\n",
        " | |$decile \\geq 5$ | 0.52 | 0.23 | 0.59 | 0.71\n",
        " | |$decile \\geq 6$ | 0.41 | 0.15 | 0.64 | 0.69\n",
        " | |$decile \\geq 7$ | 0.29 | 0.09 | 0.68 | 0.66\n",
        " | |$decile \\geq 8$ | 0.20 | 0.05 | 0.71 | 0.65\n",
        " | |$decile \\geq 9$ | 0.12 | 0.03 | 0.70 | 0.63\n",
        " | |$decile \\geq 10$| 0.05 | 0.01 | 0.70 | 0.61\n",
        " | | | | |\n",
        " black | $decile \\geq 1$ | 1.00 | 1.00 | 0.51 | 1.00 |\n",
        " | |$decile \\geq 2$ | 0.95 | 0.83 | 0.55 | 0.77\n",
        " | |$decile \\geq 3$ | 0.89 | 0.68 | 0.58 | 0.73\n",
        " | |$decile \\geq 4$ | 0.81 | 0.56 | 0.60 | 0.69\n",
        " | |$decile \\geq 5$ | 0.72 | 0.45 | 0.63 | 0.65\n",
        " | |$decile \\geq 6$ | 0.63 | 0.34 | 0.66 | 0.62\n",
        " | |$decile \\geq 7$ | 0.51 | 0.25 | 0.69 | 0.59\n",
        " | |$decile \\geq 8$ | 0.39 | 0.16 | 0.72 | 0.57\n",
        " | |$decile \\geq 9$ | 0.26 | 0.09 | 0.74 | 0.54\n",
        " | |$decile \\geq 10$| 0.12 | 0.03 | 0.79 | 0.51"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgMr7NXtuFVg",
        "colab_type": "text"
      },
      "source": [
        "### *Notebook Exercise 1*\n",
        "\n",
        "Write code to reproduce the table above.  Here are some hints to help you.\n",
        "\n",
        "* If you're fuzzy on what each of these statistic means (false positive rate, true positive rate, etc.), consider checking out [Binary Diagnostic Tests](https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Binary_Diagnostic_Tests-Single_Sample.pdf).\n",
        "* You'll want to use the column `df['two_year_recid']` as your indicator of true positive versus true negative (positive means that the person recidivated).\n",
        "* To select narrow the data frame to just contain people of a particular race you can use the following snippet (`race` would be a string that is either \"Caucasian\" or \"African-American\").\n",
        "```python\n",
        "df_for_race = df[df['race'] == race]\n",
        "```\n",
        "* To generate a particular row of the table, you'll want to loop over all possible thresholds where the model would predict recidivate ($\\hat{y} = 1$).\n",
        "* You can count the number of elements in a Pandas series that satisfy some criterion using the following technique.  For instance, if we wanted to calculate the number of elements in \"some_column\" that are greater than 0 and less than 30, we could use the following code.\n",
        "```python\n",
        "((df['some column'] > 0).sum() & (df['some column'] < 30).sum()\n",
        "```\n",
        "* It's up to you how you want to generate the table.  You can simply print out the values within a loop as you compute them, or you could populate a data frame with your calculations and then plot them (this is what we did in the solution).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQCtsioouhNF",
        "colab_type": "text"
      },
      "source": [
        "#### *Expand for Solution*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pcE8Q-h33Tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ***Solution***\n",
        "\n",
        "# we're going to create a data frame to hold all of the results.  Think of this\n",
        "# as a represntation of the \n",
        "results = pd.DataFrame(columns=['race', 'decile >=', 'true_positive_rate', 'false_positive_rate', 'positive_predictive_value', 'negative_predictive_value'])\n",
        "\n",
        "for race in ['Caucasian', 'African-American']:\n",
        "    df_for_race = df[df['race'] == race]\n",
        "    y = df_for_race['two_year_recid']\n",
        "    for thresh in range(1, 11):\n",
        "        yhat = df_for_race['decile_score'] >= thresh\n",
        "        true_positive_rate = ((y == 1) & (yhat == 1)).sum() / (y == 1).sum()\n",
        "        false_positive_rate = ((y == 0) & (yhat == 1)).sum() / (y == 0).sum()\n",
        "        if (yhat == 1).sum() == 0:\n",
        "            positive_predictive_value = float('nan')\n",
        "        else:\n",
        "            positive_predictive_value = ((y == 1) & (yhat == 1)).sum() / (yhat == 1).sum()\n",
        "\n",
        "        if (yhat == 0).sum() == 1:\n",
        "            negative_predictive_value = float('nan')\n",
        "        else:\n",
        "            negative_predictive_value = ((y == 0) & (yhat == 0)).sum() / (yhat == 0).sum()\n",
        "\n",
        "        results = results.append({'race': race,\n",
        "                                  'decile >=': str(thresh),\n",
        "                                  'true_positive_rate': true_positive_rate,\n",
        "                                  'false_positive_rate': false_positive_rate,\n",
        "                                  'positive_predictive_value': positive_predictive_value,\n",
        "                                  'negative_predictive_value': negative_predictive_value}, ignore_index=True)\n",
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q3JIKxZ223j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "legend_strings = []\n",
        "for race, df_by_race in results.groupby('race'):\n",
        "    plt.plot(df_by_race['false_positive_rate'], df_by_race['true_positive_rate'])\n",
        "    for _, row in df_by_race.iterrows():\n",
        "        ax.annotate(str(row[1]), (row[3], row[2]))\n",
        "    legend_strings.append(race)\n",
        "\n",
        "plt.legend(legend_strings)\n",
        "plt.xlabel('false positive rate')\n",
        "plt.ylabel('true positive rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8p1Po4-wfRN",
        "colab_type": "text"
      },
      "source": [
        "Notice how for a given threshold (annotated as text), the curves have vastly different false positive rates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqPwkkO7zYhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "legend_strings = []\n",
        "for race, df_by_race in results.groupby('race'):\n",
        "    plt.plot(df_by_race['negative_predictive_value'], df_by_race['positive_predictive_value'])\n",
        "    for _, row in df_by_race.iterrows():\n",
        "        ax.annotate(str(row[1]), (row[5], row[4]))\n",
        "    legend_strings.append(race)\n",
        "\n",
        "plt.legend(legend_strings)\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.xlabel('negative predictive value')\n",
        "plt.ylabel('positive predictive value')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cz8OZ5Qwl6F",
        "colab_type": "text"
      },
      "source": [
        "Notice how for a threshold of 4 or 5, the positive predictive values are almost identical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jZ1N6EMXzPY",
        "colab_type": "text"
      },
      "source": [
        "## Sanity Check Using Sklearn\n",
        "\n",
        "We can compare our calculations to the ROC curve (false positive rate versus true postive rate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlHJA8Jb4tOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "for race in ['African-American', 'Caucasian']:\n",
        "    df_filtered = df[df['race'] == race]\n",
        "    y = df_filtered['two_year_recid']\n",
        "    scores = df_filtered['decile_score']\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n",
        "    plt.plot(fpr, tpr)\n",
        "plt.legend(['Caucasian', 'African-American'])\n",
        "plt.xlabel('false positive rate')\n",
        "plt.ylabel('true positive rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}