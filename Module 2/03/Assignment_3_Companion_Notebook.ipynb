{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3 Companion Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%202/03/Assignment_3_Companion_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7_Yl4gzYH6K",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification Using Na&iuml;ve Bayes\n",
        "\n",
        "***Abstract***\n",
        "\n",
        "In this notebook you'll be implementing the Na&iuml;ve Bayes algorithm and applying it to the task of classifying the sentiment of a movie review.  We're going to take a mostly bottom-up approach here where we directly connect the math we've been learning with the algorithm.  At the end we'll compare our results to sklearn's built-in implementation of the algorithm to both sanity check our own implementation and potentially allow those who want to to run more experiments with an implementation that has more options.\n",
        "\n",
        "## Sentiment Analysis\n",
        "\n",
        "The [Wikipedia Article on Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) provides the following definition for sentiment analysis.\n",
        "\n",
        "> Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n",
        "\n",
        "In this notebook we'll be focusing on predicting the sentiment of a movie review from IMDB based on the text of the movie review.  This dataset is one that was originally used in a Kaggle competition called [Bag of Words meets Bag of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial) (you'll understand that joke by the end of this notebook!)\n",
        "\n",
        "The [data](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) consists of the following.\n",
        "\n",
        "> The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
        "\n",
        "Our goal will be to see if we can learn a model, using Na&iuml;ve Bayes on a training set to accurately estimate sentiment of new reviews.\n",
        "\n",
        "Without further ado, let's download and parse the data into a data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74jnAfgu1pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download',\n",
        "               'labeledTrainData.tsv',\n",
        "               quiet=False)\n",
        "df = pd.read_csv('labeledTrainData.tsv', header=0, delimiter='\\t')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1vAHoeEwDMn",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the average sentiment to see what we are dealing with (1 is positive sentiment and 0 is negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiyxr0P5vduW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['sentiment'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq6YwjQzwIFP",
        "colab_type": "text"
      },
      "source": [
        "Looks like we're dealing with a balanced set of positives and negatives.\n",
        "\n",
        "Next, let's look at a particular review.  To make the output look nicer, we'll create a [new Pandas series with line wrapping](https://www.geeksforgeeks.org/python-pandas-series-str-wrap/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4xQ5xN5wQqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this takes a little while to run\n",
        "reviews_wrapped = df['review'].str.wrap(80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN17b9RTxFaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(reviews_wrapped.iloc[20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caV4neeywOJ1",
        "colab_type": "text"
      },
      "source": [
        "## The Bag of Words Model\n",
        "\n",
        "We know that in order to apply Na&iuml;ve Bayes we need to convert each of our reviews into a vector of features.  There are lots of different methods to convert text into vectors.  In this notebook we'll be using a pretty basic (bugt suprisingly powerful) form of vectorization where we construct a feature vector with $k$ entries (where $k$ is the total number of unique words in the dataset) and for any particular review we set the corresponding entry to $1$ if that word appears in the dataset and $0$ otherwise.  This representation is called the ***bag of words*** since the encoding of the text into features is independent of where the words occur in the text (you could shuffle the words in the review and still have the same feature vector).  The [Wikipedia article on Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) has more information.\n",
        "\n",
        "Instead of writing our own code to convert from text to a bag of words representation we're going to use scikit learn's built-in [count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).  Before we apply it to the data, let's apply it to toy dataset to help you better understand the bag of words model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTuWKfBUa3zQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# the binary feature makes it so only the presence or absence of the word is\n",
        "# returned (rather than the count)\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "toy_data = ['This is review one.  It has some words.', 'This is review two.  It also has some words.']\n",
        "vectorizer.fit(toy_data)\n",
        "X_toy = vectorizer.transform(toy_data)\n",
        "X_toy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGW-JI4vbWRk",
        "colab_type": "text"
      },
      "source": [
        "The first thing to notice about the data is that it's stored in a [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix) format.  A sparse matrix is useful when the elements of your matrix are mostly zeros.  If you have a lot of unique words, but each piece of text only contains a small fraction of those words you will end up with a sparse matrix.  In this notebook we'll be limiting the size of our vocabulary and converting the matrix to dense.  This is to ease implementation.  If you want to leave things as a sparse matrix, please feel free to do so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0x2lYeTb5or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"feature vectors\", X_toy.todense())\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag3lrO9acJMh",
        "colab_type": "text"
      },
      "source": [
        "### Notebook Exercise 1\n",
        "\n",
        "Referencing back to what you know about the bag of words model and our toy data, explain why the vectors look the way they do.  Make up your own toy data (or add to ours) and see if the results make sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzAbKjrLcmMS",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4-MSR-5coJY",
        "colab_type": "text"
      },
      "source": [
        "***Solution***\n",
        "\n",
        "The first entry of the vector corresponds to the word 'also', which occurs only in the second review (thus the first line after feature vectors has a 0 for the first entry and the second line has a 1).  The second entry of the vector corresponds to the word 'has', which is in both reviews (so both vectors have a 1 there).  The pattern continues as you go through the vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Dz_L-pcuk1",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizing the Whole Dataset\n",
        "\n",
        "Now that you have a general idea what bag of words is all about, let's apply it to our movie reviews.  To make our lives easier we're only going to include words in our feature vector if they occur in at least 100 reviews.  Doing this will help with overfitting (although next assignment we will be learning another technique to deal with this).  While we're at it we'll also convert the sentiment labels to a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GJLVF0fv6Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(binary=True, min_df=100)\n",
        "vectorizer.fit(df['review'])\n",
        "X = vectorizer.transform(df['review']).todense()\n",
        "y = np.array(df['sentiment'])\n",
        "print(\"X.shape\", X.shape)\n",
        "print(\"y.shape\", y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dk2x8CCmTsy",
        "colab_type": "text"
      },
      "source": [
        "As a quick intuition builder, let's look at a word we think would probably differ across sentiment values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKUe7CbBmYwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terrible_index = vectorizer.get_feature_names().index('terrible')\n",
        "print(\"terrible occurs in\", X[y==1, terrible_index].mean(), \"for Y=1\")\n",
        "print(\"terrible occurs in\", X[y==0, terrible_index].mean(), \"for Y=0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHpeMudlmuzi",
        "colab_type": "text"
      },
      "source": [
        "Sorta makes sense (but please someone do some looking into what those reviews are where it terrible appears and it is Y=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOyVJscVzTIW",
        "colab_type": "text"
      },
      "source": [
        "## The General Form of Na&iuml;ve Bayes\n",
        "\n",
        "Last Assignment we showed how the Titanic problem could be solved using the Na&iuml;ve Bayes Model.  Specifically we computed what's known as the odds ratio.\n",
        "\n",
        "$$ \\frac{p(\\mathcal{Y} | \\mathcal{S}) p(\\mathcal{C}=1 | \\mathcal{S}) p(\\mathcal{M} | \\mathcal{S})p(\\mathcal{S})}{p(\\mathcal{Y}| \\neg \\mathcal{S}) p(\\mathcal{C}=1 | \\neg \\mathcal{S}) p(\\mathcal{M} | \\neg \\mathcal{S})p(\\neg \\mathcal{S})}$$\n",
        "\n",
        "Recall that $\\mathcal{Y}$ meant young passenger, $\\mathcal{C}$ represented fare class, $\\mathcal{M}$ represented \"is male\", and $\\mathcal{S}$ represented survival.\n",
        "\n",
        "Hopefully it is pretty clear that while we derived the formula in the specific case of the Titanic model, the logic we applied is completely general.  If we assume that we are doing binary classification provided values $x_1, x_2, \\ldots, x_d$, then the odds ratio can be written in the following way.\n",
        "\n",
        "$$\\frac{p(Y=1 | X_1=x_1, X_2=x_2, \\ldots, X_d = x_d)}{p(Y=0 | X_1=x_1, X_2=x_2, \\ldots, X_d = x_d)} = \\frac{p(Y=1) \\times p(X_1=x_1 | Y=1) \\times \\ldots \\times p(X_d = x_d | Y=1)}{p(Y=0) \\times p(X_1=x_1 | Y=0) \\times \\ldots \\times p(X_d = x_d | Y=0)}$$\n",
        "\n",
        "We also learned last assignment that if this odds ratio is greater than 1, we should predict positive.  While the odds ratio is a totally valid way to attack the problem, it can be numerically unstable.  Therefore, most people use the log odds ratio instead (you just hit both sides with a log)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbooHuyOg5Jy",
        "colab_type": "text"
      },
      "source": [
        "### Notebook Exercise 2\n",
        "\n",
        "Compute the log odds ratio for the Na&iuml;ve Bayes algorithm by taking the log of both sides of the preceding equation.  Simplify as much as possible using properties of logarithms.  What must be true of the log odds ratio in order to predict $Y=1$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL5KBe6AhFKj",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYuIhdC5hHXX",
        "colab_type": "text"
      },
      "source": [
        "***Solution***\n",
        "\n",
        "The answer is pretty straight forward.  We use the fact that log of a ratio is the difference in the log of the numerator and denominator.  We also use the fact that log of a product is the sum of the logs of each of the terms.\n",
        "\n",
        "$$\\log \\left ( \\frac{p(Y=1 | X_1=x_1, X_2=x_2, \\ldots, X_d = x_d)}{p(Y=0 | X_1=x_1, X_2=x_2, \\ldots, X_d = x_d)} \\right) = \\\\\n",
        " \\log p(Y=1) - \\log p(Y=0) + \\sum_{i=1}^d \\left ( \\log p(X_i = x_i | Y=1) - \\log p(X_i = x_i | Y=0) \\right )$$\n",
        "\n",
        " If the log odds ratio is greater than or equal to 0, we should predict $Y=1$ (at least our probability will be at least 0.5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_5dM72fhp2B",
        "colab_type": "text"
      },
      "source": [
        "## Fitting the Parameters of the Model\n",
        "\n",
        "What we see from looking at the log odds ratio equation is that in order to apply the model we must have an estimate of the following probabilities.\n",
        "* $p(Y=0)$\n",
        "* $p(Y=1)$\n",
        "* $p(X_i = x_i | Y=0)$ (for $i$ from $1$ to $d$)\n",
        "* $p(X_i = x_i | Y=1)$ (for $i$ from $1$ to $d$)\n",
        "\n",
        "The strategy for fitting these parameters will be the same as was described in the previous assignment (we'll see a more formal justification of why this works in the next assignment).  In order to estimate a probability, we'll just count the number of times the event occurs across the dataset.\n",
        "\n",
        "For instance, if we want to estimate $p(Y=0)$ we would count the number of instances in the dataset where $Y=0$ and divide that by the total number of instances in the dataset.  If we wanted to esitmated $p(X_i = 1 | Y = 0)$ (suppose X_i represents the word \"terrible\") we would count the number of reviews that included the word terrible and had sentiment 0 and divide that by the number of reviews that were sentiment 0 (hopefully that makes sense given the definition of $p(A|B) = \\frac{p(A, B)}{p(B)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7gV9EXjCo-",
        "colab_type": "text"
      },
      "source": [
        "### Notebook Exercise 3\n",
        "\n",
        "Write a function that takes as input $X$ and $y$ and returns a tuple containing the following elements (in this order)\n",
        "1.  The probability of Y=1\n",
        "2.  The probability of Y=0\n",
        "3.  A vector where the $i$th entry represents $p(X_i=1|Y=1)$\n",
        "4.  A vector where the $i$th entry represents $p(X_i=1|Y=0)$\n",
        "\n",
        "To help you out we've included a unit test and a function stub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjCBhMwifr_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_nb_model(X, y):\n",
        "    \"\"\" Fit the parameters of a Naive Bayes model given a bag of words model\n",
        "        with binary counts (X) and class labels (y).\n",
        "\n",
        "        Returns: a tuple with the following components in order\n",
        "                  The probability of Y=1,\n",
        "                  The probability of Y=0,\n",
        "                  A vector where the $i$th entry represents $p(X_i=1|Y=1)$\n",
        "                  A vector where the $i$th entry represents $p(X_i=1|Y=0)$\n",
        "    \n",
        "    >>> X = np.array([[1, 0, 1], [1, 0, 0], [0, 0, 1], [1, 1, 1]])\n",
        "    >>> y = np.array([1, 0, 1, 0])\n",
        "    >>> fit_nb_model(X, y)\n",
        "    (0.5, 0.5, array([0.5, 0. , 1. ]), array([1. , 0.5, 0.5]))\n",
        "    \"\"\"\n",
        "    return None\n",
        "\n",
        "import doctest\n",
        "doctest.testmod()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlsDMdu6mGSH",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Dq1qkSmJ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ***Solution***\n",
        "def fit_nb_model(X, y):\n",
        "    \"\"\" Fit the parameters of a Naive Bayes model given a bag of words model\n",
        "        with binary counts (X) and class labels (y).\n",
        "\n",
        "        Returns: a tuple with the following components in order\n",
        "                  The probability of Y=1,\n",
        "                  The probability of Y=0,\n",
        "                  A vector where the $i$th entry represents $p(X_i=1|Y=1)$\n",
        "                  A vector where the $i$th entry represents $p(X_i=1|Y=0)$\n",
        "    \n",
        "    >>> X = np.array([[1, 0, 1], [1, 0, 0], [0, 0, 1], [1, 1, 1]])\n",
        "    >>> y = np.array([1, 0, 1, 0])\n",
        "    >>> fit_nb_model(X, y)\n",
        "    (0.5, 0.5, array([0.5, 0. , 1. ]), array([1. , 0.5, 0.5]))\n",
        "    \"\"\"\n",
        "    X_1 = X[y == 1, :]\n",
        "    X_0 = X[y == 0, :]\n",
        "    return y.mean(), 1 - y.mean(), X_1.mean(axis=0), X_0.mean(axis=0)\n",
        "\n",
        "import doctest\n",
        "doctest.testmod()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwLpZg99m49t",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Once we have the model parameters fitted, we have to be able to do inference on new data.  This will amount to computing our log odds ratio and seeing if it is greater than 0.  If the log odds ratio is greater than 0, we return a predicted sentiment of 1, otherwise we return a sentiment of 0.\n",
        "\n",
        "Before having you implement the inference code, let's split into a train and test set and use your code to fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pqId7PinR2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "levGedmAni7o",
        "colab_type": "text"
      },
      "source": [
        "### Notebook Exercise 4\n",
        "\n",
        "Implement a function called `get_nb_prediction` that takes as input the NB model (`p_y_1, p_y_0, p_x_y_1, p_x_y_0`) and a dataset (X), computes the log odds ratio and returns a vector of predictions for that data.\n",
        "\n",
        "To get you started we have a function stub.\n",
        "\n",
        "Hint: you can write a nested loop with one over the data points and one over the features and compute log probabilities as you go.  (you can speed this up using numpy features, but don't worry about computational speed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWqYKRugnd-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X):\n",
        "    \"\"\" Predict the labels for the data X given the Naive Bayes model \"\"\"\n",
        "    return None\n",
        "\n",
        "# here is some test code that will call your model on the first 100 test points\n",
        "# (for speed of development).\n",
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmCyhu3Qowix",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-PDlTioz6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ***Solution***\n",
        "# This will be really slow, but we hope it is more readable for folks that are\n",
        "# less familiar with numpy\n",
        "def get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X):\n",
        "    \"\"\" Predict the labels for the data X given the Naive Bayes model \"\"\"\n",
        "    log_odds_ratios = np.zeros(X.shape[0])\n",
        "    for i in range(X.shape[0]): # loop over data points\n",
        "        print(\"progress\", i/X.shape[0])\n",
        "        log_odds_ratios[i] += np.log(p_y_1) - np.log(p_y_0)\n",
        "        for j in range(X.shape[1]): #loop over words\n",
        "            if X[i, j] == 1:\n",
        "                log_odds_ratios[i] += np.log(p_x_y_1[0, j]) - np.log(p_x_y_0[0, j])\n",
        "            else:\n",
        "                log_odds_ratios[i] += np.log(1 - p_x_y_1[0, j]) - np.log(1 - p_x_y_0[0, j])\n",
        "    return (log_odds_ratios >= 0).astype(np.float)\n",
        "\n",
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4J40YUyq8YZ",
        "colab_type": "text"
      },
      "source": [
        "## Calculating Accuracy\n",
        "\n",
        "Now we'll test our model on all of the test data and see how accurate it is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCrjZDS9rWpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test)\n",
        "print(\"accuracy is\", (y_pred == y_test).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppeSOZZ60JRP",
        "colab_type": "text"
      },
      "source": [
        "## Sanity Check\n",
        "\n",
        "Just to see if we're in the ball park, let's try scikit learn's built-in implementation of Na&iuml;ve Bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iOy-js8YjeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "np.mean(y_pred == y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}