{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3 Companion Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%202/03/Assignment_3_Companion_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74jnAfgu1pk",
        "colab_type": "code",
        "outputId": "6fe9d5f8-f4b8-4ac9-e941-f3af7c255fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import gdown\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download',\n",
        "               'labeledTrainData.tsv',\n",
        "               quiet=False)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download\n",
            "To: /content/labeledTrainData.tsv\n",
            "33.6MB [00:00, 89.3MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'labeledTrainData.tsv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orVv-oMLu4dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('labeledTrainData.tsv', header=0, delimiter='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1vAHoeEwDMn",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the average sentiment to see what we are dealing with (1 is positive sentiment and 0 is negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiyxr0P5vduW",
        "colab_type": "code",
        "outputId": "a7bac941-4022-4c08-a6c6-5c354ce8973f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df['sentiment'].mean()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq6YwjQzwIFP",
        "colab_type": "text"
      },
      "source": [
        "Looks like we're dealing with a balanced set of positives and negatives.\n",
        "\n",
        "Next, let's look at a particular review.  To make the output look nicer, we'll create a [new Pandas series with line wrapping](https://www.geeksforgeeks.org/python-pandas-series-str-wrap/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4xQ5xN5wQqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this takes a little while to run\n",
        "reviews_wrapped = df['review'].str.wrap(80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN17b9RTxFaH",
        "colab_type": "code",
        "outputId": "ec755332-d32d-4fdc-9bd9-4e4b493865b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "print(reviews_wrapped.iloc[20])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\Soylent Green\\\" is one of the best and most disturbing science fiction movies\n",
            "of the 70's and still very persuasive even by today's standards. Although flawed\n",
            "and a little dated, the apocalyptic touch and the environmental premise (typical\n",
            "for that time) still feel very unsettling and thought-provoking. This film's\n",
            "quality-level surpasses the majority of contemporary SF flicks because of its\n",
            "strong cast and some intense sequences that I personally consider classic. The\n",
            "New York of 2022 is a depressing place to be alive, with over-population,\n",
            "unemployment, an unhealthy climate and the total scarcity of every vital food\n",
            "product. The only form of food available is synthetic and distributed by the\n",
            "Soylent company. Charlton Heston (in a great shape) plays a cop investigating\n",
            "the murder of one of Soylent's most eminent executives and he stumbles upon\n",
            "scandals and dark secrets... The script is a little over-sentimental at times\n",
            "and the climax doesn't really come as a big surprise, still the atmosphere is\n",
            "very tense and uncanny. The riot-sequence is truly grueling and easily one of\n",
            "the most macabre moments in 70's cinema. Edward G. Robinson is ultimately\n",
            "impressive in his last role and there's a great (but too modest) supportive role\n",
            "for Joseph Cotton (\\\"Baron Blood\\\", \\\"The Abominable Dr. Phibes\\\"). THIS is\n",
            "Science-Fiction in my book: a nightmarish and inevitable fade for humanity! No\n",
            "fancy space-ships with hairy monsters attacking our planet.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caV4neeywOJ1",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizing the Data\n",
        "\n",
        "We know that in order to apply Na&iuml;ve Bayes we need to convert each of our reviews into a vector of features.  There are lots of different methods to convert text into vectors.  In this notebook we'll be using a super basic form of this where we construct a feature vector with $k$ entries (where $k$ is the total number of unique words in the dataset) and for any particular review we set the corresponding entry to $1$ if that word appears in the dataset and $0$ otherwise.  This representation is called the bag of words since the encoding of the text into features is independent of where the words occur in the text (you could shuffle the words in the review and still have the same feature vector).\n",
        "\n",
        "A more complete description of bag of words is given in TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfIlEWYJyH41",
        "colab_type": "text"
      },
      "source": [
        "Here we're going to use scikit learn's built-in [count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GJLVF0fv6Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "vectorizer.fit(df['review'])\n",
        "X = vectorizer.transform(df['review'])\n",
        "y = np.array(df['sentiment'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOyVJscVzTIW",
        "colab_type": "text"
      },
      "source": [
        "## Fitting the Parameters of the Na&iuml;ve Bayes Model\n",
        "\n",
        "We know from our work in the main document that the maximum likelihood values of the parameters are given by TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppeSOZZ60JRP",
        "colab_type": "text"
      },
      "source": [
        "To understand what these counts mean, we can look at one of the entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VbdstqZzins",
        "colab_type": "code",
        "outputId": "5eb2c830-372f-49cb-9210-d79499ef92a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_xcounts_for_sentiment(X, y):\n",
        "    # the y[y==1] is a trick to get around the\n",
        "    return X[y == 1, :].sum(axis=0), X[y == 0, :].sum(axis=0)\n",
        "\n",
        "xcount_for_sentiment_1, xcount_for_sentiment_0 = get_xcounts_for_sentiment(X, y)\n",
        "print(\"66150th word count for positive sentiment\", xcount_for_sentiment_1[0, 66150])\n",
        "print(\"66150th word count for negative sentiment\", xcount_for_sentiment_0[0, 66150])\n",
        "print(\"66150th word is\", vectorizer.get_feature_names()[66150])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66150th word count for positive sentiment 217\n",
            "66150th word count for negative sentiment 1118\n",
            "66150th word is terrible\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAQAzgfH1J6y",
        "colab_type": "text"
      },
      "source": [
        "Check your understanding by interpreting this output.  What might be going on with the other 217 reviews?  We leave it to you to write the code to examine them if you want to dig into this further (we're happy to help if you have questions about how to do this)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Tbz4WU1kIt",
        "colab_type": "text"
      },
      "source": [
        "## Implementing Na&iuml;ve Bayes\n",
        "\n",
        "First, we'll divide our data into a train and test set.  Then we'll walk you through the implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3vpfVmgywes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uL7DCFo2CE6",
        "colab_type": "text"
      },
      "source": [
        "Next we're going to fit the parameters of our model.  Recall that we need to compute the probability of each possible sentiment value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GNY-uD32CuJ",
        "colab_type": "code",
        "outputId": "a0be9de8-800e-4fe8-95b0-412fd7df77e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def get_y_counts_for_sentiment(y):\n",
        "    return y.sum(), (y == 0).sum()\n",
        "\n",
        "y_count_for_sentiment_1, y_count_for_sentiment_0 = get_y_counts_for_sentiment(y_train)\n",
        "\n",
        "def get_p_y(y_count_for_sentiment_1, y_count_for_sentiment_0):\n",
        "    z = y_count_for_sentiment_1 + y_count_for_sentiment_0\n",
        "    return y_count_for_sentiment_1 / z, y_count_for_sentiment_0 / z\n",
        "\n",
        "p_sentiment_1, p_sentiment_0 = get_p_y(y_count_for_sentiment_1, y_count_for_sentiment_0)\n",
        "print(p_sentiment_1, p_sentiment_0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5007466666666667 0.4992533333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYvB-MFx8lmB",
        "colab_type": "text"
      },
      "source": [
        "We also need to compute the probability of each of the features conditioned on a particular sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag7_QQTa1GPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_p_x_given_y(xcount_for_sentiment_1,\n",
        "                    xcount_for_sentiment_0,\n",
        "                    ycount_for_sentiment_1,\n",
        "                    ycount_for_sentiment_0):\n",
        "    p_of_word_sentiment_1 = xcount_for_sentiment_1 / ycount_for_sentiment_1\n",
        "    p_of_word_sentiment_0 = xcount_for_sentiment_0 / ycount_for_sentiment_0\n",
        "    return p_of_word_sentiment_1, p_of_word_sentiment_0\n",
        "\n",
        "x_count_for_sentiment_1, x_count_for_sentiment_0 = get_xcounts_for_sentiment(X_train, y_train)\n",
        "\n",
        "# add smoothing if desired\n",
        "smoothing = 1\n",
        "\n",
        "x_count_for_sentiment_1 += smoothing\n",
        "x_count_for_sentiment_0 += smoothing\n",
        "\n",
        "p_of_word_sentiment_1, p_of_word_sentiment_0 = get_p_x_given_y(x_count_for_sentiment_1,\n",
        "                                                               x_count_for_sentiment_0,\n",
        "                                                               y_count_for_sentiment_1,\n",
        "                                                               y_count_for_sentiment_0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-OFP_ip8tn2",
        "colab_type": "code",
        "outputId": "393d2200-d9e7-46f4-b951-6b149f3cdef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# remember that word 66150 is \"terrible\"\n",
        "print(p_of_word_sentiment_1[0,66150])\n",
        "print(p_of_word_sentiment_0[0,66150])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01736074129300245\n",
            "0.0894135241961329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50Ip0wjNID3P",
        "colab_type": "text"
      },
      "source": [
        "## Classifying New Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw2ytG9zHvSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_log_posterior(test_point, p_of_sentiment, p_of_word):\n",
        "    log_likelihood = np.log(p_of_sentiment)\n",
        "    for index in test_point.indices:\n",
        "        log_likelihood += np.log(p_of_word[0, index])\n",
        "    return log_likelihood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQfAKHnIIcS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_correct = 0\n",
        "for i, point in enumerate(X_test):\n",
        "    log_likelihood_sentiment_1 = get_log_posterior(point, p_sentiment_1, p_of_word_sentiment_1)\n",
        "    log_likelihood_sentiment_0 = get_log_posterior(point, p_sentiment_0, p_of_word_sentiment_0)\n",
        "    y_pred = float(log_likelihood_sentiment_1 > log_likelihood_sentiment_0)\n",
        "    y_actual = y_test[i]\n",
        "    num_correct += float(y_pred == y_actual)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5p_bKW6Iqjm",
        "colab_type": "code",
        "outputId": "ecc73c5a-fb6d-452b-9a50-1d52bfbb8ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"accuracy is\", num_correct / len(y_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 0.85056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRQqneMWdOD0",
        "colab_type": "text"
      },
      "source": [
        "## Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iOy-js8YjeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e889e0d9-d0b5-432d-959f-856b49706d34"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "np.mean(y_pred == y_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTCLUBSK-9p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}