{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4 Companion Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jl-WOh0s173d",
        "s3SSv1_zFW7Y"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%202%5C04%5CAssignment_4_Companion_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw7g0QF3UbU9",
        "colab_type": "text"
      },
      "source": [
        "# More Na&iuml;ve Bayes with Smoothing and N-Grams\n",
        "\n",
        "***Abstract***\n",
        "\n",
        "In this notebook you'll be explanding on our previous implementation of the Na&iuml;ve Bayes algorithm and exploring the fun new world of bigrams which are \"pretty useful\". We'll practice some techniques for manipulating text and take advantage of some of sklearn's built-in implementations. This notebook will help you practice some tools to use for a sequence learning mini-project.\n",
        "\n",
        "\n",
        "***Takeaways***\n",
        "\n",
        "*   When we use Na&iuml;ve Bayes, it's beneficial to use \"smoothing\" to avoid the problems that arise because our model can run into problems (probabilities of 0, or values of negative infinity) when a word shows up in our test set that was not in our training set.\n",
        "*   In English (and most other languages), word order matters (\"people eat\" is different than \"eat people\"). Therefore, it's often useful to included some information about word order in our model. If we include complete information about word order (e.g., this word showed up 100 words before that word), our model would require extreme amounts of memory. We can typically get a way with a little information about word order, such as with bigrams, which are just pairs of words.\n",
        "*   We can use bigrams (or other ngrams) in various applications in natural language processing including text classification, sequence prediction, word generation, spell checking. These applications also extend beyond actual words or letters, and these techniques can really be applied to any sequences of items (e.g. series of coin flips).  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KbrKo8jWMNn",
        "colab_type": "text"
      },
      "source": [
        "##Sentiment Analysis with Na&iuml;ve Bayes (now with smoothing)\n",
        "Building on the last assignment, we'll be focusing on predicting the sentiment of a movie review from IMDB based on the text of the movie review.  This dataset is one that was originally used in a Kaggle competition called [Bag of Words meets Bag of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial). \n",
        "\n",
        "Again, the [data](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) consist of the following.\n",
        "\n",
        "> The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
        "\n",
        "Our goal will be to see if we can learn a model, using Na&iuml;ve Bayes on a training set to accurately estimate sentiment of new reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTUjkvrNV1_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "7a46fa4e-0f99-4a1c-f26b-2ef19b598bd6"
      },
      "source": [
        "# Import our movie review data (from last assignment)\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download',\n",
        "               'labeledTrainData.tsv',\n",
        "               quiet=False)\n",
        "df = pd.read_csv('labeledTrainData.tsv', header=0, delimiter='\\t')\n",
        "df"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?authuser=0&id=1Z8bwIBa_0gFe9-C2W0goZ72lQfFMbxjS&export=download\n",
            "To: /content/labeledTrainData.tsv\n",
            "33.6MB [00:01, 32.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7759_3</td>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3630_4</td>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9495_8</td>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>3453_3</td>\n",
              "      <td>0</td>\n",
              "      <td>It seems like more consideration has gone into...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>5064_1</td>\n",
              "      <td>0</td>\n",
              "      <td>I don't believe they made this film. Completel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>10905_3</td>\n",
              "      <td>0</td>\n",
              "      <td>Guy is a loser. Can't get girls, needs to buil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>10194_3</td>\n",
              "      <td>0</td>\n",
              "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>8478_8</td>\n",
              "      <td>1</td>\n",
              "      <td>I saw this movie as a child and it broke my he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  sentiment                                             review\n",
              "0       5814_8          1  With all this stuff going down at the moment w...\n",
              "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
              "3       3630_4          0  It must be assumed that those who praised this...\n",
              "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
              "...        ...        ...                                                ...\n",
              "24995   3453_3          0  It seems like more consideration has gone into...\n",
              "24996   5064_1          0  I don't believe they made this film. Completel...\n",
              "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
              "24998  10194_3          0  This 30 minute documentary Buñuel made in the ...\n",
              "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
              "\n",
              "[25000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QYpM8qiXFDf",
        "colab_type": "text"
      },
      "source": [
        "Just like last time, we will convert from text to a **bag of words** representation using scikit learn's built-in [count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Last time, we only included words in our feature vector if they occur in at least 100 reviews. Now we reduce this limitation, so that included words only need to appear in at least 20 reviews. Note the the previous shape of X was:\n",
        "X.shape (25000, 3833), and now it should have many more features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ0UE8IoW8KN",
        "colab_type": "code",
        "outputId": "195cc182-93b1-4866-82f8-d7c10eef9d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = np.array(df['sentiment'])\n",
        "\n",
        "dfX_train, dfX_test, y_train, y_test = train_test_split(df['review'], y)\n",
        "print(\"df_train.shape\",dfX_train.shape)\n",
        "print(\"y_train.shape\",y_train.shape)\n",
        "print(\"dfX_test.shape\",dfX_test.shape)\n",
        "print(\"y_test.shape\",y_test.shape)\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True, min_df = 20) #convert a collection of text documents into a matrix of token counts\n",
        "vectorizer.fit(dfX_train) #learn a vocabulary dictionary of all tokens in the raw documents\n",
        "\n",
        "X_train = vectorizer.transform(dfX_train).todense() #transform to a document-term matrix\n",
        "X_test = vectorizer.transform(dfX_test).todense()\n",
        "print(\"X_test.shape\",X_test.shape)\n",
        "print(\"X_train.shape\", X_train.shape)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_train.shape (18750,)\n",
            "y_train.shape (18750,)\n",
            "dfX_test.shape (6250,)\n",
            "y_test.shape (6250,)\n",
            "X_test.shape (6250, 10096)\n",
            "X_train.shape (18750, 10096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ed5j_vlr-Cu",
        "colab_type": "text"
      },
      "source": [
        "We also split the data into training and test right from the start. We'll check to make sure our data is organized properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYaufLj5g65W",
        "colab_type": "code",
        "outputId": "a931db52-5a92-4fa9-bb4b-3a00dbbbef91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Looking at a review to make sure things work\n",
        "reviews_wrapped = dfX_train.str.wrap(80)\n",
        "terrible_index = vectorizer.get_feature_names().index('terrible')\n",
        "print(\"terrible occurs in\", X_train[y_train==1, terrible_index].mean(), \"for Y=1\")\n",
        "print(\"terrible occurs in\", X_train[y_train==0, terrible_index].mean(), \"for Y=0\")\n",
        "print(reviews_wrapped.iloc[1]) # Just in case you want to read a random review"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "terrible occurs in 0.01775590972296502 for Y=1\n",
            "terrible occurs in 0.09020316987554515 for Y=0\n",
            "Any movie with \\National Lampoon\\\" in the title is absolutely guaranteed to die\n",
            "a death in London,England,Paris,France,Rome,Italy,and anywhere in Germany.It may\n",
            "be an institution in the U.S. but it is practically unknown in Europe to the\n",
            "larger audience.\\\"National Lampoon's European Vacation\\\" is unlikely to rectify\n",
            "that situation. The appalling Griswalds are just that - appalling.They are not\n",
            "funny. Clearly Mr Chevy Chase thinks he's funny, after all Miss B.di Angelo\n",
            "laughs a lot at his jokes,but she's getting paid for it and didn't have to fork\n",
            "out £2.50 for the privilege. The section set in England is typical.The same old\n",
            "same old TV performers, Messrs Idle,Smith,Coltrane,Miss M.Lippman trot out the\n",
            "same old same old tired clichés,Mr Chase gets lost in the hotel\n",
            "corridor....yawn,yawn,yawn.. Bucking - ham Palace,Big Ben......I feel cheated\n",
            "that we never saw bobbies on bicycles two-by-two.........rosie red cheeks on the\n",
            "little chil - dren,need I go on? The English are buffoons,the French vicious -\n",
            "tongued Yank-haters.The Germans pompous and puffed up,(don't mention the\n",
            "war,Clark),and the Italians lecherous bottom-pinchers.Have I forgotten anything?\n",
            "Every possible \\\"comic\\\" situation is worked to death,Mr Chase gurns\n",
            "desperately,Miss di Angelo dimples sweetly,the children are embarrassingly bad.\n",
            "The fact that this franchise ran as long as it did must bring comfort to those\n",
            "who propound that you never lose money by underestimating public taste.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7esYTmjpfqgr",
        "colab_type": "text"
      },
      "source": [
        "### Fitting the Parameters of the Model & Making Predictions\n",
        "\n",
        "As you may recall from the last assignment:\n",
        "\n",
        "What we see from looking at the log odds ratio equation is that in order to apply the model we must have an estimate of the following probabilities.\n",
        "* $p(Y=0)$\n",
        "* $p(Y=1)$\n",
        "* $p(X_i = x_i | Y=0)$ (for $i$ from $1$ to $d$)\n",
        "* $p(X_i = x_i | Y=1)$ (for $i$ from $1$ to $d$)\n",
        "\n",
        "The MLE problem in the earlier part of the assignment gave a more formal justification of this process. \n",
        "\n",
        "We'll start with the solution from the last assignment. Here, we count the number of times the event occurs across the dataset in order to estimate a probability.\n",
        "\n",
        "For instance, if we want to estimate $p(Y=0)$ we would count the number of instances in the dataset where $Y=0$ and divide that by the total number of instances in the dataset.  If we wanted to estimated $p(X_i = 1 | Y = 0)$ (suppose X_i represents the word \"terrible\") we would count the number of reviews that included the word terrible and had sentiment 0 and divide that by the number of reviews that were sentiment 0. Remember that $p(A|B) = \\frac{p(A, B)}{p(B)}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvXgkDmK60vI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Essentially same functions as solutions from previous assignment\n",
        "def fit_nb_model(X, y):\n",
        "    X_1 = np.asarray(X[y == 1, :]) # all reviews with sentiment 1\n",
        "    X_0 = np.asarray(X[y == 0, :])\n",
        "    return y.mean(), 1 - y.mean(), X_1.mean(axis=0), X_0.mean(axis=0)\n",
        "\n",
        "def get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X):\n",
        "    \"\"\" Predict the labels for the data X given the Naive Bayes model \"\"\"\n",
        "    log_odds_ratios = np.zeros(X.shape[0])\n",
        "    for i in range(X.shape[0]): # loop over data points\n",
        "        if i%(X.shape[0]/10) == 0: print(\"progress\", i/X.shape[0])\n",
        "        log_odds_ratios[i] += np.log(p_y_1) - np.log(p_y_0)\n",
        "        for j in range(X.shape[1]): #loop over words\n",
        "            if X[i, j] == 1: #if this example includes word j\n",
        "                log_odds_ratios[i] += np.log(p_x_y_1[j]) - np.log(p_x_y_0[j])\n",
        "            else: \n",
        "                log_odds_ratios[i] += np.log(1 - p_x_y_1[j]) - np.log(1 - p_x_y_0[j])\n",
        "    return (log_odds_ratios >= 0).astype(np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUouXFCjnxmG",
        "colab_type": "text"
      },
      "source": [
        "## Testing the Accuracy of the Model from Assignment 3\n",
        "The following lines of code run the solution function (above) and calculate the accuracy. We're just using the first 100 entries for faster computation. \n",
        "\n",
        "**What do you observe happens when you run this?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNU-v-DzjcY-",
        "colab_type": "code",
        "outputId": "e15165c2-d3fa-48a7-b17e-1f231d560516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model(X_train, y_train)\n",
        "y_pred_train = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_train[:100,:])\n",
        "print(\"Train accuracy is\", (y_pred_train == y_train[:100]).astype(np.float).mean()) #also only need to compare first 100 y\n",
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
        "print(\"Test accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 0.0\n",
            "progress 0.1\n",
            "progress 0.2\n",
            "progress 0.3\n",
            "progress 0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "progress 0.5\n",
            "progress 0.6\n",
            "progress 0.7\n",
            "progress 0.8\n",
            "progress 0.9\n",
            "Train accuracy is 0.88\n",
            "progress 0.0\n",
            "progress 0.1\n",
            "progress 0.2\n",
            "progress 0.3\n",
            "progress 0.4\n",
            "progress 0.5\n",
            "progress 0.6\n",
            "progress 0.7\n",
            "progress 0.8\n",
            "progress 0.9\n",
            "Test accuracy is 0.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TXmSxXgtqxQ",
        "colab_type": "text"
      },
      "source": [
        "You may have noticed a \"RuntimeWarning: divide by zero encountered in log\". It is common practice to use something call [Laplace smoothing or additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to avoid this and other issues. \n",
        "\n",
        "We can see just how common this practice is by running our classification again with the sklearn toolbox."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1nt-J91rXCy",
        "colab_type": "text"
      },
      "source": [
        "Note what happens when you set alpha=0 (which means no smoothing), would be similar to what we have implemented above. \n",
        "\n",
        "You will likely see a warning that says: \"alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
        "  'setting alpha = %.1e' % _ALPHA_MIN).\"\n",
        "\n",
        "You can instead change this value to alpha = 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJn8MvlckybG",
        "colab_type": "code",
        "outputId": "47a179ae-ec64-4612-f0bf-5e0e9a9ecf0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB(alpha=0) \n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test[:100,:])\n",
        "np.mean(y_pred == y_test[:100])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ButisCHuopUG",
        "colab_type": "text"
      },
      "source": [
        "### A bit about Laplace Smoothing\n",
        "\n",
        "Imagine that you are trying to classify a review that contains the word 'awesomesauce' and that your classifier hasn't seen this word before. Naturally, the probability P(x_i|y) will be 0, making log of this go to negative infinity. Eeek!\n",
        "\n",
        "Even if the training corpus is very large, it does not contain all possible words (or combinations of words... foreshaddowing!). \n",
        "\n",
        "This is a common problem in NLP, but thankfully it has an easy fix: smoothing. This technique consists in adding a constant to each count in the P(x_i|y) formula, with the most basic type of smoothing being called add-one (Laplace) smoothing, where the constant is just 1. \n",
        "\n",
        "*Sources: [Medium article](https://medium.com/datadriveninvestor/implementing-naive-bayes-for-sentiment-analysis-in-python-951fa8dcd928) and [section 3.4 of this paper](http://www.diva-portal.org/smash/get/diva2:839705/FULLTEXT01.pdf).*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS0fO4gcugQ4",
        "colab_type": "text"
      },
      "source": [
        "### Notebook Exercise 1\n",
        "\n",
        "Revise the fit_nb_model_smooth() function below to include Laplace smoothing (right now, it's the same as our original function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdRWrkLw2Cn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "beee2272-06d7-4a9a-e342-2161f3e6caf9"
      },
      "source": [
        "def fit_nb_model_smooth(X, y,alpha):\n",
        "  X_1 = np.asarray(X[y == 1, :]) # all reviews with sentiment 1\n",
        "  X_0 = np.asarray(X[y == 0, :])\n",
        "  return y.mean(), 1 - y.mean(), X_1.mean(axis=0), X_0.mean(axis=0)\n",
        "\n",
        "\n",
        "# Code to call and run your new fitting with alpha =1\n",
        "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model_smooth(X_train, y_train,1) #Model with smoothing\n",
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
        "print(\"accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y_test"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 0.0\n",
            "progress 0.1\n",
            "progress 0.2\n",
            "progress 0.3\n",
            "progress 0.4\n",
            "progress 0.5\n",
            "progress 0.6\n",
            "progress 0.7\n",
            "progress 0.8\n",
            "progress 0.9\n",
            "accuracy is 0.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl-WOh0s173d",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YgnBMrdyMso",
        "colab_type": "code",
        "outputId": "3288bca4-1f09-449e-ebbf-f63c30d52f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "def fit_nb_model_smooth(X, y,alpha):\n",
        "    X_1 = np.asarray(X[y == 1, :])\n",
        "    X_0 = np.asarray(X[y == 0, :])\n",
        "    N_1,V_1 = X_1.shape\n",
        "    N_0,V_0 = X_0.shape #should actually be the same size in our case\n",
        "    return y.mean(), 1 - y.mean(), np.divide(X_1.sum(axis=0)+1,N_1), np.divide(X_0.sum(axis=0)+1,N_0) \n",
        "\n",
        "\n",
        "# Code to call and run your new fitting with alpha =1\n",
        "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model_smooth(X_train, y_train,1) #Model with smoothing\n",
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
        "print(\"accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 0.0\n",
            "progress 0.1\n",
            "progress 0.2\n",
            "progress 0.3\n",
            "progress 0.4\n",
            "progress 0.5\n",
            "progress 0.6\n",
            "progress 0.7\n",
            "progress 0.8\n",
            "progress 0.9\n",
            "accuracy is 0.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x08auTXzqD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "978cdd42-cf4b-49ae-d76c-d8bf29de1dcd"
      },
      "source": [
        "X_0 = np.asarray(X_train[y_train == 0, :])\n",
        "print(np.shape(X_0))\n",
        "print(np.shape(p_x_y_0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9401, 10039)\n",
            "(10039,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-woHjkn2zz6",
        "colab_type": "text"
      },
      "source": [
        "#***Sequence Prediction: Whats next?***\n",
        "\n",
        "Time, the final, final frontier. Or, that thread that ties together events. These events could be major things in your life, or just a series of word or musical notes. Often the sequence of events contains very important information that we want to capture. For example, word order is often essential to the meaning of a phrase. Or, a set of descending musical notes gives a different feeling than an ascending one.\n",
        "\n",
        "We can apply sequence information for various tasks, for example: \n",
        "-Predicting if a coin is fair or not \n",
        "- Choosing the next song to play in a playlist based on previous songs\n",
        "- Deciding what to suggest when watching videos\n",
        "- Determining if text came from one corpus or another\n",
        "- Predicting the next word (or letter) in a phrase (or word)\n",
        "\n",
        "In this notebook, we'll explore these last two applications, building on our movie reivew data set (out of convenience).\n",
        "\n",
        "***What other applications can you think of for sequence prediction?***\n",
        "\n",
        "###One problem with the Bag of Words\n",
        "Until now, we've been using the Bag of Words strategy. Here, words are treated individually. Consider two sentences \"big red machine and carpet\" and \"big red carpet and machine\". If you use a bag of words approach, you will get the same vectors for these two sentences. However, we can clearly see that in the first sentence we are talking about a \"big red machine\", while the second sentence contains information about the \"big red carpet\". Hence, context information is very important. \n",
        "\n",
        "[*Source*](https://stackabuse.com/python-for-nlp-developing-an-automatic-text-filler-using-n-grams/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd0WtdrM1K-7",
        "colab_type": "text"
      },
      "source": [
        "##\"Hello Bigrams\" and Other Ngrams\n",
        "\n",
        "Enter, the bigram (two-words). The bigram model can help us capture context information.\n",
        "\n",
        "**What is an N-gram?**\n",
        "A contiguous sequence of N items from a given sample of text or speech or any other sequence.  Here an item can be a character, a word or a sentence and N can be any integer. When N is 2, we call the sequence a bigram. Similarly, a sequence of 3 items is called a trigram, and so on.\n",
        "\n",
        "Bigrams (or ngrams) are one strategy for sequence prediction (though they can also be used for other tasks including classification). We'll explore these below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WpKd3-s3sDF",
        "colab_type": "text"
      },
      "source": [
        "### A mini-primer on text formatting and tokens\n",
        "\n",
        "If you're really comfortable in python, you can skip this.\n",
        "\n",
        "Otherwise, take a look at some examples of manipulating text (this might be useful in your next assignment if you use your own text with its own formatting quirks).\n",
        "\n",
        "**Start by writing some text and assigning it to s.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCoFD2wok82Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write some text and store it in the variable s\n",
        "s = \"This is where your sentence goes, or however much I want to write >\"\\\n",
        "\"your own profound thoughts! Oh-my! You should probably include some repeat phrases like repeat phrases or like repeat phrases\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkpNUjU_3nyB",
        "colab_type": "code",
        "outputId": "27e961d3-a77f-48b3-8730-f20015ab242f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import re\n",
        "# Now, we need to tidy up our text to get rid of the riff-raff (to make it more consistent to find common pairs of words)\n",
        "# Convert to lowercases\n",
        "s = s.lower()  \n",
        "# Replace all none alphanumeric characters with spaces\n",
        "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
        "# Replace series of spaces with single space\n",
        "s = re.sub(' +',' ',s) \n",
        "print(s)\n",
        "\n",
        "# Tokens refer to the words in our case (whatever the chunks are that we are breaking things up into)\n",
        "tokens = [token for token in s.split(\" \") if token != \"\"] # Break sentence in the token, remove empty tokens\n",
        "print(tokens)\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is where your sentence goes or however much i want to write your own profound thoughts oh my you should probably include some repeat phrases like repeat phrases or like repeat phrases\n",
            "['this', 'is', 'where', 'your', 'sentence', 'goes', 'or', 'however', 'much', 'i', 'want', 'to', 'write', 'your', 'own', 'profound', 'thoughts', 'oh', 'my', 'you', 'should', 'probably', 'include', 'some', 'repeat', 'phrases', 'like', 'repeat', 'phrases', 'or', 'like', 'repeat', 'phrases']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIXth_XDrTI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "# Essentially the same as above, but putting it into a function for later\n",
        "def clean_text(s):\n",
        "  s = s.lower() # Convert to lowercases\n",
        "  s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s) # Replace all non alphanumeric characters with spaces\n",
        "  s = re.sub(' +',' ',s) # Replace series of spaces with single space\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxPlyTo01hou",
        "colab_type": "text"
      },
      "source": [
        "Now that we have a sense of how the word parsing works, let's move this into a function called generate_ngrams that takes in our text and the number of words to group by (n). For a bigram, we should choose n=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJopKT2x34or",
        "colab_type": "code",
        "outputId": "ad03189d-2908-4bb1-ea87-274a63b6e783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "def generate_ngrams(s, n):   \n",
        "    s = clean_text(s) # Clean text\n",
        "    # Break sentence in the token, remove empty tokens\n",
        "    tokens = [token for token in s.split(\" \") if token != \"\"]    \n",
        "    # Generate sequences of tokens starting from different\n",
        "    # elements of the list of tokens.\n",
        "    # The parameter in the range() function controls how many sequences\n",
        "    # to generate.\n",
        "    sequences = [tokens[i:] for i in range(n)]\n",
        "    # Use the zip function to help us generate n-grams\n",
        "    # Concatentate the tokens into ngrams and return\n",
        "    ngrams = zip(*sequences)\n",
        "    #ngrams = zip(*[tokens[i:] for i in range(n)]) # Or you could combine into one line like this.\n",
        "    return [\" \".join(ngram) for ngram in ngrams]\n",
        "\n",
        "generate_ngrams(s,2)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this is',\n",
              " 'is where',\n",
              " 'where your',\n",
              " 'your sentence',\n",
              " 'sentence goes',\n",
              " 'goes or',\n",
              " 'or however',\n",
              " 'however much',\n",
              " 'much i',\n",
              " 'i want',\n",
              " 'want to',\n",
              " 'to write',\n",
              " 'write your',\n",
              " 'your own',\n",
              " 'own profound',\n",
              " 'profound thoughts',\n",
              " 'thoughts you',\n",
              " 'you should',\n",
              " 'should probably',\n",
              " 'probably include',\n",
              " 'include some',\n",
              " 'some repeat',\n",
              " 'repeat phrases',\n",
              " 'phrases like',\n",
              " 'like repeat',\n",
              " 'repeat phrases',\n",
              " 'phrases or',\n",
              " 'or like',\n",
              " 'like repeat',\n",
              " 'repeat phrases']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZ0jAmQe4pV",
        "colab_type": "text"
      },
      "source": [
        "Yay, ngrams! (Bigrams in our case.)\n",
        "Often, it will be more interesting to collapse things and look at a the frequencies of our ngrams.\n",
        "\n",
        "This example uses the nltk library to create your ngrams (a second option for future work)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzgQaVhm_nZ_",
        "colab_type": "code",
        "outputId": "87ebb866-e862-409a-8eb7-3f89f51cf39e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import collections\n",
        "from nltk.util import ngrams\n",
        "\n",
        "s = clean_text(s) # Clean text\n",
        "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
        "bigramWords = list(ngrams(tokens, 2))\n",
        "bigramFreq = collections.Counter(bigramWords)\n",
        "\n",
        "bigramFreq.most_common(10)\n",
        "#print(bigramFreq.most_common(10))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('repeat', 'phrases'), 3),\n",
              " (('like', 'repeat'), 2),\n",
              " (('this', 'is'), 1),\n",
              " (('is', 'where'), 1),\n",
              " (('where', 'your'), 1),\n",
              " (('your', 'sentence'), 1),\n",
              " (('sentence', 'goes'), 1),\n",
              " (('goes', 'or'), 1),\n",
              " (('or', 'however'), 1),\n",
              " (('however', 'much'), 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41V1Y5TPyYSt",
        "colab_type": "text"
      },
      "source": [
        "##Applying bigrams to the movie reviews\n",
        "Now, we'll explore bigrams with a larger data set. Oh hey, we already loaded in a bunch of movie reviews, how convenient!\n",
        "\n",
        "First, we'll want to clean up the text in our reviews a bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEvVQAyJ2vIG",
        "colab_type": "text"
      },
      "source": [
        "###Notebook Exercise 2\n",
        "\n",
        "Check 5-10 of the movie reviews to make sure our text cleaning is working. dfX_train contains all the movie reviews in the training set.\n",
        "\n",
        "(a) Consider the word parsing done in the previous example. Think about how your text cleaning (e.g., removing hyphens) affected the bigrams. What are some potential limitations of this type of text processing?\n",
        "\n",
        "(b) Determine how you want to inspect the reviews without checking them all. Do you want to just look at the raw text for each? Or perhaps you want to look at the bigrams for each example? Or perhaps you want to look at \n",
        "\n",
        "(c) Update the clean_text() function to remove the most common formatting issue.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9XCTRDS4dkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e9b1c3f7-a55c-4709-e96b-0150166ad0a7"
      },
      "source": [
        "#Here's a little code to get you started\n",
        "print(\"An original text from the training set:\")\n",
        "print(dfX_train[2])\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An original text from the training set:\n",
            "The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp0aOzzm2vzT",
        "colab_type": "text"
      },
      "source": [
        "####Expand for solution\n",
        "\n",
        "(a) Depending on the context of what you want to do, you may choose to keep in things like hyphens (perhaps ninety-nine is more meaningful together than apart). Or perhaps appostrophes are meaningful in your corpus. Whenever we are dealing with text, we want to think about our syntax choices, like including flags for start and end words in a sentence (instead of assuming one infinite string of words). \n",
        "\n",
        "(b) \"Dealer's choice\" here, some examples below (you don't need to use this approach). \n",
        "\n",
        "(c) Adding the line s = re.sub('br />',' ',s)  will remove those pesky break indicators, just make sure to do this before you run the line that removes all of the non-alphaneumeric characters. Otherwise, you'll end up with what appears to be the word \"br\" everywhere, and we doubt every movie reviewer is freezing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZMbLwn7jY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "3b2dd29b-eb7c-43be-9a8b-a2f3caedd421"
      },
      "source": [
        "# Checking the first few movie reviews\n",
        "for i in range(10):\n",
        "  origtext = list(dfX_train)[i]\n",
        "  print(\"Original text: \",origtext)\n",
        "  cleaned = clean_text(origtext)\n",
        "  print(\"Cleaned  text: \",cleaned)\n",
        "  tokens = [token for token in cleaned.split(\" \") if token != \"\"]\n",
        "  bigramWords = list(ngrams(tokens, 2))\n",
        "  bigramFreq = collections.Counter(bigramWords)\n",
        "  print(\"Bigrams: \", bigramFreq.most_common(10))\n",
        "  print('\\n')\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text:  Damn, I thought I'd seen some bad westerns. Can't top this one though. Hell I think I'd rather have my eyes stapled open for a Trinity Triple Feature for cryin out loud. I dont think I'll be able to watch Ben Hur again without laughing my ass off. Just really bad.<br /><br />But hey, if you like stupid westerns with acknowledged stars in the thing take a peek at Shoot Out with Gregory Peck. It's just as bad, but much funnier. 1/10\n",
            "Cleaned  text:  damn i thought i d seen some bad westerns can t top this one though hell i think i d rather have my eyes stapled open for a trinity triple feature for cryin out loud i dont think i ll be able to watch ben hur again without laughing my ass off just really bad br br but hey if you like stupid westerns with acknowledged stars in the thing take a peek at shoot out with gregory peck it s just as bad but much funnier 1 10\n",
            "Bigrams:  [(('i', 'd'), 2), (('think', 'i'), 2), (('damn', 'i'), 1), (('i', 'thought'), 1), (('thought', 'i'), 1), (('d', 'seen'), 1), (('seen', 'some'), 1), (('some', 'bad'), 1), (('bad', 'westerns'), 1), (('westerns', 'can'), 1)]\n",
            "\n",
            "\n",
            "Original text:  I was surprised, that ''The Secret Fury'' was an enjoyable good film...... Probably because, I didn't have any expectations for this movie..... Though, the film does have it's plot holes..... I would say, that you couldn't guess who was behind the whole scheme, until the very end of the movie..... At first, I thought, it was Robert Ryan, using the same method, like ''Gaslight'' where husband tries to drive his wife mad, but I was wrong...... The main problem, with the movie is, they drive at a whole other direction, which gave no clues at the beginning...... I thought, Robert Ryan & Claudette Colbert carried their parts well...... Plus, Vivian Vance, a fine character actress, who steals scenes in this one...... Those who like movies, that keeps you guessing, will like this one......\n",
            "Cleaned  text:  i was surprised that the secret fury was an enjoyable good film probably because i didn t have any expectations for this movie though the film does have it s plot holes i would say that you couldn t guess who was behind the whole scheme until the very end of the movie at first i thought it was robert ryan using the same method like gaslight where husband tries to drive his wife mad but i was wrong the main problem with the movie is they drive at a whole other direction which gave no clues at the beginning i thought robert ryan claudette colbert carried their parts well plus vivian vance a fine character actress who steals scenes in this one those who like movies that keeps you guessing will like this one \n",
            "Bigrams:  [(('i', 'was'), 2), (('the', 'movie'), 2), (('i', 'thought'), 2), (('robert', 'ryan'), 2), (('this', 'one'), 2), (('was', 'surprised'), 1), (('surprised', 'that'), 1), (('that', 'the'), 1), (('the', 'secret'), 1), (('secret', 'fury'), 1)]\n",
            "\n",
            "\n",
            "Original text:  Reading my review of THE HOUSE THAT SCREAMED, many may assume that I'm some 14 year old who thinks SCREAM is considered \\classic\\\" horror. This is not the case, as I'm 30 years old and have been watching horror films for most of my life. But admittedly, I'm a child of the 80's that grew up on slasher/zombie/ghost/cannibal, etc...types of horror films - so I do typically prefer horror films that are more graphic and faster-paced. Just like someone who can appreciate different music, painting, or in this case, film - but not necessarily like them - I can appreciate why some people may enjoy this sort of film...I just don't...<br /><br />THE HOUSE THAT SCREAMED is an exceedingly dull and tedious film about a school for wayward girls. The heavy-handed mistress of the school rules with an iron hand (or whip in some cases...) to keep the girls in line. She has a young son who creeps around and peeps on the girls while they shower (in their nightgowns no less (?!?)...), and meanwhile, girls are disappearing from the school as they are the victim of a murderer who's lurking about the campus...<br /><br />I can see why THE HOUSE THAT SCREAMED is often compared to SUSPIRIA (which is a masterpiece of a film in my opinion...), in terms of the atmosphere of the school itself and the interaction between the girls and their guardians - but this film is so dull and uneventful that I could barely stay awake. I'm all for \\\"tension\\\" and \\\"suspense\\\" in horror films - but this film held neither for me. Luckily, I wasn't expecting a whole lot going into this one, so I can't say I was really disappointed - THE HOUSE THAT SCREAMED just reinforced the fact that I personally don't typically enjoy most horror films much older than from the 70's. This isn't a hard and fast rule, but those that I HAVE enjoyed definitely seem to be more of the exception. Probably a \\\"must-see\\\" for horror fans who enjoy more understated and suggestive horror films - but as I don't really know too many fans of that sort of material, I can't really recommend this one...4/10\"\n",
            "Cleaned  text:  reading my review of the house that screamed many may assume that i m some 14 year old who thinks scream is considered classic horror this is not the case as i m 30 years old and have been watching horror films for most of my life but admittedly i m a child of the 80 s that grew up on slasher zombie ghost cannibal etc types of horror films so i do typically prefer horror films that are more graphic and faster paced just like someone who can appreciate different music painting or in this case film but not necessarily like them i can appreciate why some people may enjoy this sort of film i just don t br br the house that screamed is an exceedingly dull and tedious film about a school for wayward girls the heavy handed mistress of the school rules with an iron hand or whip in some cases to keep the girls in line she has a young son who creeps around and peeps on the girls while they shower in their nightgowns no less and meanwhile girls are disappearing from the school as they are the victim of a murderer who s lurking about the campus br br i can see why the house that screamed is often compared to suspiria which is a masterpiece of a film in my opinion in terms of the atmosphere of the school itself and the interaction between the girls and their guardians but this film is so dull and uneventful that i could barely stay awake i m all for tension and suspense in horror films but this film held neither for me luckily i wasn t expecting a whole lot going into this one so i can t say i was really disappointed the house that screamed just reinforced the fact that i personally don t typically enjoy most horror films much older than from the 70 s this isn t a hard and fast rule but those that i have enjoyed definitely seem to be more of the exception probably a must see for horror fans who enjoy more understated and suggestive horror films but as i don t really know too many fans of that sort of material i can t really recommend this one 4 10 \n",
            "Bigrams:  [(('of', 'the'), 6), (('horror', 'films'), 6), (('the', 'house'), 4), (('house', 'that'), 4), (('that', 'screamed'), 4), (('that', 'i'), 4), (('i', 'm'), 4), (('i', 'can'), 4), (('don', 't'), 3), (('the', 'school'), 3)]\n",
            "\n",
            "\n",
            "Original text:  Man's Castle is a wonderful example of a Pre-Code film. It involves realistic events with truly enjoyable and imperfect characters. Spencer Tracy plays Bill, a free soul without a dime in his pocket. He makes a living doing odd jobs and traveling to a new city when he gets bored of his surroundings. One night, he meets Trina, a beauty by any standards who is cold and alone. She has refused to resort to prostitution so she has not eaten for several days, but the two take very well to each other and form a relationship. His free spirit tempts him to leave her, so life is rocky, but there is a true spark between the two, even if they live in a shack by the river.<br /><br />Tracy is one of the great actors of the silver screen. His characters are amazing and relatable. We can see his thoughts on his face, making him easy to identify with, even if we believe he is behaving badly. Young is great in pre-code films. Her character is very sweet but far from perfect, making her all the more likable.<br /><br />Pre-code elements include skinny dipping, pregnancy before marriage, and crime.\n",
            "Cleaned  text:  man s castle is a wonderful example of a pre code film it involves realistic events with truly enjoyable and imperfect characters spencer tracy plays bill a free soul without a dime in his pocket he makes a living doing odd jobs and traveling to a new city when he gets bored of his surroundings one night he meets trina a beauty by any standards who is cold and alone she has refused to resort to prostitution so she has not eaten for several days but the two take very well to each other and form a relationship his free spirit tempts him to leave her so life is rocky but there is a true spark between the two even if they live in a shack by the river br br tracy is one of the great actors of the silver screen his characters are amazing and relatable we can see his thoughts on his face making him easy to identify with even if we believe he is behaving badly young is great in pre code films her character is very sweet but far from perfect making her all the more likable br br pre code elements include skinny dipping pregnancy before marriage and crime \n",
            "Bigrams:  [(('pre', 'code'), 3), (('is', 'a'), 2), (('she', 'has'), 2), (('the', 'two'), 2), (('even', 'if'), 2), (('br', 'br'), 2), (('of', 'the'), 2), (('man', 's'), 1), (('s', 'castle'), 1), (('castle', 'is'), 1)]\n",
            "\n",
            "\n",
            "Original text:  That was the first thing that sprang to mind as I watched the closing credits to Europa make there was across the screen, never in my entire life have I seen a film of such technical genius, the visuals of Europa are so impressive that any film I watch in it's wake will only pale in comparison, forget your Michael Bay, Ridley Scott slick Hollywood cinematography, Europa has more ethereal beauty than anything those two could conjure up in a million years. Now I'd be the first to hail Lars von Trier a genius just off the back of his films Breaking the Waves and Dancer in the Dark, but this is stupid, the fact that Europa has gone un-noticed by film experts for so long is a crime against cinema, whilst overrated rubbish like Crouching Tiger, Hidden Dragon and Life is Beautiful clean up at the academy awards (but what do the know) Europa has been hidden away, absent form video stores and (until recently) any British TV channels. <br /><br />The visuals in Europa are not MTV gloss; it's not a case of style over substance, its more a case of substance dictating style. Much like his first film The Element of Crime, von Trier uses the perspective of the main character to draw us into his world, and much like Element, the film begins with the main character (or in the case of Europa, we the audience) being hypnotized. As we move down the tracks, the voice of the Narrator (Max von Sydow) counts us down into a deep sleep, until we awake in Europa. This allows von Trier and his three cinematographers to pay with the conventions of time and imagery, there are many scenes in Europa when a character in the background, who is in black and white, will interact with a person in the foreground who will be colour, von Trier is trying to show us how much precedence the coloured item or person has over the plot, for instance, it's no surprise that the first shot of Leopold Kessler (Jean-marc Barr) is in colour, since he is the only character who's actions have superiority over the film. <br /><br />The performances are good, they may not be on par with performances in later von Trier films, but that's just because the images are sometimes so distracting that you don't really pick up on them the first time round. But I would like to point out the fantastic performance of Jean-Marc Barr in the lead role, whose blind idealism is slowly warn down by the two opposing sides, until he erupts in the films final act. Again, muck like The Element of Crime, the film ends with our hero unable to wake up from his nightmare state, left in this terrible place, with only the continuing narration of von Sydow to seal his fate. Europa is a tremendous film, and I cant help thinking what a shame that von Trier has abandoned this way of filming, since he was clearly one of the most talented visual directors working at that time, Europa, much like the rest of his cinematic cannon is filled with a wealth of iconic scenes. His dedication to composition and mise-en-scene is unrivalled, not to mention his use of sound and production design. But since his no-frills melodramas turned out to be Breaking the Waves and Dancer in the Dark then who can argue, but it does seems like a waste of an imaginative talent. 10/10\n",
            "Cleaned  text:  that was the first thing that sprang to mind as i watched the closing credits to europa make there was across the screen never in my entire life have i seen a film of such technical genius the visuals of europa are so impressive that any film i watch in it s wake will only pale in comparison forget your michael bay ridley scott slick hollywood cinematography europa has more ethereal beauty than anything those two could conjure up in a million years now i d be the first to hail lars von trier a genius just off the back of his films breaking the waves and dancer in the dark but this is stupid the fact that europa has gone un noticed by film experts for so long is a crime against cinema whilst overrated rubbish like crouching tiger hidden dragon and life is beautiful clean up at the academy awards but what do the know europa has been hidden away absent form video stores and until recently any british tv channels br br the visuals in europa are not mtv gloss it s not a case of style over substance its more a case of substance dictating style much like his first film the element of crime von trier uses the perspective of the main character to draw us into his world and much like element the film begins with the main character or in the case of europa we the audience being hypnotized as we move down the tracks the voice of the narrator max von sydow counts us down into a deep sleep until we awake in europa this allows von trier and his three cinematographers to pay with the conventions of time and imagery there are many scenes in europa when a character in the background who is in black and white will interact with a person in the foreground who will be colour von trier is trying to show us how much precedence the coloured item or person has over the plot for instance it s no surprise that the first shot of leopold kessler jean marc barr is in colour since he is the only character who s actions have superiority over the film br br the performances are good they may not be on par with performances in later von trier films but that s just because the images are sometimes so distracting that you don t really pick up on them the first time round but i would like to point out the fantastic performance of jean marc barr in the lead role whose blind idealism is slowly warn down by the two opposing sides until he erupts in the films final act again muck like the element of crime the film ends with our hero unable to wake up from his nightmare state left in this terrible place with only the continuing narration of von sydow to seal his fate europa is a tremendous film and i cant help thinking what a shame that von trier has abandoned this way of filming since he was clearly one of the most talented visual directors working at that time europa much like the rest of his cinematic cannon is filled with a wealth of iconic scenes his dedication to composition and mise en scene is unrivalled not to mention his use of sound and production design but since his no frills melodramas turned out to be breaking the waves and dancer in the dark then who can argue but it does seems like a waste of an imaginative talent 10 10\n",
            "Bigrams:  [(('in', 'the'), 7), (('von', 'trier'), 6), (('the', 'first'), 4), (('it', 's'), 3), (('europa', 'has'), 3), (('in', 'europa'), 3), (('case', 'of'), 3), (('much', 'like'), 3), (('of', 'the'), 3), (('the', 'film'), 3)]\n",
            "\n",
            "\n",
            "Original text:  Ooverall, the movie was fairly good, a good action plot with a fair amount of explosions and fight scenes, but Chuck Norris did hardly anything, except for disarm the bomb and shoot a few characters. The movie was very similar to the events of Sept. 11, with a bin laden-like terrorist sending a video to the president (Urich) and threatening to detonate it. Judson Mills had some superb action roles, taking out Rashid's compound and various kick-butt roles but, there was a lack of Chuck Norris. Judson took over most of the action, leaving Joshua (chuck) with Que on her computer. But, overall, it was realistic and didn't lack the action, but only did it on Mr. Norris' part. I gave the film 7/10.\n",
            "Cleaned  text:  ooverall the movie was fairly good a good action plot with a fair amount of explosions and fight scenes but chuck norris did hardly anything except for disarm the bomb and shoot a few characters the movie was very similar to the events of sept 11 with a bin laden like terrorist sending a video to the president urich and threatening to detonate it judson mills had some superb action roles taking out rashid s compound and various kick butt roles but there was a lack of chuck norris judson took over most of the action leaving joshua chuck with que on her computer but overall it was realistic and didn t lack the action but only did it on mr norris part i gave the film 7 10 \n",
            "Bigrams:  [(('the', 'movie'), 2), (('movie', 'was'), 2), (('with', 'a'), 2), (('chuck', 'norris'), 2), (('to', 'the'), 2), (('the', 'action'), 2), (('ooverall', 'the'), 1), (('was', 'fairly'), 1), (('fairly', 'good'), 1), (('good', 'a'), 1)]\n",
            "\n",
            "\n",
            "Original text:  Outside the household is a different world and the family struggle to tread the line between Dads authority and their hopes and dreams.<br /><br /> The period is captured; The bakelite light swithes, the Georgian floorpan, the picture rails, the wall paper, the short skirts, the cheeky lads, the Mini van, shiny modern mangles....<br /><br /> The location is captured; A wind lashed glacier hewn rocky landscape, walls of local stone, community, freedom.<br /><br /> But there is much much more; Childhood, happiness, sensuality, pride, values, freedom, authority, rebellion, violence (in the deepest sense), love, struggle, puberty, naivety, morality, trust, faith, deceit, machismo, manners, maturity, loss, poverty, sacrifice, horror, acceptence, revelation, comedy and parenthood are all there. (And in no particular order!).<br /><br /> This film is a richly woven expression of family tensions that are as relevent today as ever. The fact that some of these aren't tackled directly is testament in itself to the attitudes of the day but the fact that they are all here is a testament to the acting skill, the story and the direction.<br /><br /> If there's anything bad about this film, it's that all this deeply entrenched and wonderfully enacted tension is swept away a little too lightly towards the end. Maybe I missunderstand - the doom and gloom felt by many teenagers really does disappear if they deal with it (**) - maybe the film is trying to send even that message too - well worth doing.<br /><br /> What is the film trying to say? Kids: Parents were young too, parents struggle too, everyone makes mistakes, everyone learns, things change, struggle can end happily. Parents: Don't try too hard! Try to remember that your support is the key to their well being.<br /><br /> It sounds simplistic doesn't it? Sometimes the film feels like that too but it's then that you notice how much is being being challenged and uncovered.<br /><br /> The film is a classic.<br /><br />(**) - Not the problems themsleves.\n",
            "Cleaned  text:  outside the household is a different world and the family struggle to tread the line between dads authority and their hopes and dreams br br the period is captured the bakelite light swithes the georgian floorpan the picture rails the wall paper the short skirts the cheeky lads the mini van shiny modern mangles br br the location is captured a wind lashed glacier hewn rocky landscape walls of local stone community freedom br br but there is much much more childhood happiness sensuality pride values freedom authority rebellion violence in the deepest sense love struggle puberty naivety morality trust faith deceit machismo manners maturity loss poverty sacrifice horror acceptence revelation comedy and parenthood are all there and in no particular order br br this film is a richly woven expression of family tensions that are as relevent today as ever the fact that some of these aren t tackled directly is testament in itself to the attitudes of the day but the fact that they are all here is a testament to the acting skill the story and the direction br br if there s anything bad about this film it s that all this deeply entrenched and wonderfully enacted tension is swept away a little too lightly towards the end maybe i missunderstand the doom and gloom felt by many teenagers really does disappear if they deal with it maybe the film is trying to send even that message too well worth doing br br what is the film trying to say kids parents were young too parents struggle too everyone makes mistakes everyone learns things change struggle can end happily parents don t try too hard try to remember that your support is the key to their well being br br it sounds simplistic doesn t it sometimes the film feels like that too but it s then that you notice how much is being being challenged and uncovered br br the film is a classic br br not the problems themsleves \n",
            "Bigrams:  [(('br', 'br'), 9), (('is', 'a'), 4), (('the', 'film'), 4), (('br', 'the'), 3), (('film', 'is'), 3), (('and', 'the'), 2), (('is', 'captured'), 2), (('are', 'all'), 2), (('this', 'film'), 2), (('the', 'fact'), 2)]\n",
            "\n",
            "\n",
            "Original text:  This is what the musical genre was made of. Humor, talent, romance, and action all rolled into one.<br /><br />Frank Sinatra was wonderful. Nothing else needs to be said. Marlon Brando, although not a singer, did a great job winning the hearts of many with his portrayal of Sky Masterson. The fact that he couldn't sing added to his character. The ladies in the film were alright, but the men in the movie definitely stole the show.<br /><br />It is a true classic that can be appreciated at any age. It connects with all audiences and makes you smile and laugh.<br /><br />Definitely a movie to be watched and enjoyed!\n",
            "Cleaned  text:  this is what the musical genre was made of humor talent romance and action all rolled into one br br frank sinatra was wonderful nothing else needs to be said marlon brando although not a singer did a great job winning the hearts of many with his portrayal of sky masterson the fact that he couldn t sing added to his character the ladies in the film were alright but the men in the movie definitely stole the show br br it is a true classic that can be appreciated at any age it connects with all audiences and makes you smile and laugh br br definitely a movie to be watched and enjoyed \n",
            "Bigrams:  [(('br', 'br'), 3), (('to', 'be'), 2), (('in', 'the'), 2), (('this', 'is'), 1), (('is', 'what'), 1), (('what', 'the'), 1), (('the', 'musical'), 1), (('musical', 'genre'), 1), (('genre', 'was'), 1), (('was', 'made'), 1)]\n",
            "\n",
            "\n",
            "Original text:  \\I went to the movies, to see 'Beat Street' / it wasn't bad, it was kinda' neat / 'Krush Groove' was a flick, that I didn't mind / but when it came to 'Rappin', I drew the line.\\\" Word to your mother.<br /><br />Want me to stop?<br /><br />That's just a small sample of the stupa-fly style of rhymin' on display in this waste of film and location permits. This movie is seriously wack (thats 80s-speak for just f*cking awful). As an emcee, Mario Van Peebles is one hell of an actor. And as an actor, Mario Van Peebles is one hell of a bodybuilder.<br /><br />Any film calling itself \\\"Rappin'\\\" had better deliver at that genre's highest standard of the time. So why were 6 year olds rolling in the aisles, even back in the day when standards were so knee-high-to-\\\"Webster\\\"-low? Because this rap is weak. So weak that not even B.E.T. or Comedy Central will touch it with a 10-foot gold-rope chain.<br /><br />Blondie's \\\"Rapture\\\" is def poetry next to this bit of Dr. Suess in the hood. So don't be a boobie, avoid this movie!<br /><br />\"\n",
            "Cleaned  text:   i went to the movies to see beat street it wasn t bad it was kinda neat krush groove was a flick that i didn t mind but when it came to rappin i drew the line word to your mother br br want me to stop br br that s just a small sample of the stupa fly style of rhymin on display in this waste of film and location permits this movie is seriously wack thats 80s speak for just f cking awful as an emcee mario van peebles is one hell of an actor and as an actor mario van peebles is one hell of a bodybuilder br br any film calling itself rappin had better deliver at that genre s highest standard of the time so why were 6 year olds rolling in the aisles even back in the day when standards were so knee high to webster low because this rap is weak so weak that not even b e t or comedy central will touch it with a 10 foot gold rope chain br br blondie s rapture is def poetry next to this bit of dr suess in the hood so don t be a boobie avoid this movie br br \n",
            "Bigrams:  [(('br', 'br'), 5), (('in', 'the'), 3), (('of', 'the'), 2), (('this', 'movie'), 2), (('as', 'an'), 2), (('mario', 'van'), 2), (('van', 'peebles'), 2), (('peebles', 'is'), 2), (('is', 'one'), 2), (('one', 'hell'), 2)]\n",
            "\n",
            "\n",
            "Original text:  Before he became defined as Nick Charles in the Thin Man Series, William Powell played another urbane detective named Philo Vance. The supporting cast is strong in this early talkie, and Powell's star quality is evident. Mary Astor, who eight years later would be defined by her portrayal of Brigid O'Shaughnessy, does a good job here as the featured woman who finds herself in the middle of it all.\n",
            "Cleaned  text:  before he became defined as nick charles in the thin man series william powell played another urbane detective named philo vance the supporting cast is strong in this early talkie and powell s star quality is evident mary astor who eight years later would be defined by her portrayal of brigid o shaughnessy does a good job here as the featured woman who finds herself in the middle of it all \n",
            "Bigrams:  [(('in', 'the'), 2), (('before', 'he'), 1), (('he', 'became'), 1), (('became', 'defined'), 1), (('defined', 'as'), 1), (('as', 'nick'), 1), (('nick', 'charles'), 1), (('charles', 'in'), 1), (('the', 'thin'), 1), (('thin', 'man'), 1)]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDRJFEs-2yFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(s):\n",
        "  s = s.lower() # Convert to lowercases\n",
        "  s = re.sub('<br />',' ',s) #Added this new line to get rid of the breaks\n",
        "  s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s) # Replace all non alphanumeric characters with spaces\n",
        "  s = re.sub(' +',' ',s) # Replace series of spaces with single space\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsh1wWW07g4i",
        "colab_type": "text"
      },
      "source": [
        "#### Now, let's clean all the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JMfLVyry0z4",
        "colab_type": "code",
        "outputId": "4d18101e-ae26-4cfc-c5e8-4415687cacfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "def clean_series_data(sdata):\n",
        "  sdata = list(sdata)\n",
        "  for i in range(len(sdata)):\n",
        "    sdata[i] = clean_text(sdata[i])\n",
        "    if i%(len(sdata)/5)==0:\n",
        "      print(sdata[i]) #Printing occasional text can be helpful for making sure that your cleaning is working how you want it to. Or you can comment this out.\n",
        "  return sdata\n",
        "\n",
        "dfX_train = clean_series_data(dfX_train)\n",
        "dfX_test = clean_series_data(dfX_test)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "if i could say it was better than gymkata i at least felt my money was not totally wasted then i saw steven segal s on deadly ground this movie should see a resurrection though on mst 3k if santa claus conquers the martians could make tom servo s head explode one wonders what mayhem this movie could cause there is a very good reason why kurt thomas never had a movie career the writers of this dreck should be forced to wear placards every day of their lives that say bitch slap me i was a writer on gymkata \n",
            "what a piece of stupid tripe i won t even waste time evaluating any of the points of this show it s not worth the time the one comment i will make is why get such a dumb inarticulate doofus to be the star there aren t many more dismal testimonials to the deteriorating mental condition of the networks than the fact that fox has stated it will not bring back john doe a decent series but will bring back brain dead drivel like joe millionaire for yet another round of killing the brain cells of the american public fox has lost it imho \n",
            "full house is a wonderful sitcom that is about a dad danny tanner whose wife had just died in a car crash so danny asks his brother in law jesse katsopolis and best friend joey gladstone to help raise his three girls donna jo dj tanner stephanie tanner and michelle tanner this is my favorite show ever and i can watch it all day long and something on full house is always making me laugh and there are sad parts also there is never a dull moment in full house the main characters are played by bob saget danny tanner john stamos jesse katsopolis dave coulier joey gladstone candace cameron dj tanner jodie sweetin stephanie tanner and mary kate ashley olsen michelle tanner \n",
            "the comedic might of pryor and gleason couldn t save this dog from the tissue thin plot weak script dismal acting and laughable continuity in editing this mess together it has a very few memorable moments but the well dries up quickly as a kid i remember this as a luke warm vehicle for the two actor comedians but there was always something strange about the flow and feeling of what was being conveyed in each scene and how this ties to the plot overall watching it again after many years it screams schlock a mania i m not so concerned with the racial controversy as i wouldn t mind seeing a movie take that issue on with a little levity the most obvious fault to me is that the scenes are laid out like a jumbled non related series of 2 minute situation comedy bits any not very good ones at that that were stapled together by the editor after an all nighter at the local watering hole characters change feelings and motivations on a dime without rhyme nor reason between scenes and within scenes making this feel as though no one had any idea of what to get out of the screenplay not that it was any gem to start with i feel bad for the two actors whose legacy is marred by this disaster that should never have been made maybe my sense of humor has become too refined \n",
            "since the day i saw this film when it came out in 1981 it has been one of my top 3 favorites the blurb i wrote for amazon is below and i m just thrilled that it s finally coming out on dvd on 10 17 06 the film s 25th anniversary the last credit in this film explains its appeal thank you to the people of manhattan on whose island this was filmed a charming and witty romantic comedy it is a love story written to new yorkers peter bogdanovich is a native who can identify every location west 12th street the ansonia the old fao schwartz the plaza the roxy chez brigitte and city limits which was a country western club one gets the impression that the entire ensemble cast clicked as well off screen as they do on and this intimacy is clearly communicated i laughed i cried it was better than cats not only an ode to dorothy stratten it was also audrey hepburn s last feature appearance she had a cameo subsequent to this film and her inner beauty seeps from the screen buy it make a big tub of popcorn and curl up with someone you love \n",
            "it s very hard to say just what was going on with the lady from shanghai and what the film could have been without studio interference orson welles prime interest in film at this point was to raise money for his theater indeed funding his own projects is what drove him to seek out acting jobs he made lady from shanghai for his soon to be ex wife rita hayworth harry cohn was fearful for rita s image and held back the release of this movie for one year the plot concerns an irish sailor michael o hara who falls in love with else hayworth stunning with short blond hair her husband is a well known criminal attorney arthur bannister everett sloane who is as crippled on the inside as he is out he hires o hara to work on his yacht and there o hara is drawn deeper and deeper into a web of murder and deceit the lady from shanghai moves at a snail s pace though i agree with one of the posters that films today are criticized for taking time to build a plot still this movie drags the scene in the fun house is fantastic welles wanted it without music though and i believe the studio cut it down it s a shame the photography throughout is stunning atmospheric bold and very stylish welles was an excellent actor handsome in his youth charismatic and possessing a magnificent voice and technique but in many films it s almost as if he doesn t trust himself or doesn t take the time to develop a character instead he relies on externals such as accents and fake noses one of the only times he didn t do this was tomorrow is forever where the director gets an excellent deeply felt performance out of him contrast that with compulsion where he shows he is a master of pure technical acting as he phones in his performance here welles is doing quadruple duty as director star co writer and narrator sporting a completely unnecessary accent and looking intense was a fast way to a characterization nevertheless he is always compelling the supporting players are excellent including sloan and glenn anders hayworth gorgeous and soft voiced her singing was again dubbed by anita ellis is as usual a complete goddess and one of the great screen presences what a sad life for such a vibrant beauty any film that orson welles directed is worth seeing and the lady from shanghai is no exception but this one leaves the viewer frustrated as does the magnificent ambersons as does any work that welles did within the studio system he was a great artist who should have been given a freer reign he wasn t he was a strange dichotomy he needed more freedom but as is evidenced by some of his later work he needed the structure of the studio alas he couldn t have both \n",
            "a beautiful film touching profoundly up the simple yet divine aspects of humanity this movie was almost perfect and seeing as nothing in this world can be truly perfect that is pretty good the only minor thing i subjectively object to is the pacing at some points in the middle of the story the acting is also very good and all the actors easily top actors in high profile films the actual directing seems to have been well thought through and the script must have been amazing there are some truly breathtaking moments of foreshadowing and a quite gorgeous continuing circular composition of the story the moment in the movie when the main character achieves that feeling of being in heaven is the perfect ending to a truly brilliant yarn \n",
            "if you want to know what kind of music white people listened to in 1974 this is the movie for you but you ll have to listen to a lot of flutes and violins too see my remarks on my girl 1 for the reference indulgent admission i approached my girl 2 with cynicism and annoyance having just viewed its predecessor but as an adoptee preparing to finally set upon a search for my birthmother my girl 2 made me look with its theme of searching for mother put another way anything i liked about my girl 2 had nothing whatsoever to do with my girl 2 but relating to a protagonist who asks like so many adoptees who s my mama and if there are home movies of my mom in an acting troupe i ll be sure to make my own movie about it people are listless movies should not be listless my girl 2 like my girl 1 is just listless avoid unless you re a complete sap who s comforted by a series of small annoyances \n",
            "went to wal mart and found this film on dvd and had no idea whether i made a bad purchase or a good one it is the later outcome to my viewing the entire film from beginning to end michelle rodriquez diana guzman bloodrayne 05 gave a great performance and her looks are beautiful sexy and at the same time a real study in the art of how acting is really performed diana did not like her home life and especially her own father for the abuse he gave her mother she decides to get boxing training in a local brooklyn gym and is not really well accepted by the male boxers in her high school there is plenty of friction between her female classmates and guys there is lots of action in the ladies bathroom and references as to private parts of guys if you like boxing and seeing a hot sexy gal do wonders in the ring and knock the boys on their you know what this is the film for you \n",
            "tenshu is imprisoned and sentenced to death when he survives electrocution the government officials give him a choice to either be electrocute at a greater degree or agree to some experiments he chooses the experimentation and is placed in a large metallic cell with a bad ass criminal who also survived the electrocution they can have whatever the want in the room within reason but they can t leave after a few days there meals are cut down to one per day and the room temp is set up too 100 after some more alarms are sounded at intervals so they can t sleep one day a witch come into their cell albeit a glassed off portion what happens next i ll let you find out i may be in the minority here but i liked the build up it was intriguing to me now if the payoff was half as good as the build up was i would have rated this so much higher my grade c media blaster s 2 dvd set extras disc 1 director s cut trailers for versus aragami attack the gas station and deadly outlaw rekka disc 2 theatrical cut commentary with hideo sakaki ryuhei kitamura sakaguchi takuand tsutomu takahashi cast and crew interview making of original trailer and promo teasers \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGDdKfQP1lv0",
        "colab_type": "text"
      },
      "source": [
        "###Notebook Exercise 3 \n",
        "\n",
        "Find the top 10 bigrams for the positive and negative movie reviews in the training data. (You'll need to deal with the fact "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9gdqb6g4vsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes up here\n",
        "\n",
        "#bigramFreqPositive =\n",
        "#bigramFreqNegative = [] # this too\n",
        "#print(\"Positive bigrams:\")\n",
        "#bgfp = bigramFreqPositive.most_common(10)\n",
        "#for bg in bgfp:\n",
        "#  print(bg)\n",
        "\n",
        "#print(\"Negative bigrams \\n\")\n",
        "#bgfp = bigramFreqNegative.most_common(10)\n",
        "#for bg in bgfp:\n",
        "#  print(bg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8aljKSsB7A2",
        "colab_type": "text"
      },
      "source": [
        "####Expand for solution\n",
        "\n",
        "Note: If you are still getting 'br','br' as your top bigram. Go back and run the second block in the notebook (where dfX_train gets assigned). Then run the block above that includes the function clean_series_data(). If that doesn't work, repeat the process, after first running the solution where clean_text(s) is defined. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFkxMjSy0SwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A solution (though there are likely faster, cleaner ways to do this)\n",
        "wordsPositive = list()\n",
        "wordsNegative = list()\n",
        "n = 2\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "  tokens = [token for token in dfX_train[i].split(\" \") if token != \"\"]\n",
        "  if y_train[i]==1:\n",
        "    wordsPositive.extend(tokens)\n",
        "  else:\n",
        "    wordsNegative.extend(tokens)\n",
        "\n",
        "bigramWordsPositive = list(ngrams(wordsPositive, n))\n",
        "bigramFreqPositive = collections.Counter(bigramWordsPositive)\n",
        "bigramWordsNegative = list(ngrams(wordsNegative, n))\n",
        "bigramFreqNegative = collections.Counter(bigramWordsNegative)\n",
        "\n",
        "print(\"Positive bigrams:\")\n",
        "bgfp = bigramFreqPositive.most_common(10)\n",
        "for bg in bgfp:\n",
        "  print(bg)\n",
        "print('\\n')\n",
        "print(\"Negative bigrams\")\n",
        "bgfp = bigramFreqNegative.most_common(10)\n",
        "for bg in bgfp:\n",
        "  print(bg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKZmbjRT7-hS",
        "colab_type": "text"
      },
      "source": [
        "#### Try a different sized n-gram (instead of a bigram / 2-gram)\n",
        "Not shockingly, this list is not very exciting. It turns out, people use some pretty standard words bigrams when talking about movies (e.g. \"this film\" and \"of the\")... really riveting stuff. If we look at a greater number of bigrams (e.g., the top 100), we can eventually start to find something relevant among mostly trite pairings.\n",
        "\n",
        "However, it might be interesting to look at a different sized ngram than the bigram. **Try something in the n = 4 to 7 range.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjeDdSF6FZsL",
        "colab_type": "text"
      },
      "source": [
        "## Classifying movie review sentiment with bigrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSEKwuOKncmZ",
        "colab_type": "text"
      },
      "source": [
        "Let's revisit our Na&iuml;ve Bayes model, but now using bigrams as our features instead of single words. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6BU3Z-g-Gcs",
        "colab_type": "text"
      },
      "source": [
        "#### Use CountVectorizer to get top bigrams and then classify sentiment\n",
        "The code below gives a black box approach to classifying with ngrams. \n",
        "\n",
        "The ngram_range(2,2) makes our code use bigrams. \n",
        "\n",
        "Note that we now have a different shape to our data because it is stored in sparse form (no longer using todense()). If we try to store this in dense form, we will run into RAM errors, which we could combat by limiting the number of ngrams that we include in our CountVectorizer by setting max_features=10000 limits the total number of feautures.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBxljA3JZsKn",
        "colab_type": "code",
        "outputId": "1735e94f-9b70-4fa8-e907-62274cf36c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ngramvectorizer = CountVectorizer(ngram_range=(2,2))\n",
        "ngramvectorizer.fit(dfX_train) #learn a vocabulary dictionary of all tokens in the raw documents\n",
        "\n",
        "X_train_ngram = ngramvectorizer.transform(dfX_train)\n",
        "X_test_ngram = ngramvectorizer.transform(dfX_test)\n",
        "print(\"X_train_ngram.shape\", X_train_ngram.shape)\n",
        "print(\"X_test_ngram.shape\", X_test_ngram.shape) "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_ngram.shape (18750, 1171796)\n",
            "X_test_ngram.shape (6250, 1171796)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KNyelzfhMcK",
        "colab_type": "code",
        "outputId": "ace71470-a3ba-43ee-ec68-37896c750f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Actually run the model and print results\n",
        "# If this is taking too long, you can run it on a subset of your data.\n",
        "model = MultinomialNB(alpha=1)\n",
        "model.fit(X_train_ngram, y_train)\n",
        "\n",
        "y_pred_train = model.predict(X_train_ngram)\n",
        "print(\"Training accuracy: \", np.mean(y_pred_train == y_train))\n",
        "y_pred = model.predict(X_test_ngram)\n",
        "print(\"Testing accuracy: \",np.mean(y_pred == y_test))\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy:  0.99808\n",
            "Testing accuracy:  0.88336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNsiQutMEWSC",
        "colab_type": "text"
      },
      "source": [
        "###Notebook Exercise 4\n",
        "\n",
        "(a) Try manipulating the size of the ngram and the number of features to explore their effects. \n",
        "\n",
        "(b) What method is essentially the same as using the parameter: ngram_range=(1,1)?  What are the relative strenghts/weaknesses of these two approaches?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3SSv1_zFW7Y",
        "colab_type": "text"
      },
      "source": [
        "#### Expand for solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcuSh4O3Lvg4",
        "colab_type": "text"
      },
      "source": [
        "(a) This depends on what you looked at. We observed that a full set of bigrams performed better than a bag of words. However, this was not the case if we limit the number of features.\n",
        "\n",
        "(b) Bag of Words. An 1-gram is just a list of words that occurred.  The Bag of Words takes less memory to run... think about how many unique single words exist in text and compare this to how many unique word pairs exist in the same text. \n",
        "\n",
        "However, bigrams can provide us with more information about the context that the words are used. For example, a positive review might include 'not terrible' while a negative review might include 'so terrible'. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSUIpaW5EJi8",
        "colab_type": "text"
      },
      "source": [
        "## Predicting the next words with bigrams\n",
        "\n",
        "Now, we'll explore the idea of choosing the next word in a sequence based on the previous word. We'll do a simple implementation of this, which you will then improve upon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkDsQpVoGIPN",
        "colab_type": "text"
      },
      "source": [
        "Since we already have a list of the bigrams, we can use these to build our predictor.  We'll just use the positive bigram list ***bigramWordsPositive*** that was generated in the solution to Exercise 3. You may need to go back and run that block of code if you haven't already.\n",
        "\n",
        "This code creates a dictionary to store 2nd word in each bigram under the key of the first word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fsbbASsUOAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigramLookup = {}\n",
        "\n",
        "for i in range(len(bigramWordsPositive)-1):\n",
        "    w1 = bigramWordsPositive[i][0]\n",
        "    w2 = bigramWordsPositive[i][1]\n",
        "    #print(w1,w2)\n",
        "    if  w1 not in bigramLookup.keys():\n",
        "      bigramLookup[w1] = {w2:1}\n",
        "    elif w2 not in bigramLookup[w1].keys():\n",
        "      bigramLookup[w1][w2] = 1\n",
        "    else:\n",
        "      bigramLookup[w1][w2] = bigramLookup[w1][w2] + 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suhe9vUwrF24",
        "colab_type": "text"
      },
      "source": [
        "Now, we can write some code to generate an ouptut based on a starting word (curr_sequence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sJmGIbYausK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "275f842f-8f08-445e-aacb-f6817f815958"
      },
      "source": [
        "import random\n",
        "\n",
        "curr_sequence = \"my\" # Starting word\n",
        "output = curr_sequence\n",
        "for i in range(50):\n",
        "    if curr_sequence not in bigramLookup.keys():\n",
        "      print(\"not in my keys, choosing seed word \")\n",
        "      output += '. '\n",
        "      curr_sequence = 'the'\n",
        "      output += curr_sequence\n",
        "    else: \n",
        "      possible_words = list(bigramLookup[curr_sequence].keys())\n",
        "      next_word = possible_words[random.randrange(len(possible_words))] #Randomly choose a word\n",
        "      output += ' ' + next_word\n",
        "      curr_sequence = next_word\n",
        "      \n",
        "\n",
        "print(output)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my uni friends soon richard attenborough as pawns being punished by jodie sweetin and doorway to innocent naive dimbulb who hates smita whom giovanna they played erica gavin of otto oh yes you study no ambitions yet discovered that frame standing in working for gunga din does cloud in assisting inspector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juG7MTxOfk0b",
        "colab_type": "text"
      },
      "source": [
        "###Notebook Exercise 5\n",
        "\n",
        "Well, put together some words, sometimes coherently, sometimes not so much. What a great opportunity for... next_word!\n",
        "\n",
        "Modify the above review generation code to improve it in some way. For example, you might select the next word based on it's probability, instead of just randomly choosing any word that was previously paired.\n",
        "\n",
        "If you're feeling saucy, you could build this review generator to take use some higher value of ngrams. And if you're even saucier, you could build a function that writes your whole review based on the input of a starting phrase and whether the review should be positive or negative.\n",
        "\n",
        "***No solution for this one, but come to class prepared to talk about what you tried.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqWZ8BpHmfgj",
        "colab_type": "text"
      },
      "source": [
        "###Optional Exercise: Build a word completion tool using ngrams on letters instead of words. \n",
        "\n",
        "If you flew through this notebook (because you're familiar with this content or a python wiz, a fun way to challenge yourself is to build a tool that uses ngrams on letters instead of words. This can be applied to word auto-complete or to spell checking.  Be sure to think about the way you want to format your text. How many features will your data have if you do a 1-gram, 2-gram, or 3-gram?"
      ]
    }
  ]
}