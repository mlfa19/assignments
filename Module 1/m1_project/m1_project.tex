
\documentclass{tufte-handout}
\usepackage{fontawesome}
\usepackage{../../CommonLatexPackages/machine_learning_preamble_1.0}

\fancypagestyle{firstpage}

{\rhead{Module 1 Project\linebreak \textit{Version: \today}}}

\title{Module 1 Project: Computer Vision Application}
\author{Machine Learning}
\date{Fall 2019}

\begin{document}

\maketitle
\thispagestyle{firstpage}

%\begin{learningobjectives}
%\bi
%\item Apply 
%
%\ei
%\end{learningobjectives}

\section*{Motivation}

Computer vision and image classification tools are rapidly reshaping how we live our lives. Machine learning approaches have driven recent progress in an array of technologies that have the potential to realize huge positive impacts on our world. These tools have become increasingly easy to apply to masses of existing data, however, these tools and their real-world application do not always have the results that their creators intend. To achieve better outcomes, it is important to deeply consider the potential implications and limitations throughout the application design and implementation process. This project aims to help you to begin to develop the skills to understand, implement, and critically evaluate machine learning systems. 

\section*{Goal-Setting and Customization}

You will be considering your own learning goals for this project, as well as the learning goals of your teammate(s) (if applicable). You can customize this project to support your own learning, and we are happy to help you shape the project to support your goals and challenge yourself. 

\section*{Project Description}
In this project, you will contemplate a potential application for computer vision (machine learning of images) and you will implement a machine learning algorithm on a data set in support of this application. As part of this project, you will:
\bi[leftmargin=.5in]
\item Document the important considerations for your application (e.g., what data are available for training, how well would the algorithm need to work to create value, how could the algorithm and application be tested (beyond the testing you choose to implement), what are the implications of this application in the world, what are the stakeholders, what are the risks). 
\item Build and iterate on a computer vision model as a step toward this application. We don’t expect you to build the entire application, just to work toward implementing an early version of an algorithm that could be used for this application. It’s likely that you will use a convolutional neural network, but not required. You will build and test multiple versions of your model.
\item Visualize key aspects of your data/model. Include at least two visualizations in your report, but you’ll probably find it valuable to visualize various aspects of the data and model (this is helpful in sanity checking and can give good insight into how the model is working). 
\item Test and evaluate your model. This will likely include accuracy on training and test sets. Depending on your application, you may also want to collect your own photos, find other images online, and/or manipulate images from your original test set (adding noise, shifting/flipping images, etc). You will also evaluate the effectiveness of this model for your specific application and describe the limitations of your current model and data.
\item Document your final analysis pipeline for transparency. (A simplified version that does not need to include every parameter, visualization, or tweak that you tried.)
\ei

\section*{Resources}
\bi
\item Interactive visualization for considerations in your application: \href{https://www.cdt.info/ddtool/}{https://www.cdt.info/ddtool/}. The background for this visualization is here: \href{https://cdt.org/issue/privacy-data/digital-decisions/} {https://cdt.org/issue/privacy-data/digital-decisions/} 
\item FAT ML principles for accountable algorithms (from the first day of class): \href{https://www.fatml.org/resources/principles-for-accountable-algorithms}{https://www.fatml.org/resources/principles-for-accountable-algorithms}
\item Suggested questions to consider about your application in Appendix A.
\item Suggested data sets in Appendix B and \href{https://www.notion.so/Entry-Level-Computer-Vision-Datasets-cc6fb53f51324779b29e26337642a649}{here}
\ei


\section*{Appendix A: Some questions to consider}
\href{https://www.notion.so/ANN-Project-Framing-76e1b6af347f475a983487996ac9760d}{source}
\bi
\item What do you want your model to be able to do?
\item How can you imagine your model (or an extension of it) being used in the real world? Feel free to get creative.
\item If those ideas came true, who might they affect? In what ways?
\item What measures would your model's real-world implementers need to take to ensure its effects live up to your intentions?
\item What pitfalls might your model fall into, and what could you quantitatively measure to avoid those?
\item Why was the dataset you used to train your model created?
\item In what other ways could the same data be used? How do you feel about those possibilities?
\item How was that dataset assembled? From where was the data sourced? Who or what labeled it? If there are any elements of this process you think were either particularly well done or problematic, how so?
\item If your dataset contains information about people, to what degree did those people have agency over their inclusion? Do you feel that matters in this case? Why or why not?
\item Skimming through your dataset, does anything stand out to you about representation in its contents?
\item Do you feel the potential use cases for this dataset justify it being created and published? Why or why not?
\ei

\section*{Appendix B: Computer Vision Datasets}
\subsection*{PyTorch built-in datasets}\label{pytorch-built-in-datasets}

\bi
\item
  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#torchvision.datasets.CIFAR10}{CIFAR-10}:
  60,000 labeled 32x32 color images of 10 classes of objects
\item
  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#torchvision.datasets.CIFAR100}{CIFAR-100}:
  60,000 labeled 32x32 color images of 100 fine classes of objects, also
  grouped into 20 course superclasses
\item
  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#emnist}{EMNIST}:
  800,000 labeled 28x28 grayscale images of handwritten digits and
  letters (uppercase and lowercase) that expand the MNIST dataset
\item
  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#fashion-mnist}{Fashion-MNIST}:
  70,000 labeled 28x28 grayscale images of 10 classes of clothing
  articles
\item
  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#imagenet}{ImageNet
  2012}: 1,331,167 labeled color images of tens of thousands of classes
  of nouns in the WordNet hierarchy
\item
  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#kmnist}{KMNIST}:
  70,000 labeled 28x28 grayscale images of 10 classes of handwritten
  Japanese Hiragana characters
\item
  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#lsun}{LSUN}:
  708,564 labeled large-scale color images of 10 classes of
  scenes/settings
\item
  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#mnist}{MNIST}:
  70,000 labeled 28x28 grayscale images of 10 classes of handwritten
  digits
\item
  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#svhn}{SVHN}:
  Color images of house numbers (addresses) obtained from Google Street
  View w/ labeled bounding boxes around individual digits
\ei

\subsection*{TensorFlow built-in}\label{tensorflow-built-in-datasets}

Note: the links to these have moved to the \href{https://www.tensorflow.org/datasets/catalog/overview#all_datasets}{Tensorflow datasets} documentation page.

\bi
\item
Cats and Dogs: 25,000 labeled images of cats and dogs
\item
CelebA: 202,599 total face images of 10,177 identities w/ facial landmarks,
  aligned \& cropped images, bounding boxes, and binary attribute labels
\item
  CIFAR-10:
  60,000 labeled 32x32 color images of 10 classes of objects
\item
CIFAR-100:
  60,000 labeled 32x32 color images of 100 fine classes, also grouped
  into 20 course superclasses
\item
 CIFAR-10-C:
  Images from CIFAR-10 manipulated using 15 common corruptions at 5
  levels of severity each
\item
Colorectal Histology: 5,000 labeled 150x150 color histological images of 8
  tissue types of human colorectal cancer
\item
CBIS-DDSM:
  2,620 labeled scanned film mammography studies of normal, benign, and
  malignant cases of human breast cancer
\item
 Diabetic
  Retinopathy Detection: 88,712 labeled images of eyes with diabetic
  retinopathy at none, mild, moderate, severe, and proliferative levels
  of severity
\item
EMNIST:
  800,000 labeled 28x28 grayscale images of handwritten digits and
  letters (uppercase and lowercase) that expand the MNIST dataset
\item
  Fashion-MNIST:
  70,000 labeled 28x28 grayscale images of 10 classes of clothing
  articles
\item
 Horses
  or Humans: 1,283 labeled 300x300 color images of cgi humans and
  horses
\item
ImageNet
  2012: 1,331,167 labeled color images of tens of thousands of classes
  of nouns in the WordNet hierarchy
\item
  ImageNet-C
  2012: Images from ImageNet 2012 manipulated using 12 common
  corruptions at 5 levels of severity each
\item
 KMNIST:
  70,000 labeled 28x28 grayscale images of 10 classes of handwritten
  Japanese Hiragana characters
\item
  LSUN:
  708,564 labeled large-scale color images of 10 classes of
  scenes/settings
\item
 MNIST:
  70,000 labeled 28x28 grayscale images of 10 classes of handwritten
  digits
\item
 MNIST-C:
  Images from MNIST manipulated using 15 common corruptions at 5 levels
  of severity each
\item
  Omniglot:
  38,300 labeled grayscale images of 1,623 classes of handwritten
  characters from 50 alphabets w/ stroke data in {[}x,y,t{]} coordinates
  where time (t) is in milliseconds
\item
 Oxford
  Flowers: 8,189 labeled color images of 102 classes of flowers
\item
  Oxford-IIIT
  Pet: 7,349 labeled color images of 37 classes of pet breeds w/ tight
  bounding boxes around the heads and pixel-level
  foreground-background-boundary segmentation
\item
  PatchCamelyon:
  327,680 labeled 96x96 color images of histopathologic lymph node scans
  where metastatic tissue is either present or absent
\item
PetFinder:
  72,776 color images of dogs and cats w/ many attribute labels
\item
  Quick,
  Draw! Bitmap: 50,426,266 labeled 28x28 grayscale images of 345
  classes of drawn objects contributed by Quick, Draw! players
\item
NWPU-RESISC45:
  31,500 labeled 256x256 color images of 45 classes of scenes/settings
\item
  Rock
  Paper Scissors: 2,892 labeled 300x300 color images of cgi hands in
  rock, paper, and scissors gestures
\item
small
  NORB: 48,600 labeled 96x96 color images of 5 classes of toys
\item
 Sun397:
  108,753 labeled color images of 397 classes of scene/settings
\item
 SVHN-cropped:
  600,000 labeled 32x32 color images of digits of house numbers
  (addresses) obtained from Google Street View
\item
TF
  Flowers: 3,670 labeled color images of 5 classes of flowers
\item
 UC
  Merced Land Use: 2,100 labeled 256x256 color aerial images of 21
  classes of land uses collected from USGS National Map Urban Area
  Imagery
\ei

\subsection*{Other datasets}\label{other-datasets}

\bi
\item
  \href{http://eidolon.univ-lyon2.fr/~remi1/Bark-101/}{Bark-101}: 2,594
  labeled color images of 101 classes of tree bark
\item
  \href{http://www.vision.caltech.edu/Image_Datasets/Caltech101/}{Caltech
  101}: 9,146 labeled color images of 101 classes of objects
\item
  \href{http://www.vision.caltech.edu/Image_Datasets/Caltech256/}{Caltech
  256}: 30,607 labeled color images of 256 classes of objects
\item
  \href{https://dataturks.com/projects/dominique.paul.info/cars2}{CARS}:
  604 labeled color images with either cars or no cars
\item
  \href{http://www.jdl.ac.cn/peal/index.html}{CAS-PEAL-R1}: 30,900
  labeled grayscale images of 1,040 faces (exclusively Chinese)
\item
  \href{https://github.com/detectRecog/CCPD}{CCPD}: 300,000 labeled
  color images of license plates w/ bounding boxes, license plate
  numbers, and several dimension labels
\item
  \href{https://github.com/BayesWatch/cinic-10}{CINIC-10}: A drop-in
  replacement for CIFAR-10 with 270,000 images collected by downsampling
  images from ImageNet
\item
  \href{https://dataturks.com/projects/miaozh17/Crack\%20Classification}{CRACK}:
  1,428 labeled 299x299 color images with either cracks or no cracks
\item
  \href{https://cyberextruder.com/face-matching-data-set-download/}{CyberExtruder
  Ultimate Face Matching Dataset}: 10,205 labeled 600x600 color images
  of 1000 faces scraped from the internet
\item
  \href{http://iab-rubric.org/resources/dfw.html}{DFW}: 11,157 labeled
  color images of 1000 subjects' faces, including examples of that
  subject attempting to obfuscate their face and of other individuals
  attempting to impersonate that subject
\item
  \href{https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces}{DiF}:
  1,000,000 color images of diverse faces w/ dimension labels for
  objective physical properties
\item
  \href{https://www.nist.gov/itl/iad/image-group/color-feret-database}{FERET}:
  14,126 labeled 384x286 color images of 1,119 faces
\item
  \href{https://www.vision.ee.ethz.ch/datasets_extra/food-101/}{Food-101}:
  101,000 labeled 512x512 color images of 101 classes of food
\item
  \href{http://www.ivl.disco.unimib.it/activities/food475db/}{Food-475}:
  247,636 labeled color images of 475 classes of food
\item
  \href{http://www.ivl.disco.unimib.it/activities/food524db/}{Food-524}:
  247,636 labeled color images of 524 classes of food
\item
  \href{http://eidolon.univ-lyon2.fr/~remi1/HistAerialDataset/}{HistAerial}:
  4,900,000 labeled grayscale aerial images of 7 classes of ground use
\item
  \href{https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/}{IMDb}:
  460,723 color images of celebrities w/ bounding boxes around faces and
  attribute labels including DOB, name, binary gender (literally)
\item
  \href{https://github.com/xpwu95/IP102}{IP102}: 75,000 labeled color
  images of 102 classes of insects
\item
  \href{https://github.com/smilell/AG-CNN}{LAG}: 11,760 labeled color
  images of suspicious or negative glaucoma samples
\item
  \href{http://vis-www.cs.umass.edu/lfw/index.html}{LFW}: 13,233 labeled
  250x250 color images of 5,749 faces
\item
  \href{https://talhassner.github.io/home/projects/lfwa/index.html}{LFW-a}:
  Images from LFW that have been aligned using a commercial face
  alignment software
\item
  \href{http://conradsanderson.id.au/lfwcrop/}{LFWcrop}: Images from LFW
  that have been cropped to only include the contents of facial bounding
  boxes
\item
  \href{https://github.com/Tencent/tencent-ml-images}{ML-Images}:
  17,698,491 labeled color images of 11,166 classes of objects
\item
  \href{http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html}{Multi-PIE}:
  750,000 labeled color images of 337 faces
\item
  \href{https://www.ri.cmu.edu/project/pie-database/}{PIE}: 41,368
  labeled color images of 68 faces
\item
  \href{https://github.com/facebookresearch/qmnist}{QMNIST}: An
  extension of the MNIST data set to include 50,000 additional test
  images
\item
  \href{https://stevewongv.github.io/derain-project.html}{Real Rain}:
  29,500 labeled color image pairs, one with and one without rain
\item
  \href{https://dataturks.com/projects/sheerun/rooms}{ROOMS}: 10,029
  labeled (out of 20,001 total) color images of 6 classes of room types
\item
  \href{http://www.scface.org/}{SCface}: 4,160 labeled color images of
  130 faces taken from 6 security cameras of varying quality in either
  visible light or infrared mode
\item
  \href{https://sites.google.com/view/sof-dataset}{SoF}: 42,592 labeled
  color images of 112 faces (exclusively glasses-wearing)
\item
  \href{http://vision.stanford.edu/aditya86/ImageNetDogs/}{Stanford
  Dogs}: 20,580 labeled color images of 120 breeds of dogs w/ bounding
  boxes
\item
  \href{https://github.com/PKU-IMRE/VERI-Wild}{VERI-Wild}: 416,314
  labeled color images of 40,671 vehicles
\ei





\end{document}
