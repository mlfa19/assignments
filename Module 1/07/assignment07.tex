\documentclass[assignment07_Solutions]{subfiles}

\invalidatemargin

%\IfSubStr{\jobname}{\detokenize{Solutions}}{\toggletrue{solutions}}{\toggletrue{solutions}}

\IfSubStr{\jobname}{\detokenize{Solutions}}{\toggletrue{solutions}}{\togglefalse{solutions}}

\fancypagestyle{firstpage}

{\rhead{Assignment 7 \linebreak \textit{Version: \today}}}

\title{Assignment 7: Pytorch, Vanishing Gradients, and Convolutional Neural Networks}
\author{Machine Learning}
\date{Fall 2019}

\begin{document}

\maketitle
\thispagestyle{firstpage}


\begin{learningobjectives}
\bi
\item Learn the basics of Pytorch
\item Understand the vanishing gradient problem and how to fix it
\item Learn about convolutional neural networks from a conceptual perspective
\ei
\end{learningobjectives}

\begin{priorknowledge}
\bi
\item Multilayer perceptron
\item Backpropagation
\ei
\end{priorknowledge}

\section{Pytorch and Autograd Algorithms}

In this assignment you will be learning about another new Python library: pytorch.  \href{https://pytorch.org/}{Pytorch} is an open-source library designed for high-performance (meaning fast, rather than necessarily high accuracy) machine learning.  There are a number of other similar frameworks out there.  We chose pytorch due to its straightforward data handling, flexibility, and support for debugging.  Below, we'll list some other frameworks and why we ultimately decided to go with pytorch.

\bi
\item \href{https://keras.io/}{Keras} also lets you construct high-performance neural networks in Python.  The biggest downside in our eyes is that it hides away a lot of the details of how data moves through the network.  This makes it a bit too abstract for our purposes (e.g., harder to debug and less flexible).
\item \href{https://www.tensorflow.org/}{TensorFlow} is another package that is similar to pytorch.  Tensorflow is more popular than pytorch, but it is a bit more complicated and has a steeper learning curve.
\item sklearn (which you've gotten to use) has support for Multilayer Perceptrons (as we saw in the last notebook).  That said, it doesn't support that many different types of networks, is hard to customize, and doesn't support high performance computation using GPUs (graphics processing units).
\item numpy could also be an option.  It is very flexible, but it is too low-level.  Like sklearn it doesn't support high performance computation with GPUs.
\ei

Before you go through some resources on how to use pytorch, let's discuss what we are hoping to get from pytorch.
\bi
\item The ability to automatically compute gradients of our loss function with respect to parameters of a neural network!
\item The ability to train neural networks on a GPU (for performance boosts of up to 20 times).  Luckily, Google is nice enough to let you use their GPUs through Colab!
\item Tools for debugging common problems with neural networks.
\item A module called torchvision for working with images in neural networks.
\ei

Go through the following tutorials for pytorch.  This will give you the barebones.  Later in this document you'll explore pytorch again via a companion notebook.
\begin{externalresources}[(15 minutes)]
We recommend you go through these with Colab open so you can try the blocks of code yourself.
\bi
\item \href{https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html}{Tensor Tutorial} (\href{http://nb.mit.edu/f/55481}{on NB})
\item \href{https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html}{Autograd Tutorial} (\href{http://nb.mit.edu/f/55482}{on NB})
\ei
\end{externalresources}

\begin{externalresources}[(60 minutes)]
Now that you have the barebones basics for Pytorch, let's look back at our old friend the logistic regression model.  In this \href{https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module\%201/07/Assignment_07_Companion_Pytorch_Titanic.ipynb}{companion notebook} you'll play with an implementation of logistic regression using pytorch and compare it with sklearn's implementation of logistic regression.  In the notebook we'll use the Titanic data as our example.
\end{externalresources}


\section{Backpropagation and the Problem of Vanishing (or Exploding) Gradients}
One possible reaction to reading about the autograd capabilities in pytorch is to think ``Why didn't Paul and Sam tell me about this before?  Why have we been wasting our time doing all this math to compute gradients?''  There are two really good (in our opinion) answers to this question.
\be
\item Knowing how backpropagation works gives you a more powerful mental model of how these networks are trained, how to diagnose failures in training, and potentially how to fix these failures.
\item The math you learned along the way is useful even outside of machine learning.
\ee

One particularly important way in which knowing backpropagation helps you understand how to train a better network is when confronting something known as the \emph{vanishing gradient problem} (or its more dramatic sounding counterpart the \emph{exploding gradient problem}).  You'll be reading all about it in just a bit, but before we dive in let's review the multi-layer perceptron (MLP).  If you feel good about the MLP, skip the next box.

\begin{recall}[Multi-layer perceptron]
The multi-layer perceptron consists of a bunch of single-layer perceptrons stacked on top of each other.  Last assignment we saw a particular special case of the MLP where we take logistic regression models and stack them.  A cartoon version of the network applied to the titanic dataset is shown below.

\begin{center}
\includegraphics[width=0.7\linewidth]{figures/titanicmlpsimple}
\end{center}

We saw in the companion notebook that such networks are capable of learning useful internal representations for prediction.  To make this figure more precise, we can zoom in on how data propagates from lower layers to higher layers in the network (view 1 in the figure below) and ultimately to the loss function (view 2 in the figure below).

\vspace{1em}

Before presenting the figure, for your convenience here is a notation guide for the figure.
\vspace{1em}

\begin{tabular}{| c | l |}
\hline
$l$ & the loss function for the network (log loss in this case) \\
\hline
$m$ & the number of layers in the network \\
\hline
$n_{k}$ & the number of nodes in the $k$th layer \\
\hline
$x_j^{(k)}$& The $j$th node in the $k$th layer of the network ($k = 1$ is the input \\
&and $k = m$ is the output) \\
\hline
$\mathbf{x^{(k)}}$ & The nodes at the $k$th layer in vector form \\
\hline
$s_{i}^{(k)}$ & the $i$th summation node in the $k$th layer \\
\hline
$w_{i,j}^{(k)}$ & the weight connecting the $j$th node in layer k to the $i$th node in\\
&  layer $k+1$ \\
\hline
$\mlvec{w_{i}^{(k)}}$ & the vector of weights to the $i$th summation node in layer $k+1$.\\
\hline
\end{tabular}

\begin{center}
\includegraphics[width=0.7\linewidth]{figures/mlpfullforward}
\end{center}

\end{recall}

In the previous assignment you derived the backpropagation algorithm, which can be used to compute the gradient of the weights in an MLP with respect to the loss function.  What's beautiful about this formula is that it works for networks of arbitrary depth.  You may have heard of \href{https://en.wikipedia.org/wiki/Deep_learning}{deep learning}, which is where networks of dozens or even hundreds of layers are trained on a particular task.  Despite their complexity, these networks are typically trained using the backpropagation algorithm you met in the last assignment!

Despite the relative simplicity of the backpropagation equations, there are some dangers lurking when applying the algorithm to deep networks ($m$ very large).  The most common issues that one encounters are \textbf{vanishing and exploding gradients}.  A vanishing gradient is when the gradient of the loss with respect to the weights in a layer becomes vanishingly small as you move from the output layer towards the input layer.  An exploding gradient is the opposite problem (when the gradient of the loss with respect to the weights in a layer becomes exceedingly large as you move from the output layer towards the input layer).

\begin{exercise}[(45 minutes) or (105 minutes including optional part)]
Before starting this exercise, go and learn about the vanishing gradient problem.  Here are a few resources to look at.  You do not need to read all of them, so please pick the ones that seem the most inline with how you learn.
\begin{externalresources}
\bi
\item \href{https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b}{Rohan Kapur's Intuitive Explanation of the Vanishing Gradient Problem} (\href{http://nb.mit.edu/f/55483}{on NB})
\item Video resource: \href{https://www.youtube.com/watch?v=SKMpmAOUa2Q}{Deep Learning Simplified: An Old Problem}
\ei
\end{externalresources}


Last assignment we derived the following backpropagation equations.

\begin{align}
(\nabla_{\mathbf{w^{(k)}_i}}) l &= \mlvec{x^{(k)}} \sigma\left (s_i^{(k+1)} \right )\left (1-\sigma\left (s_i^{(k+1)} \right )\right) \frac{\partial l}{\partial x_i^{(k+1)}} & \mbox{gives us the gradient of the weights going into a unit} \\
\frac{\partial l}{\partial x^{(k)}_i} &= \sum_{j=1}^{n_{k+1}} w^{(k)}_{j,i} \sigma \left ( s_{j}^{(k+1)} \right) \left (1-\sigma \left ( s_{j}^{(k+1)} \right)\right) \frac{\partial l}{\partial x^{(k+1)}_j} \label{eq:recursion} & \mbox{recursively gives the partial of the loss w.r.t. the units} \\
\frac{\partial l}{\partial x^{(m)}_1} &= -y\frac{1}{x_1^{(m)}} + (1-y) \frac{1}{1-x_1^{(m)}} & \mbox{provides the base case of the recursion}
\end{align}

\bes
\item If we examine these equations, we can understand where the vanishing gradient problem comes from.  In particular, Equation~\ref{eq:recursion} provides the crucial recursive formula for defining the partial derivatives of the loss with respect to nodes in the network in terms of the same partial derivatives for nodes deeper in the network (i.e., closer to the output).  Based on Equation~\ref{eq:recursion}, what aspects of the network might contribute to or counteract the vanishing gradient problem? Consider things such as the values of the summation units $s_j^{(k+1)}$, the values of the weights, and the number of units at each layer of the network (the $n_k$'s).

\begin{boxedsolution}
\bi
\item The product of $\sigma(s_j^{(k+1)})(1-\sigma(s_j^{(k+1)}))$ is at most $\frac{1}{4}$.  Further, if the value of $s_j^{(k+1)}$ is either very positive or very negative, the product will be very, very small.  Thus $\sigma(s_j^{(k+1)})(1-\sigma(s_j^{(k+1)}))$ will tend to decrease gradients as they go through the network.
\item The weight term $w_{j,i}^{(k)}$ could act to either increase gradients or decrease gradients depending on whether, on average, its magnitude is bigger or smaller than 1.
\item Since we are summing over $n_{k+1}$ units, having more units will tend to increase the values of the gradient as you move backward down the layers of the network.
\ei
\end{boxedsolution}

\item It turns out that the sigmoid function is only one viable choice for transforming the summation units into the values of the units for the next layer.  In fact, many different  non-linear functions can be used (e.g., \href{http://mathworld.wolfram.com/HyperbolicTangent.html}{tanh}).  The function used in this capacity is called an \emph{activation function}.  If we let $a$ refer to our activation function, then Equation~\ref{eq:recursion} becomes

\begin{align}
\frac{\partial l}{\partial x^{(k)}_i} &= \sum_{j=1}^{n_{k+1}} w^{(k)}_{j,i} a' \left ( s_{j}^{(k+1)} \right)  \frac{\partial l}{\partial x^{(k+1)}_j} \label{eq:recursion2} \enspace .
\end{align}


One very popular choice of activation function is called \href{https://en.wikipedia.org/wiki/Rectifier_(neural_networks)}{rectified linear} (or ReLu for \emph{rectified linear unit}).  A graph of the ReLu activation function is shown below.

\begin{center}
\includegraphics[width=0.4\linewidth]{figures/relu}
\end{center}

How might choosing the ReLu as an activation function help with the vanishing gradient problem in comparison to choosing the sigmoid as the activation function (use the backpropagation equations to arrive at your answer)?

\begin{boxedsolution}
the derivative of the function $ReLu(s^{(k+1)}_{j})$ will be 1 if $s^{(k+1)}_{j}$ is greater than 0.  The derivative will be 0 if $s^{(k+1)}_{j} \leq 0$.  Looking back at our equation, this means that the gradients from upper layers will either be preserved (if $s^{(k+1)}_{j} > 0$) or zeroed out (if $s^{(k+1)}_{j} \leq 0$).  As long as at least a few of the values $s^{(k+1)}_{1}, \ldots, s^{(k+1)}_{n_{k+1}}$ are positive, the partial derivative magnitudes won't be decreased to 0.  In order to avoid exploding gradients, you would want to make sure the weight magnitudes are in the appropriate range to counteract exponential growth in the partial derivatives (this is why weight initialization is the focus of a lot of research in deep learning).
\end{boxedsolution}

\item \textbf{Going Beyond (optional)} Using pytorch, perform a computational investigation of the vanishing gradient problem.  We suggest the following steps (note: we have our own version of this in the solutions if you want to check it out):
\bi
\item Create an MLP where the number of layers is passed in as a parameter.
\item Before doing any training of the network, feed an arbitrary input into the network, compute the loss, and then compute the gradient of the loss with respect to the weights at each level of the network.
\item Summarize the partial derivatives of the weights at each level in the network (e.g., take the mean of the absolute values) and plot this quantity as a function of $k$ (the layer number).  It may be helpful to use a log scale for the y-axis.
\ei
\ees
\begin{boxedsolution}
This is pretty rough, but \href{https://colab.research.google.com/drive/1lGlW-rnBnT-wh48HmNJlNb43FJuFQn01}{here} is our go of this.  We'll update after class.
\end{boxedsolution}
\end{exercise}

\section{Convolutions and Image Filtering}

Depending on your math background you may have seen the convolution operation before.  There are various types of convolutions, including convolutions with continuous functions, discrete functions, in 1D, in 2D, or in higher dimensions.  In this class we're only going to be talking about discrete, 2D convolutions.  It turns out that such convolutions provide a compelling way to think about processing data with intrinsically 2D structure (such as images).

Check out at least one of the resources below to learn how convolutions can be used to filter and transform images.

\begin{externalresources}[(30 minutes)]
\bi
\item \href{http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html}{Image Filtering} (\href{http://nb.mit.edu/f/55484}{on NB})
\item Video resource: \href{https://www.youtube.com/watch?v=C_zFhWdM4ic}{How blurs and filters work}
\ei
\end{externalresources}

\begin{exercise}[(10 minutes)]
This is not an exercise per se.  On the quiz you should mark whether or not you were able to engage with the readings and get a basic grasp of the topic.  As a basic guide, you should have some familiarity with the following ideas.
\bi
\item What a 2D convolutional kernel is and how it is applied to a 2D matrix (e.g., an image).
\item What happens when applying the kernel around the edges of the 2D matrix?
\item Understand how to reason about what a kernel does to an image based on the form of the kernel.
\ei
\end{exercise}


\section{Convolutional Neural Networks}
We now come to a particular type of neural network called a convolutional neural network.  These networks supplement the perceptron layers we've seen thus far with convolutional layers that apply various filtering operations to the data.  These networks are very well-suited for working with images since they are able to customize their filtering operations to the dataset in a way that maximizes their ability to solve the task.  There are lots of great resources on convolutional neural networks out there.  Please don't feel like you have to go through all of these.

\begin{externalresources}[(60 minutes)]
\bi
\item \href{https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/}{Intuitive Explanation of Convnets} (\href{http://nb.mit.edu/f/55489}{on NB}) (this one is a very well done high-level overview)
\item \href{http://scs.ryerson.ca/~aharley/vis/conv/flat.html}{Beautiful interactive visualization of a Convnet trained to recognize handwritten digits} (this is great, especially for visual learners.  Make sure to click on the activations at a layer to see the learned filter.  Don't miss this one!)
\item \href{https://cs.stanford.edu/people/karpathy/convnetjs/}{Convnet.js} (convnets training in your browser?!? Another good one for visual learners)
\item \href{https://www.datascience.com/blog/convolutional-neural-network}{Convolutional Neural Networks Explained} (\href{http://nb.mit.edu/f/55488}{on NB}) (this one is quite similar to the ``intuitive explanation of convnets.''  It's a little shorter though.)
\item \href{https://www.youtube.com/watch?v=bNb2fEVKeEo&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv\&index=5}{Lecture 5 from CS231 at Stanford} (if you want a lecture, this one is good.  it starts getting a bit too low-level around the 35 minute mark.  there are some nice slides on neural network history at the beginning).
\ei
\end{externalresources}

\begin{exercise}[(15 minutes)]
Open up \href{https://cs.stanford.edu/people/karpathy/convnetjs/}{Convnet.js} and try it on either MNIST or CIFAR-10.  Watch the network train.  Interpret as much of what you see as possible.  If you don't understand something, ask about it on NB (\href{http://nb.mit.edu/f/55494}{Convnet.js MNIST on NB} or \href{http://nb.mit.edu/f/55495}{Convnet.js on CIFAR-10 on NB})
\end{exercise}

\companionnotebook{Assignment_07_Companion_Pytorch_Titanic}

\end{document}
